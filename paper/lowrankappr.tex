\documentclass[12pt,a4paper,fleqn,leqno]{article}
\usepackage{mathtext}
\usepackage{cmap}
\usepackage[utf8x]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{amsmath,amssymb,amsthm,amscd,amsfonts}
\usepackage{euscript}
\usepackage{relsize}
\usepackage{mathdots}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{caption2}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{titlesec}
\usepackage{sicpro_rus}
\usepackage{mathtext}%русские буквы в формулах

\usepackage[colorlinks, urlcolor=blue, pdfborder={0 0 0 [0 0]}]{hyperref}

\hyphenation{Struc-tu-red}
\hyphenation{Ran-do-mized}
\hyphenation{Ma-xi-mi-za-tion}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\tr}{tr}

\def\rank{\mathop{\mathrm{rank}}}
%\newtheorem{definition}{Определение}%[section]
\newtheorem{proposition}{Предложение}%[section]
\newtheorem{algorithm}{Algorithm}%[section]

\input{letters}

%\input{letters_series_mathbb}
\input{newcommands}
%\input{newcommands2dssa}


\sectionfont{\centering}

\subsectionfont{\centering}
\subsubsectionfont{\normalsize}
\setcounter{page}{1}


\author{Звонарев Никита}
\title{Итеративные алгоритмы взвешенной аппроксимации рядами конечного ранга}
\begin{document}
\noindent УДК 519.246.8+519.254

\begin{center}{
\fontsize{18pt}{23pt}\selectfont\bf%
  \MakeUppercase{
 Итеративные алгоритмы взвешенной аппроксимации рядами конечного ранга
}}
\end{center}

\begin{center}{\bpv\bmv {Н.К.~Звонарев}\\
\footnotesize\it Санкт-Петербургский государственный университет,\\
Математико-механический факультет
\\
\rm
Россия, 198504, Санкт-Петербург, Петродворец, Университетский пр., 28\\
E-mail: \textcolor {blue}{\underline{mb\_93@mail.ru}}}
\end{center}
\begin{center}{\bpv {Н.Э.~Голяндина}\\
\footnotesize\it Санкт-Петербургский государственный университет,\\
Математико-механический факультет
\\
\rm
Россия, 198504, Санкт-Петербург, Петродворец, Университетский пр., 28\\
E-mail: \textcolor {blue}{\underline{nina@gistatgroup.com}}}
\end{center}
\hspace{1.25cm}\begin{minipage}{12.16cm}\bpv\bpv\bmv \noindent
\footnotesize{\bf Ключевые слова:}\/ Временные ряды, итерации Cadzow, ряды конечного ранга, взвешенный метод наименьших квадратов, косоугольное SVD-разложение, Singular Spectrum Analysis

\bpv\bpv\noindent  В работе рассматривается задача аппроксимации временных рядов рядами конечного ранга. Эта задача актуальна в задачах обработки сигналов, в частности, при анализе зашумлённых сигналов для выделения сигнала. В результате применения взвешенного метода наименьших квадратов (МНК) возникает оптимизационная задача, не имеющая решения в явном виде. Один из численных методов локального поиска минимума (итерации Cadzow) хорошо известен. Однако  итерации Cadzow могут работать только с весами специфичного вида, убывающими к краям ряда. В то же время, при анализе временного ряда естественным кажется брать одинаковые веса, порождающие обычную евклидову метрику. Поэтому в работе строятся и исследуются несколько новых методов с целью получить равные или примерно равные веса. Для предлагаемых методов рассматриваются вопросы сходимости, трудоёмкости и точности. Методы сравниваются на численном примере.

\end{minipage}\bls\bmv

\section{Введение}
Рассмотрим задачу выделения сигнала $\tsY = (y_1, \ldots, y_N)$ из наблюдаемого зашумлённого сигнала $\tsX = \tsY + \tsN$, где $\tsY$  обладает некоторой заданной структурой, а именно, $\tsY$ управляется некоторой \emph{линейной рекуррентной формулой} (ЛРФ) порядка $r$:
\begin{equation*}
y_n = \sum_{i = 1}^{r} a_i y_{n-i}, \quad n = r + 1, \ldots, N.
\end{equation*}
Вообще говоря, ряды, управляемые ЛРФ, могут быть записаны в параметрической форме в виде $y_n = \sum_i P_i(n) \exp(\alpha_i n) \cos(2 \pi \omega_i n + \psi_i)$. Однако, параметрический подход к задаче не приводит к хорошим оценкам, так как параметров много, и их оценки неустойчивы.

Хорошо зарекомендовали себя так называемые \emph{subspace-based} методы. Идея таких методов следующая: зафиксируем длину окна $L$, $1 \le L \le N$, $K = N - L + 1$, и построим по ряду $\tsY$ траекторную матрицу
\begin{equation*}
\bfY = \begin{pmatrix}
y_1 & y_2 & \ldots & y_K \\
y_2 & y_3 & \ldots & y_{K + 1} \\
\vdots & \vdots & \vdots & \vdots \\
y_L & y_{L + 1} & \ldots & y_N
\end{pmatrix}.
\end{equation*}
Если ряд управляется минимальной ЛРФ порядка $r$, $r < \min(L, K)$, то $\text{rank} \bfY = r < L$. Таким образом, $\bfY$ --- ганкелева матрица ранга $r$.

Пусть $\bfX$ --- траекторная матрица ряда $\tsX$. Тогда задачу оценивания $\tsY$ можно рассматривать как задачу аппроксимации матрицы $\bfX$ ганкелевой матрицей ранга, не превосходящего $r$:
\begin{equation}\label{introd_task}
||\bfY - \bfX||^2_\rmF \to \min_{\substack{\text{rank} \bfY \le r \\ \bfY \in \calH}}
\end{equation}

Этой задаче посвящено много работ, например, \cite{Cadzow1988}, \cite{Gillard2014}. Методы решения --- итеративные, например, метод Cadzow состоит из альтернативных проекций на множество ганкелевых матриц и матриц ранга не больше $r$. Хотя целевая функция не унимодальная, и сходимость к глобальному минимуму не гарантируется, тем не менее, задача \eqref{introd_task} считается достаточно хорошо исследованной.

Заметим, что задача \eqref{introd_task} эквивалента задаче
\begin{equation}\label{introd_task_2}
\sum_{i = 1}^n w_i(x_i - y_i)^2 \to \min_{\substack{\tsY: \rank \bfY \le r \\ \bfY \in \calH}},
\end{equation}
\begin{equation*}
\text{где} \quad w_i = \begin{cases}
i & \text{для $i = 1, \ldots, L-1,$}\\
L & \text{для $i = L, \ldots, K,$}\\
N - i + 1 & \text{для $i = K + 1, \ldots, N.$}
\end{cases}.
\end{equation*}

На краях ряда вес меньше, чем в середине, то есть \eqref{introd_task_2} является задачей взвешенного МНК для ряда (заметим, что чем меньше $L$, тем ближе веса к равномерным).

Целью данной работы было рассмотреть методы, решающие задачу \eqref{introd_task_2} c равными весами $w_i$, и сравнить результаты с точки зрения точности оценивания сигнала $\tsY$. Все рассматриваемые методы являются итерационными. Если интерес представляет оценка сигнала, которая не обязательно управляется ЛРФ, то в качестве оценки сигнала можно брать первую итерацию с целью уменьшения трудоёмкости. Таким образом, рассматриваемые методы сравнивались по точности оценки сигнала на первой итерации и в пределе. Заметим, что известный метод Singular Spectrum Analysis (SSA) \cite{Golyandina.Zhigljavsky2012} можно
рассматривать как одну итерацию метода Cadzow.

Структура работы следующая.  В разделе~\ref{sec:lowrank_appr} рассматривается задача для матриц аппроксимации ганкелевыми матрицами неполного ранга. 
Описывается общая структура итеративных алгоритмов в виде альтернативных проекций, приводятся методы построения проекторов, доказывается теорема о сходимости.
В разделе~\ref{sec:ts_matrices} описывается связь между задачами аппроксимации временных рядов и матриц, соотношение межжду весами в постановках задачи 
взвешенного МНК. Раздел~\ref{sec:alg} посвящён рассматриваемым алгоритмам. В разделе~\ref{sec:simul} сравнение алгоритмов на типичном численном примере.
Работа завершается короткими выводами и обсуждением дальнейших направления развития в разделе~\ref{sec:concl}. В приложении приведено доказательство результатов
о разделимости двух синусов, имеющих отношение к скорости сходимости некоторых из рассматриваемых алгоритмов.

\section{Аппроксимация ганкелевыми матрицами неполного ранга}
\label{sec:lowrank_appr}
\subsection{Общий алгоритм}
Рассмотрим задачу аппроксимации матрицы ганкелевой матрицей неполного ранга по некоторой (полу)норме $\|\cdot\|$.
Обозначим $\spaceR^{L\times K}$ пространство матриц размер $L$ на $K$, $\calM_r\subset \spaceR^{L\times K}$ множество матриц ранга, не превосходящего $r$,
$\calН \spaceR^{L\times K}$ --- множество ганкелевых матриц.
Заметим, что  $\calM_r$ не является линейным и даже не выпуклым множеством. Однако, $\calM_r$ является мультипликативным, т.е.
если $\bfX\in \calM_r$, то и $a\bfX\in \calM_r$ для любого $a$.
Пространство $\calН$ является линейным.

Задача имеет вид
\be
\label{eq:gen_task}
||\bfX - \bfY|| \to \min_\bfY, \mbox{\ где\ } \bfY \in \calH \cap \calM_r.
\ee

Чтобы привести схему алгоритма для решения данной задачи, введем проекторы на соответствующие подпространства
по норме $\|\cdot\|$, $\Pi_r$ --- проектор на $\calM_r\subset \spaceR^{L\times K}$. Оба проектора являются ортогональными,
так как для ортогональности достаточно того, чтобы пространство было мультипликативным. Заметим, что результат проектирования
на пространство матриц неполного ранга может быть неоднозначно определен, однако в дальнейшем будет предполагать, что
в случае неоднозначности выбрано произвольное значение из допустимых.

\begin{proposition} \label{pythaprop}
Пусть $X$ --- гильбертово пространство, $\calM$ --- замкнутое относительно умножения на скалярную величину подмножество ($x \in \calM \Rightarrow \alpha x \in \calM \, \forall \alpha \in \sfR$), $\Pi_\calM$ --- оператор проектирования на $\calM$ \footnote{На самом деле, проекция может быть определена неоднозначно, но дальше будем предполагать, что оператор однозначен. В данном случае, можно выбирать любой вариант для проекции, это никак не влияет на истинность утверждений.}. Тогда для любого $x \in X$ выполняется теорема Пифагора: $||x||^2 = ||x - \Pi_\calM x||^2 + ||\Pi_\calM x||^2$.
\end{proposition}
\begin{proof}
Представим множество $\calM$ в виде $\calM = \bigcup\limits_{l \in L}l$, где $L$ --- множество всех прямых, лежащих в $\calM$ и проходящих через $0$, и $l \cap m = 0$ для любых $l, m \in L$, $l \neq m$. Тогда операция проектирования может быть записана так: вначале мы выбираем прямую $l$ такую, что $\text{dist}(x, l) \rightarrow \min\limits_{l \in L}$, после чего $y = \Pi_\calM x$ --- это проекция $x$ на $l$. Получаем нужное свойство.
\end{proof}

Для решения задачи \eqref{eq:gen_task} можно использовать итеративный метод альтернативных проекций в виде
\be
   \bfY_{k+1}=\Pi_\calH \Pi_{\calM_r} \bfY_{k}, \mbox{\ где\ } \bfY_{0}=\bfX.
\ee

Докажем теорему относительно сходимости данного метода.

\begin{theorem}
\label{th:converg}
\begin{enumerate}
Пусть пространство $\calM_r$ является замкнутым в топологии, порождаемой нормой $\|\cdot\|$. Тогда
\item $||\bfX_k - \Pi_{\calM_r}\bfX_k|| \to 0$ при $k \to +\infty$, $||\Pi_{\calM_r}\bfX_k - \bfX_{k+1}|| \to 0$ при $k \to +\infty$.
\item Существует сходящаяся подпоследовательность $\bfX_{i_1}, \bfX_{i_2}, \ldots$ такая, что её предел $\bfX^*$ лежит в $\calM_r \cap \calH$.
\end{enumerate}
\end{theorem}
\begin{proof}
Воспользуемся неравенствами \cite{Chu.etal2003}
\begin{equation}
\label{chuprop}
||\bfX_k - \Pi_{\calM_r} \bfX_k|| \ge ||\Pi_{\calM_r} \bfX_k - \bfX_{k + 1}|| \ge ||\bfX_{k+1} - \Pi_{\calM_r} \bfX_{k + 1}||.
\end{equation}

\begin{enumerate}
\item Согласно неравенствам \eqref{chuprop}, последовательности $||\bfX_k - \Pi_{\calM_r} \bfX_k||$, $k = 1, 2, \ldots$, и $||\Pi_{\calM_r} \bfX_k - \bfX_{k + 1}||$, $k = 1, 2, \ldots$ являются невозрастающими. Очевидно, они ограничены снизу нулём. Поэтому они имеют одинаковый предел $c$, опять же согласно \eqref{chuprop}.

Докажем, что $c = 0$. Предположим противное: существует $d > 0$ такое, что для любого $k = 1, 2, \ldots$: $||\bfX_k - \Pi_{\calM_r} \bfX_k|| > d$, $||\Pi_{\calM_r} \bfX_k - \bfX_{k + 1}|| > d$. Согласно утверждению \ref{pythaprop}, справедливо следующее:
\begin{gather*}
||\bfX_k||^2 = ||\Pi_{\calM_r} \bfX_k||^2 + ||\bfX_k - \Pi_{\calM_r} \bfX_k||^2 =\\ ||\bfX_k - \Pi_{\calM_r} \bfX_k||^2 + ||\Pi_{\calM_r} \bfX_k - \bfX_{k + 1}||^2 + ||\bfX_{k + 1}||^2.
\end{gather*}
Таким образом, $||\bfX_{k+1}||^2 < ||\bfX_k||^2 - 2d^2$. Расписывая неравенство аналогично дальше, получим, что для любого $j = 1, 2, \ldots$: $||\bfX_{k+j}||^2 < ||\bfX_k||^2 - 2 j d^2$. Возьмём любое $k$, например $k = 1$, и $j = \lceil ||\bfX_k||^2 / (2d^2) \rceil + 1$. Тогда $||\bfX_{k+j}||^2 < 0$, чего не может быть.
\item Рассмотрим последовательность $(\Pi_{\calM_r} \bfX_k)$, $k = 1, 2, \ldots$. Она ограничена, так как $||\Pi_{\calM_r} \bfY|| \le ||\bfY||$ и $||\Pi_{\calH} \bfY|| \le ||\bfY||$ для любого $\bfY \in \sfR^{L \times K}$ (это справедливо, например, и по утверждению \ref{pythaprop}). Тогда мы можем выбрать из неё сходящуюся подпоследовательность $(\Pi_{\calM_r} \bfX_{i_k})$, $\bfX^*$ --- её предел, при этом $||\Pi_{\calM_r} \bfX_{i_k} - \bfX_{i_k + 1}|| = ||\Pi_{\calM_r} \bfX_{i_k} - \Pi_\calH \Pi_{\calM_r} \bfX_{i_k}|| \to 0$ при $k \to + \infty$. Учитывая, что $||\bfY - \Pi_\calH \bfY||$ --- композиция непрерывных отображений, получаем, что $||\bfX^* - \Pi_\calH \bfX^*|| = 0$ и зная, что $\calM_r$ --- замкнутое множество, получаем, что $\bfX^* \in \calM_r \cap \calH$. Осталось увидеть, что последовательность $(\Pi_\calH \Pi_{\calM_r} \bfX_{i_k})$ --- сходящаяся, так как $\Pi_\calH$ --- непрерывное отображение, и её предел равен $\bfX^*$. Получаем, что $\bfX_{i_k + 1}$ --- требуемая подпоследовательность.
\end{enumerate}
\end{proof}

TODO: про поправки в общем виде?

Ниже мы будем рассматривать нормы, порождённые взвешенным фробениусовым скалярным произведением в виде
\be
\label{eq:w_inner_prod}
\langle\bfX, \bfY\rangle_M = \sum_{l = 1}^L \sum_{k = 1}^K m_{l, k} x_{l, k} y_{l, k}.
\ee

Относительно такой нормы пространство $\calM_r$ является замкнутым и, следовательно, утверждения теоремы~\ref{th:converg} верны.

\subsection{Вычисление проекторов}

Будем рассматривать норму $\|\cdot\|_\bfM$, порождённую \eqref{eq:w_inner_prod}.

\paragraph{Проектор $\Pi_\calH$.} Несложно показать, что проектор $\Pi_\calH$
можно вычислить явным образом согласно следующему утверждению.

\begin{proposition}
Для $\widehat{\bfY}=\Pi_\calH \bfX$
\begin{equation*}
\hat{y}_{ij} = \frac{\sum_{l,k: l+k=i+j} m_{l,k} x_{l,k}}{\sum_{l,k: l+k=i+j} s_{l,k}}.
\end{equation*}
\end{proposition}

Явный вид проектора $\Pi_{\calM_r}$ в общем случае не получить.
Рассмотрим различные случаи.

\paragraph{Случай явного вида проектора $\Pi_{\calM_r}$.} Сначала рассмотрим случай, когда проектор можно выписать явно.
Хорошо известно, что если все веса $m_{ij}$ равны 1, то проектор $\Pi_{\calM_r} \bfX$
вычисляется как сумма первых компонент сингулярного разложения матрицы $\bfX$.
Обозначим этот проектор как $\Pi_r$.
Следующее утверждение предложение описывает случаи, когда нахождение проектора
сводится к применению оператора $\Pi_r$.  (TODO: Написать формально, какой вид через SVD?)

\begin{proposition}
\label{prop:projS}
Пусть существует симметричная, неотрицательно определённая матрица  $\bfS$ порядка $K \times K$,
такая что $\|\cdot\|_\bfM = \tr(\bfX \bfS \bfX^\rmT)$.
Предположим также, что пространство столбцов матрицы $\bfX$ лежит в пространстве столбцов матрицы $\bfX$.
Тогда 
\be
\label{eq:PiMr}
\Pi_{\calM_r} \bfX = (\Pi_r \bfB) (\bfO_\bfS^{\rmT})^\dagger,
\ee
где $\bfO_\bfS$ --- такая матрица, что $\bfS = \bfO_\bfS^{\rmT}\bfO_\bfS$,
$\bfB = \bfX \bfO_\bfS^{\rmT}$, $(\bfO_\bfS^{\rmT})^\dagger$ обозначает псевдообратную матрицу Мура-Пенроуза к матрице $\bfO_\bfS^{\rmT}$.
\end{proposition}
\begin{proof}
Доказательство является прямым следствием того, что рассматриваемая норма порождается косоугольным скалярным произведением в пространстве строк матрицы $\bfX$, см. детали в \cite{Golyandina2013}.
\end{proof}

\begin{remark}
Заметим, что условия утверждения~\ref{prop:projS} могут быть выполнены только если матрица $\bfS$ диагональная.
\end{remark}

\paragraph{Проектор $\Pi_{\calM_r}$ в общем случае.}
Так как в явном виде проектор не находится, то в общем случае используются итеративные алгоритмы.
Один из них описан в \cite{Srebro2003}

\begin{algorithm}
\label{alg:weightedSVD}
\textbf{Вход}: исходная матрица $\bfX$, ранг $r$, матрица весов $\bfM$,
критерий остановки STOP (например, число итераций $Q$).

\textbf{Результат}:
Матрица $\widehat\bfY$ как оценка $\Pi_{\calM_r} \bfX$.

\begin{enumerate}
\item
$\bfY_0 = \bfX$, $k=0$.
\item
$\bfY_{k+1} = \Pi_r(\bfX \odot \bfM + \bfY_{k} \odot (\bfU -  \bfM))$, где
$\bfU \in \sfR^{L \times K}$,  $\bfU = \begin{pmatrix}
1 & \cdots & 1 \\
\vdots & \ddots & \vdots \\
1 & \cdots & 1
\end{pmatrix}$, $k\leftarrow k+1$.
\item
Если STOP, то $\widehat\bfY = \bfY_k$.
\end{enumerate}
\end{algorithm}

Заметим, что в случае, когда $m_{ij}$ равняется 0 или 1, алгоритм является EM-алгоритмом \cite{Srebro2003},
соответственно, для него выполнены свойства EM-алгоритмов и он сходится к локальному минимуму в задаче поиска проектора.
В случае нулевых весов формально неважно, какие значения стоят в матрице на этих местах. Однако для сходимости алгоритма
это может быть существенно.

\section{Временные ряды и задача аппроксимации матриц}
\label{sec:ts_matrices}
\footnote{Сюда скопировала материал, но не правила}
\subsection{Постановка задачи для временных рядов}
Рассмотрим временной ряд $\tsX = (x_1, \ldots, x_N)$ длины $N \ge 3$. Зафиксируем длину окна $L$, $1 < L < N$, положим $K = N - L + 1$. Также рассмотрим последовательность векторов:
\begin{equation}\label{l_lagged}
X_i = (x_i, \ldots, x_{i + L - 1})^\rmT, \qquad i = 1, \ldots, K.
\end{equation}
\emph{Траекторным пространством} ряда $\tsX$ назовём $$\spX^{(L)}(\tsX) = \spX^{(L)} = \text{span}(X_1, \ldots, X_{N-L+1}).$$

\begin{definition}
Пусть $0 \le r \le L$. Будем говорить, что ряд $\tsX$ \emph{имеет $L$-ранг $r$}, если $\dim \spX^{(L)} = r$.
\end{definition}

Заметим, что ряд $\tsX$ может иметь $L$-ранг $r$ только тогда, когда
\begin{equation}
r \le \min(L, N-L+1). \label{min_condition}
\end{equation}
Скажем, что при фиксированном $r$ длина окна $L$ является \emph{допустимой}, если для неё выполнено условие \eqref{min_condition}.

В дальнейшем будет предполагаться, что $L$ не превосходит $K$, так как транспонирование не изменит ситуацию, а строчный ранг матрицы равен её столбцовому рангу.

Пусть $\sfX_N$ --- множество всех временных рядов длины $N$, $\sfX_N^r$ --- множество всех временных рядов длины $N$ $L$-ранга $r$. Для заданныx $\tsX \in \sfX_N$ --- исходный временной ряд, $1 < L < N$ --- длина окна и $r$, удовлетворяющего условию \eqref{min_condition}, рассмотрим задачу:
\begin{equation} \label{L-rank_task}
f_q(\tsY) \to \min_{\tsY \in \sfX_N^r}, \quad f_q(\tsY) = \sum \limits_{i=1}^N q_i(x_i - y_i)^2,
\end{equation}
где $f_q(\tsY) = \sum \limits_{i=1}^N q_i(x_i - y_i)^2$, $y_i$ --- $i$-е измерение ряда $\tsY$, а $q_1, \ldots, q_N$, $q_i \ge 0$, $i = 1, \ldots, N$ --- некоторые неотрицательные веса. В частном случае, рассматривается целевая функция $f(\tsY) = \rho^2(\tsX, \tsY)$ --- квадрат евклидова расстояния в $\sfR^N$. Она совпадает с $f_q(\tsY)$ при $q_i = 1$, $i = 1, \ldots, N$.
%-------

%\subsection{Отображение на множество ганкелевых матриц}
Пусть $\tsX$ --- временной ряд длины $N$, а $\bfX \in \calH$ --- матрица, где $\calH = \calH^{L \times K}$ --- множество всех ганкелевых матриц размера $L \times K$. Тогда между $\sfX_N$ --- множеством всех временных рядов длины $N$ и $\calH$ можно построить отображение $\calT$, действующее по правилу
\begin{equation*}
\calT(\tsX) = \bfX: \hat x_{l, k} = x_{l + k - 1}, \quad \bfX = (\hat x_{l,k}), \quad \tsX = (x_1, \ldots, x_N).
\end{equation*}
Нетрудно заметить, что это отображение является биективным.

Так как есть взаимно-однозначное соответствие между пространство рядов и ганкелевыми матрицами,
задачу~\eqref{L-rank_task} можно записать на матричном языке.

\subsection{Эквивалентные целевые функции задачи \eqref{L-rank_task}}
В пространстве рядов целевая функция явным образом задаётся через скалярное произведение
\begin{equation}
\label{eq:norm_ser}
    \langle\tsX,\tsY\rangle_q = \sum_{i = 1}^N q_i x_i y_i,
\end{equation}
где $q_i$ --- положительные веса.

Рассмотрим два скалярных произведения в пространстве матриц, являющихся расширениями
обычного фробениусова скалярного произведения.

Введём
\begin{equation}
\label{eq:norm1M}
    \langle\bfX,\bfY\rangle_{1,\bfM} = \sum_{i = 1}^L \sum_{j=1}^K m_{i,j} x_{i,j} y_{i,j}.
\end{equation}
для матрицы $\bfM$ с положительными элементами и
\begin{equation}
\label{eq:norm2S}
    \langle\bfX,\bfY\rangle_{2,\bfS} = \tr(\bfX \bfS \bfY^\rmT)
\end{equation}
для положительно определённой (или неотрицательно определённой для полунормы) матрицы $\bfS$.

Заметим, что если матрица $\bfM$ состоит из всех единиц, т.е. $m_{i.j}=1$,
и если $\bfS$ --- единичная матрица, то оба скалярных произведения совпадают
с обычным фробениусовым.

\begin{proposition}
\label{prop:equiv_tasks}
1. Пусть $\bfX = \calT(\tsX)$,  $\bfY = \calT(\tsY)$. Тогда $\langle\tsX,\tsY\rangle_q= <\bfX,\bfY>_{1,\bfM}$ тогда и только тогда, когда
\begin{equation}\label{qi_mi}
q_i = \sum_{\substack{1 \le l \le L \\ 1 \le k \le K \\ l+k-1=i}} m_{l,k}.
\end{equation}

2. Для диагональной матрицы $\bfS$, $\langle\bfX,\bfY\rangle_{1,\bfM}= \langle\bfX,\bfY\rangle_{2,\bfS}$ тогда и только тогда, когда
\begin{equation}\label{sk_mlk}
m_{l,k}=s_{k,k}.
\end{equation}
\end{proposition}
\begin{proof}
Для доказательства первой части утверждения заметим, что
\begin{equation*}
\langle \bfX, \bfY \rangle_{1,\bfM} = \sum_{i = 1}^L \sum_{j = 1}^K m_{i,j} x_{i + j - 1} y_{i + j - 1},
\end{equation*}
Доказательство второй части следует из того, что для диагональной матрицы $\bfS$
\begin{equation*}
\langle \bfX, \bfY \rangle_{2,\bfS} = \sum_{l=1}^L \sum_{k=1}^K s_{k,k} x_{l,k} y_{l, k}.
\end{equation*}
\end{proof}

Заметим, что вторая матричная норма с диагональной матрицей $\bfS$ является частным случаем первой.
Однако, ценность записи первой нормы в виде
второй состоит в том, что аппроксимация матрицами меньшего ранга по первой норме --- это сложная задача при неединичных весах
$m_{i.j}$, а аппроксиммация по второй норме --- естественная задача, решаемая с помощью косоугольного сингулярного разложения.

\section{Алгоритмы}
\label{sec:alg}
\subsection{Алгоритм Cadzow}
Этот алгоритм служит для решения задачи () с весами $m_{ij}=1$, что соответствует задаче
() с весами $w_i$, задаваемыми \ref{}.
Алгоритм был предложен в \cite{Cadzow1988}. Его недостатком является то. что веса $w_i$ не являются равными,
на краях они меньше, чем в середине. Чем меньше длина окна. тем ближе веса к равным.

\begin{algorithm}[Cadzow]
\textbf{Вход}: Временной ряд $\tsX$, длина окна $L$, ранг $r$,
критерий остановки STOP (например, число итераций Q).

\textbf{Результат}:
Ряд $\widehat\tsY$ как оценка аппроксимации $\tsX$ рядом конечного ранга $r$.

\begin{enumerate}
\item
$\bfY_0 = \calT \tsX$, $k=0$.
\item
$\bfY_{k+1} = \Pi_\calH  \Pi_{\calM_r} \bfY_{k}$, $k\leftarrow k+1$.
\item
Если STOP, то $\widehat\tsY = \calT^{-1} \bfY_k$.
\end{enumerate}
\end{algorithm}

\subsection{Алгоритм Weighted Cadzow}

Пусть веса $w_{i}=1$. Тогда, по утверждению~\ref{prop:equiv_tasks}, эквивалентные матричные веса имеют вид 
\begin{equation} 
\label{Mw}
   m_{l, k} = \frac{1}{w_{l + k - 1}},
\end{equation}

\begin{algorithm}[Weighted Cadzow]
\textbf{Вход}: Временной ряд $\tsX$, длина окна $L$, ранг $r$,
критерии остановки STOP1 для внешних итераций и STOP2 для внутренних.

\textbf{Результат}:
Ряд $\widehat\tsY$ как оценка аппроксимации $\tsX$ рядом конечного ранга $r$.

\begin{enumerate}
\item
$\bfY_0 = \calT \tsX$, $k=0$.
\item
Получение $\widehat\bfZ$ по алгоритму~\ref{alg:weightedSVD} с критерием остановки STOP2, примененному к $\bfY_k$ для оценивания $\Pi_{\calM_r} \bfY_{k}$.
\item
$\bfY_{k+1} = \Pi_\calH  \widehat\bfZ$, $k\leftarrow k+1$.
\item
Если STOP1, то $\widehat\tsY = \calT^{-1} \bfY_k$.
\end{enumerate}
\end{algorithm}

\subsection{Алгоритм Extended Cadzow}

Постановка задачи в этом алгоритме несколько отличается от общей постановки задачи.
Формально, мы продлеваем ряд в обе стороны на $L-1$ точек некоторыми значениями, приписывая им вес 0, т.е.
считая их пропусками. Таким образом, расширенный ряд $\widetilde\tsX$ будет иметь длину $N+2L-2$, а его траекторная матрица
$\widetilde\bfX$ будет иметь размер $L$ на $N+L-1$.

Для нового расширенного ряда мы применяем общую схему с весами $m_{ij}=\calT \tsI$, где ряд $\tsI$ имеет 
значения 1 на местах исходного ряда и значения 0 на местах пропусков.
(TODO: поформальнее)

\begin{algorithm}[Extended Cadzow]
\textbf{Вход}: Временной ряд $\tsX$, длина окна $L$, ранг $r$,
критерии остановки STOP1 для внешних итераций и STOP2 для внутренних,
значения, которыми дополнен ряд слева и справа, $\tsL_{L-1}$ и $\tsR_{L-1}$.

\textbf{Результат}:
Ряд $\widehat\tsY$ как оценка аппроксимации $\tsX$ рядом конечного ранга $r$.

\begin{enumerate}
\item
$\bfY_0 = \calT \widetilde\tsX$, $k=0$.
\item
Получение $\widehat\bfZ$ по алгоритму~\ref{alg:weightedSVD} с критерием остановки STOP2, примененному к $\bfY_k$ для оценивания $\Pi_{\calM_r} \bfY_{k}$.
\item
$\widetilde\bfY_{k+1} = \Pi_\calH  \widehat\bfZ$, $k\leftarrow k+1$.
\item
Если STOP1, то $\widehat\tsY = \calT^{-1} \bfY_k$, где $\bfY_k$ состоит из столбцов матрицы $\widetilde\bfY_{k}$
c $L$-го по $N$-й.
\end{enumerate}
\end{algorithm}

\subsection{Алгоритмы Oblique Cadzow}

Эти алгоритмы могут быть применены, если выполнены условия утверждения~\ref{prop:projS}.

\begin{algorithm}[Oblique Cadzow]
\label{alg:obliqueCadzow}
\textbf{Вход}: Временной ряд $\tsX$, длина окна $L$, ранг $r$, матрица $\bfS=\diag(s_1,\ldots, s_K)$, где $K=N-L+1$,
критерий остановки STOP.

\textbf{Результат}:
Ряд $\widehat\tsY$ как оценка аппроксимации $\tsX$ рядом конечного ранга $r$.

\begin{enumerate}
\item
$\bfY_0 = \calT \tsX$, $k=0$.
\item
$\bfY_{k+1} = \Pi_\calH  \Pi_{\calM_r} \bfY_{k}$, $k\leftarrow k+1$, где 
$\Pi_{\calM_r}$ задан формулой \eqref{eq:PiMr}.
\item
Если STOP, то $\widehat\tsY = \calT^{-1} \bfY_k$.
\end{enumerate}
\end{algorithm}


Так как исходно задачей является аппроксимация временного ряда с равными весами, поставим задачу нахождения соответствующей матрицы $\bfS$.
Оказывается такой невырожденной матрицы не существует, поэтому рассмотрим несколько вариантов.

\subsubsection{Алгоритм Cadzow ($\alpha$)}
Найдём такую матрицу $\bfS$, чтобы полунорма $||\cdot||_{\bfS}$ соотносилась с расстоянием $f_q(\tsY) = f(\tsY)$, встречающимся в \eqref{L-rank_task}, то есть выполнялось равенство \eqref{qi_mi} при всех $q_i = 1$. Воспользуемся следующей леммой:
\newtheorem{lemma}{Лемма}
\begin{lemma}\label{zhiglemma}\cite{Gillard2014}
Пусть $\tsX \in \sfX_N$, $\bfX = \calT(\tsX) \in \sfR^{L \times K}$. Если $h = N/L$ --- целое, тогда $\tr(\bfX \bfS \bfX^\rmT) = \tsX^\rmT \tsX$, где $\bfS$ --- диагональная матрица со следующими диагональными элементами:
\begin{equation*}
s_{k,k} = \begin{cases}
1, & \text{если} \quad k = jL+1 \quad \text{для некоторых} \quad j = 0, \ldots, h-1, \\
0, & \text{в противном случае}
\end{cases}.
\end{equation*}
\end{lemma}
\begin{proof}
В предположении, что $h = N/L$ --- целое, получим $N = hL$ и $K = (h - 1)L + 1$.
По определению, элементы матрицы $\bfX$ $x_{l,k} = \hat{x}_{l+k-1}$, где $(\hat{x}_k)$ --- элементы ряда $\tsX$. Получим следующее:
\begin{gather*}
\tr(\bfX \bfS \bfX^\rmT) = \sum_{l=1}^L \sum_{k=1}^K \sum_{k'=1}^K x_{l,k} s_{k, k'} x_{l,k'} = \sum_{l=1}^L \sum_{k=1}^K s_{k, k} x_{l,k}^2 =\\ \sum_{l=1}^L \sum_{j = 0}^{h - 1}x_{l,jL+1}^2 = \sum_{l=1}^L \sum_{j = 0}^{h - 1}\hat{x}_{l + jL}^2 = \sum_{n=1}^N \hat{x}_n^2.
\end{gather*}
\end{proof}

Первая проблема, которая встречается при этом подходе --- нулевые элементы на диагоналях матрицы $\bfS$. Таким образом, ранг матрицы $\bfS$ заведомо меньше $K$. В \cite{Gillard2014} предложено заменить нули на диагоналях на некоторое малое $\alpha$, чтобы исправить проблему. Это исправляет проблему ранга, но, как будет видно дальше, оставляет свойства метода, решающего задачу \eqref{L-rank_task}, плохими.

%\newtheorem{consequence}{Следствие}
\begin{proposition}\label{zhigconseq}
В предположениях леммы \ref{zhiglemma}: пусть $h = N/L$ --- целое, и матрица $\bfS$ --- диагональная со следующими диагональными элементами:
\begin{equation}\label{zhigweights}
s_{k,k} = \begin{cases}
1, & \text{если} \quad k = jL+1 \quad \text{для некоторых} \quad j = 0, \ldots, h-1, \\
\alpha, & \text{в противном случае,}
\end{cases}
\end{equation}
где $0 \le \alpha \le 1$. Тогда веса $q_i$, определённые в \eqref{qi_mi}, выглядят следующим образом
\begin{equation*}
q_i = \begin{cases}
1 + (i - 1) \alpha & \text{для $i = 1, \ldots, L-1,$}\\
1 + (L - 1) \alpha & \text{для $i = L, \ldots, K-1,$}\\
1 + (N - i) \alpha & \text{для $i = K, \ldots, N.$}
\end{cases}
\end{equation*}
\end{proposition}
\begin{proof}
Достаточно просуммировать $s_{k,k}$ правильное число раз, равное размеру $i$-й побочной диагонали. 
\end{proof}

Используя соотношение \eqref{sk_mlk} и взяв $\bfS$, описанную в лемме \ref{zhiglemma}, мы получим следующую $\bfM$:
\begin{equation*}
\bfM = \begin{pmatrix}

1 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & \cdots & 1 \\
1 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & \cdots & 1 \\
\vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots & \cdots & \cdots & 1 \\
1 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & \cdots & 1
\end{pmatrix}.
\end{equation*}
Таким образом, расстояние берётся только по $h$ столбцам вместо $K$, а домножение на $\bfO_\bfS^{\rmT}$ обнуляет у матрицы $\bfX$ $K - h$ столбцов. 

\begin{remark}
Оптимизационная задача с $\alpha=0$ соответствует поиску произвольной. не обязательно ганкелевой, матрицы ранга, не превосходящего $r$, 
ближайшей по фробениусовой норме к матрице 
\be
\label{eq:traj_noinersect}
\begin{pmatrix}
x_1&x_{L+1}&\cdots&x_{K}\\
\vdots&\vdots&\cdots&\vdots\\
x_L&x_{2L}&\cdots&x_N
\end{pmatrix}.
\ee
Эта задача отличается от задачи аппроксимации рядами конечного ранга.

При $\alpha=1$ метод Cadzow($\alpha$) совпадает с обычным методом Cadzow.
 
\end{remark}

Будем называть алгоритм~\ref{alg:obliqueCadzow} с матрицей $\bfS$, заданной в \eqref{zhigweights}, алгоритмом Cadzow($\alpha$).

\subsubsection{Алгоритм Cadzow c $\widehat\bfS$}

Подойдём к задаче со стороны матриц: найдём матрицу $\bfS$ такую, что полученная норма $||\cdot||_{\bfS}$ будет наиболее близка к матрице $\bfM$, полученной в \eqref{Mw}. Как можно заметить в доказательстве леммы \ref{zhiglemma}, нас устроит только диагональная $\bfS$. Рассмотрим множество $\sfZ^{L \times K} \subset \sfR^{L \times K}$ --- матрицы, у которых элементы в столбцах равны. Разумным выбором станет матрица $\bfZ \in \sfR^{L \times K}$, $\bfZ=(z_{l,k})$, $z_{l,k} = s_{k,k}$ такая, что
\begin{equation*}
||\bfM - \bfZ|| \to \min_{\bfZ \in \sfZ^{L \times K}}.
\end{equation*}
Решение сводится к усреднению элементов матрицы $\bfM$ по столбцам. В итоге, полученная матрица $\hat \bfS$ будет иметь следующие диагональные элементы:
\begin{equation}\label{my_s}
\hat s_{k,k} = \frac{1}{L}\sum_{l=1}^L m_{l, k}.
\end{equation}

Будем называть алгоритм~\ref{alg:obliqueCadzow} с матрицей $\bfS=\widehat\bfS$, алгоритмом Cadzow $\widehat\bfS$.

\subsubsection{Соответствие между алгоритмомами и весами $q_i$ в \eqref{L-rank_task}}

Обозначим веса $q(\alpha)$ и $\hat q_i$, которые порождаются матрицей $\bfS$ в алгоритмах Cadzow ($\alpha$) и Cadzow с $\widehat\bfS$ соответственно.

Справедливы следующие утверждения. 

\begin{proposition} \label{myweightstat}
Матричные веса $\hat s_k$, определённые в \eqref{my_s}, равны
\begin{equation*}
\hat s_{k,k} = \begin{cases}
\frac{1}{L}\left(\frac{k}{L} + \sum_{j=k}^{L-1} \frac{1}{j} \right),& k = 1, \ldots, L-1, \\
\hat s_{N - k + 1, N - k + 1}, & k = K - L + 2, \ldots K, \\
1/L, &\text{в противном случае}.
\end{cases}
\end{equation*}
\end{proposition}

\begin{proof}
Достаточно подставить в \eqref{my_s} $m_{l,k}$, определённые в \eqref{Mw}.
\end{proof}

\begin{proposition} \label{myserweightstat}
Пусть $N \ge 4(L-1)$. Тогда веса $\hat{q_i}$, определённые в \eqref{qi_mi},
выглядят следующим образом
\begin{equation*}
\hat{q_i} = \begin{cases}
\frac{i(i+1)}{2 L^2} + \frac{i}{L}(1 + H_{L-1} - H_i), &1 \le i \le L-1, \\
1 + \frac{2iL-i-i^2}{2L^2} + \frac{L-i}{L}(H_{L-1} - H_{i - L}), & L \le i \le 2L-1, \\
\hat{q}_{N-i+1}, &N-2L+2 \le i \le N, \\
1, &\text{в противном случае},
\end{cases}
\end{equation*}
где $H_0 = 0$, а $H_i = \sum_{j=1}^i 1/j$ --- $i$-е гармоническое число.
\end{proposition}

\begin{proof}
Доказательство выполняется непосредственными вычислениями согласно равенствам \eqref{my_s} и \eqref{Mw}.
\end{proof}

Отнормированные веса $q_i$ (так, чтобы сумма была равна 1) при $\alpha = 1$ (стандартный алгоритм Cadzow), $\alpha = 0$ (равные веса $q_i$) и $\alpha = 0.1$,
 а также $\hat{q}_i$ при $N = 40$, $L = 8$ представлены на рисунке~\ref{img_weights}.
\begin{figure}[!h] \begin{center}
\includegraphics[width = 15cm]{weights.pdf}\caption{Веса ряда}\label{img_weights}
\end{center}\end{figure}

\subsection{Комментарии к алгоритмам. Сравнение}

Итак, в дальнейшем будем рассматривать методы
Weighted Cadzow, Extended Cadzow, Cadzow ($\alpha$), $0< \alpha \leq 1$, совпадающий с обычным методом Cadzow при $\alpha=1$,
и Cadzow с $\widehat\bfS$.
Заметим, что длина окна $L$ является параметром всех рассматриваемых методов.

\begin{itemize}
\item
Все методы итеративные и, вообще говоря, они не обязаны сходится к глобальному экстремум в задаче МНК  (теоретически, даже сходимость можно иметь место только по подпоследовательностям; однако, во всех проведённых численных экспериментах сходимость имела место). Поэтому сравнение методов, даже
решающих одну и ту же задачу, имеет смысл. 
\item
Нас интересует сравнение методом не по достигаемый минимум в задаче МНК, а по точности оценивания выделяемого сигнала $\bfY$.
Вполне возможно, что слишком хорошая аппроксимация исходного ряда может привести к переподгонке, результатом которой
может явиться ухудшение качества оценивания сигнала.
\item
Методы Weighted Cadzow и Extended Cadzow решают задачу \eqref{L-rank_task} c единичными весами $q_i$. Остальные методы решат задачу с весами, отличающимися от равных в той или иной степени.
\item
Однако каждая итерация методов Weighted Cadzow и Extended Cadzow отличается повышенной трудоемкостью, так как использует
ещё один итеративный алгоритм.
\item
Трудоемкость методов определяется как трудоемкостью одной итерации, так как числом итераций. Поэтому с этой точки зрения скорость сходимость
представляет значительный интерес.
\item
На примере обычного метода Cadzow, известно, что при решении реальных задач одна итерация может представлять значительный интерес, как по трудоёмкости, так и по широкому спектру решаемых задач. А именно, одна итерация метода Cadzow --- это известным метод Singular Spectrum Analysis (SSA), который умеет решать существенно большее число задач, чем сам итеративный метод. Поэтому представляет интерес также точность оценивания сигнала, выполненная с помощью одной итерации во всех рассматриваемых методах.
\item 
В методе SSA есть понятие разделимости, которое определяет свойство метода (приближённо) находить сигнал по наблюдаемой сумме. Тем самым разделимость
тесно связана с точностью первой итерации итеративного метода. В свою очередь, естественно предположить, что точность первой итерации связана со скоростью сходимости метода. Поэтому вопросы разделимости имеют отношение к скорости сходимости итеративных алгоритмов. 
\item
Связь разделимости с длиной окна $L$ для метода SSA хорошо изучена (см., например, \cite{Golyandina2010}). А именно, оптимальная длина окна близка
к половине длины ряда. Маленькие длины окна $L$ приводят к плохой разделимости. Влияние параметра $\alpha$ в классе алгоритмов 
Cadzow ($\alpha$) на разделимость исследуется в приложении (раздел~\ref{sec:app}) на примере разделения двух гармоник. Там показано, что малые значения
$\alpha$ приводят к плохой разделимости, хотя именно они соответствуют примерно равным весам $q_i$ в задаче \eqref{L-rank_task}.
В целом, виден следующий эффект: параметры, соответствующие более равномерным весам, приводят к худшей разделимости.
\item
Вполне возможно, что хорошая скорость сходимости и точность оценивания сигнала являются свойствами, которые не выполняются одновременно, как в силу 
противоречия между равномерностью весов и разделимостью, так и потому что медленная сходимость вполне может привести к сходимости алгоритма к лучшему значению оптимизационной задачи.
\end{itemize}

\section{Численные эксперименты}
\label{sec:simul}
В этом разделе мы приведем численные результаты, призванные продемонстрировать указанные выше выводы и соображения.
Сравнение было проведено для случая выделения синуса и экспоненциально-модулированного синуса.
Так как результаты, в целом, аналогичны, мы приведем только результаты, полученные для выделения гармонического ряда.

Был взят следующий сигнал:
\begin{equation*}
\tsX = (x_{1}, \ldots, x_N), \qquad x_{i} = 5\sin{\frac{2 k \pi}{6}}, \quad k = 1, \ldots, N, \quad N = 40
\end{equation*}
и рассматривался ряд в виде $\tsY = \tsX + \tsR$, где  $\tsR$ --- гауссовский белый шум с математическим ожиданием $0$ и дисперсией, равной $1$.
Точность оценивания сигнала оценивалась с помощью среднеквадратического отклонения CKO по 1000 реализациям.
Сравнение проводилось на одних и тех же реализациях исходного ряда. Результаты сравнения были проверены на значимость и оказались значимыми 
при уровне значимости 5\%. \footnote{TODO?}.

Рассмотрим сначала класс методов Oblique Cadzow, включающий в себя и обычный метод Cadzow.
Рисунок \ref{img_cadzowspeed2} показывает одновременно скорость сходимости для разных значения параметра $\alpha$ и двух разных длин окна $L$.
По оси x откладывается номер итерации, по оси y --- СКО оценки сигнала, делённое на число точек в ряде.  \footnote{Перерисовать картинку с меньшим макс. числом итераций}

\begin{figure}[!h] 
\begin{center}
\includegraphics[width = 15cm]{cadzowspeed_2.pdf}
\caption{Оценка отклонения от $\tsX$ при различном числе итераций и различных $L$}
\label{img_cadzowspeed2}
\end{center}
\end{figure}

В пределе самым лучших оказался самый медленно сходящийся метод, а именно, из рассмотренных методов это Cadzow (0.1) и длиной окна $L=8$.
Этот метод также соответствует наиболее равномерным весам из рассматриваемых в примере.
Заметим, что точность всех рассматриваемых методов различается не очень сильно, от 0.3 ($\alpha=0.1$, $L=8$) в лучшем случае до 0.35 в худшем
($\alpha=1$, $L=20$). Однако в первом случае ошибка 0.35 достигается уже на первой итерации, в то время как во втором для
достижения ошибки 0.35 требуется 7--8 итераций.

Рассмотрим более подробно распределение ошибки по элементам ряда При этом включим в рассмотрение и
методы Extended и Weighted.
В качестве критериев остановки будем использовать в алгоритмах число итераций, равное 100, в STOP и STOP1
и ... в STOP 2. \footnote{Вставить число}

Возьмем длину окна $L=20$.  На рисунках~\ref{fig:s1_it1}~и~\ref{fig:s1_it100} по оси x откладывается номер точки ряда,
а по оси y --- СКО от истинного значения сигнала в данной точке. Рисунок~\ref{fig:s1_it1} показывает ошибки на первой итерации,
а рисунок~\ref{fig:s1_it100} --- на итерации с номером 100.
Видно, что в обоих случаях самым точным оказывается метод Extended Cadzow. Из методов, не имеющих внутренних итераций,
на первой итерации выигрывают методы обычный Cadzow и Cadzow c $\widehat\bfS$. В пределе (после 100-й итерации результаты практически не меняются)
наилучшим из них, что не удивительно после анализа рисунка~\ref{img_cadzowspeed2}, оказался Cadzow ($\alpha=0.01$).


\begin{figure}[!h] 
\begin{center}
\includegraphics[width = 15cm]{s1_it1.pdf}
\caption{Ошибка восстановления в точке на одной итерации для ряда $\tsX$}
\label{fig:s1_it1}
\end{center}
\end{figure}

\begin{figure}[!h] 
\begin{center}
\includegraphics[width = 15cm]{s1_it100.pdf}
\caption{Ошибка восстановления в точке на ста итерациях для ряда $\tsX$}
\label{fig:s1_it100}
\end{center}
\end{figure}

Приведем таблицу~\ref{fintable}, где отражены результаты методов.
В таблице $T$ --- число итераций, $\tsX$ --- сигнал, $\tsY$ --- исходный ряд; $L=20$.
\footnote{сюда бы ещё Extended и Weighted для двух разных $L$}
\footnote{перестала понимать таблицу. Почему ошибка восстановления и расстояние до зашумленного
ряда при шуме =1 примерно одинаковые?}

\begin{table}[!h]
\begin{center}
\caption{Сравнение алгоритмов}\label{fintable}
\begin{tabular}{|c|c|c|c|c|}
\hline
$P$: & $\tsX$, $T = 1$ & $\tsY$, $T = 1$ & $\tsX$, $T = 100$ & $\tsY$, $T = 100$  \\
\hline
Cadzow, $\alpha = 1$ & 0.3749 & 0.3762 & 0.3714 & 0.3661 \\
\hline
Cadzow, $\alpha = 0.1$ & 0.4312 & 0.4434 & 0.3254 & 0.3220 \\
\hline
Cadzow $\hat{\bfS}$ & 0.3643 & 0.3718 & 0.3494 & 0.3439 \\
\hline
Weighted Cadzow & 0.3640 & 0.3734 & 0.3393 & 0.3343 \\
\hline
Extended Cadzow & 0.3330 & 0.3474 & 0.3171 & 0.3191 \\
\hline
\end{tabular}
\end{center}
\end{table}



\section{Заключение}
\label{sec:concl}
В работе были рассмотрены известные итеративные алгоритмы и предложены новые для аппроксимации ряда рядами конечного ранга с целью
оценивания сигнала в зашумлённом ряде по взвешенному методу наименьших квадратов.


Был рассмотрен довольно широкий набор алгоритмов с целью получить равные веса в МНК. В рассматриваемых алгоритмах равные веса удалось получить 
получить только с помощью вложенных итераций, которые сходятся только к локальному экстремуму и вдобавок делают алгоритм очень трудоёмкими.
Итеративные методы без вложенных итераций дают только приближённо равные веса.

Для рассматриваемого класса алгоритмов типа алгоритмов Cadzow была доказана сходимость внешних итераций алгоритмов по подпоследовательностям.

На примере зашумлённого синуса с помощью моделирования были получены результаты по точности и скорости сходимости предлагаемых алгоритмов.
Результаты показали, что самым точным оказывается самый трудоемкий метод. 
Были рассмотрены вопросы соотношения скорости сходимости, трудоёмкости и точности методов.
Акцент был сделан также на точности оценки с помощью одной итерации методов.

В дальнейшем предполагается провести расширенное численное и аналитическое исследование методов и получить более конкретные рекомендации по
соотношению трудоёмкости и точности алгоритмов.

\bibliographystyle{plain}
\bibliography{ssa2012}
\addcontentsline{toc}{section}{Список литературы}

\section{Приложение: Разделимость двух гармоник для алгоритма Cadzow($\alpha$)}
\label{sec:app}

Введём ещё одну характеристику алгоритмов, показывающую, насколько хорошо они раскладывают временной ряд на его аддитивные компоненты. Основным применением описанных алгоритмов является задача оценки сигнала, поэтому данное качество нам важно для получения как можно более точной оценки.

Пусть $\bfS \in \sfR^{K \times K}$ --- симметричная неотрицательно определённая матрица, $\tsX_1$ и $\tsX_2$ ---  два разных временных ряда длины $N$, $\bfX^1$, $\bfX^2$ --- их траекторные матрицы. Тогда \emph{коэффициентом корреляции $i$-го и $j$-го столбца} назовём следующую величину:
\begin{equation}\label{col_corr}
\rho^c_{i,j} = \frac{(X^1_i, X^2_j)}{||X^1_i|| ||X^2_j||},
\end{equation}
где $X^k_i$ --- $i$-й столбец матрицы $\bfX^k$, $k = 1, 2$, $(\cdot, \cdot)$ --- скалярное произведение, $||\cdot||$ --- евклидова норма. \emph{Коэффициентом корреляции $i$-й и $j$-й строки} назовём следующую величину:
\begin{equation}\label{row_corr}
\rho^r_{i,j} = \frac{(X^{1,i}, X^{2,j})_\bfS}{||X^{1,i}||_\bfS ||X^{2,j}||_\bfS},
\end{equation}
где $X^{k,i}$ --- $i$-я строчка матрицы $\bfX^k$, $k = 1, 2$, а $(\cdot, \cdot)_\bfS$ --- скалярное произведение с матрицей $\bfS$ в $\sfR^K$, определённая следующим образом: $(X, Y)_\bfS = ((X\bfS)^\sfT, Y^\sfT)$, так как $X$, $Y$ --- вектор-строчки, $|| \cdot ||_\bfS$ --- норма, порождённая этим скалярным произведением. Скажем, что ряды $\tsX_1$ и $\tsX_2$ \emph{слабо $\varepsilon$---разделимы}, если
\begin{equation}\label{weak_sep_eq}
\rho = \max(\max_{1 \le i,j \le K}|\rho^c_{i,j}|, \max_{1 \le i,j \le L}|\rho^r_{i,j}|) < \varepsilon.
\end{equation}
Нас будет интересовать порядок $\varepsilon$ при различных матрицах $\bfS$ и рядах $\tsX_1 = (c, c, \ldots)$ --- некоторая константа и $\tsX_2 = (\cos(2 \pi \omega k)), k = 1, 2, \ldots$, а так же при различных $L$ и $K$ при условии, что мы будем брать только $N = L + K - 1$ компонент ряда. Когда $\bfS$ --- единичная матрица, ответ известен: $\varepsilon$ имеет порядок $1/\min(L,K)$. Этот результат может быть найден в \cite{Golyandina.etal2001}.
\begin{lemma}
Пусть $\bfS$ определена аналогично Следствию \ref{zhigconseq}:  Если $h = N/L$ --- целое, тогда  $\bfS$ --- диагональная матрица со следующими диагональными элементами:
\begin{equation*}
s_{k,k} = \begin{cases}
1, & \text{если} \quad k = jL+1 \quad \text{для некоторых} \quad j = 0, \ldots, h-1, \\
\alpha, & \text{в противном случае}
\end{cases},
\end{equation*}
где $0 \le \alpha \le 1$. Тогда $\rho$ имеет порядок $1/\min(L, N/L+\alpha(K - N/L))$ при $L, K \to \infty$.
\end{lemma}
\begin{proof}
Необходимо оценить порядки следующих величин:
\begin{gather*}
\rho^c_{i,j} = \frac{\sum_{k=j}^{j + L - 1} \cos(2 \pi \omega k)}{\sqrt{L (\sum_{k=j}^{j + L - 1} \cos^2(2 \pi \omega k))}},\\ \rho^r_{i,j} = \frac{\sum_{k=1}^K s_{k,k}\cos(2 \pi \omega (j + k - 1))}{\sqrt{(\sum_{k=1}^K s_{k,k}) (\sum_{k=1}^K s_{k,k}\cos^2(2 \pi \omega (j + k - 1)))}}.
\end{gather*}
Для доказательства используем следующие факты:
\begin{gather*}
\sum_{k=1}^n \cos(ak + b) = \csc(a/2) \sin(an / 2) \cos \left(\frac{an + a + 2b}{2} \right), \\
\sum_{k=1}^n \cos^2(ak + b) = \frac{1}{4}(\csc(a) \sin(2an + a + 2b) -\\ - \csc(a)\sin(a + 2b) + 2n),
\end{gather*}
для любых вещественных $a, b$ и положительного целого $n$.
Таким образом, когда ряд $\tsX_2$ не представляет из себя константу, числитель в $\rho^c_{i,j}$ имеет порядок $O(1)$, а знаменатель --- $O(L)$. Первая часть доказана, и её доказательство целиком аналогично случаю, когда $\bfS$ --- единичная матрица.

Для доказательства второй части необходимо разбить суммы и в числителе, и в знаменателе согласно соответствующим значениям $s_{k,k}$:
\begin{gather*}
\sum_{k=1}^K s_{k,k}\cos(2 \pi \omega (j + k - 1)) = \sum_{\substack{1 \le k \le K \\ s_{k,k} = 1}}\cos(2 \pi \omega (j + k - 1)) +\\ \sum_{\substack{1 \le k \le K \\ s_{k,k} = \alpha}}\alpha \cos(2 \pi \omega (j + k - 1)) = O(1) + \alpha O(1) = O(1),\\
\sum_{k=1}^K s_{k,k} = N/L + \alpha(K - N/L),\\
\sum_{k=1}^K s_{k,k}\cos^2(2 \pi \omega (j + k - 1)) = \sum_{\substack{1 \le k \le K \\ s_{k,k} = 1}}\cos^2(2 \pi \omega (j + k - 1)) +\\ \sum_{\substack{1 \le k \le K \\ s_{k,k} = \alpha}}\alpha \cos^2(2 \pi \omega (j + k - 1)) = O(N/L) + \alpha O(K - N/L).
\end{gather*}
\end{proof}
\emph{Замечание:} для выполнения леммы дополнительно требуется, чтобы ряды, составленные из элементов $(j, j + L, \ldots, (h-1)L + j)$, где $1 \le j \le L$, также не представляли из себя константу, что эквивалентно тому, что $\omega \ne k/L$ для любого целого $k$.

Таким образом, разделимость константы и синуса становится хуже, чем при обычном варианте: при $\alpha$, близких к нулю, оптимальным выбором $L$ будет $L \approx \sqrt{N}$, и, таким образом, получаем порядок разделимости $1/\sqrt{N}$.
\begin{lemma}
Пусть $\bfS$ определена аналогично Утверждению \ref{myweightstat}. Тогда $\rho$ имеет порядок $\max \left(1/L, \frac{H_L}{\sqrt{NK}} \right)$ при $L, K \to \infty$, где $H_L$ --- $L$-е гармоническое число.
\end{lemma}
\begin{proof}
Необходимо оценить порядки следующих величин:
\begin{gather*}
\rho^c_{i,j} = \frac{\sum_{k=j}^{j + L - 1} \cos(2 \pi \omega k)}{\sqrt{L (\sum_{k=j}^{j + L - 1} \cos^2(2 \pi \omega k))}}, \\ \rho^r_{i,j} = \frac{\sum_{k=1}^K \hat s_{k,k}\cos(2 \pi \omega (j + k - 1))}{\sqrt{(\sum_{k=1}^K \hat s_{k,k}) (\sum_{k=1}^K \hat s_{k,k}\cos^2(2 \pi \omega (j + k - 1)))}}.
\end{gather*}
Доказательство порядка для $\rho^c_{i,j}$ целиком аналогично предыдущему пункту, поэтому сразу перейдем ко второму. Докажем корреляцию только первых строчек --- для остальных доказательство будет целиком аналогичным. Рассмотрим числитель $\rho^r_{1,1}$:
\begin{gather*}
\sum_{k=1}^K \hat s_{k,k}\cos(2 \pi \omega k) = \sum_{k=1}^{L-1} \hat s_{k,k}\cos(2 \pi \omega k) + \sum_{k=L}^{K - L + 1} \frac{\cos(2 \pi \omega k)}{L} +\\ \sum_{k=K - L + 2}^{K} \hat s_{k,k}\cos(2 \pi \omega k) = I_1 + I_2 + I_3,
\end{gather*}
который разбился на три части. Для центральной справедлива оценка $O(1/L)$, а для левой части доказательство аналогично правой.
\begin{gather*}
\bigg|\sum_{k=1}^{L-1}\frac{1}{L}\left(\frac{k}{L} + \sum_{j=k}^{L-1} \frac{1}{j} \right) \cos(2 \pi \omega k)\bigg| = \bigg|\sum_{k=1}^{L-1} \frac{k \cos(2 \pi \omega k)}{L^2} + \\ + \frac{1}{L}\sum_{k = 1}^{L-1}\sum_{j = k}^{L-1}\frac{\cos(2 \pi \omega k)}{j}\bigg| \le
\bigg|\sum_{k=1}^{L-1} \frac{k \cos(2 \pi \omega k)}{L^2}\bigg| + \bigg|\frac{1}{L}\sum_{k = 1}^{L-1}\sum_{j = k}^{L-1}\frac{\cos(2 \pi \omega k)}{j}\bigg|.
\end{gather*}
Используя тот факт, что
\begin{gather*}
\sum_{k=1}^n k \cos(ak + b) = -\frac{1}{4}\csc^2(a/2)(-(n+1)\cos(an+b) + \\ + n\cos(an + a + b) + \cos b),
\end{gather*}
получаем:
\begin{gather*}
\bigg|\sum_{k=1}^{L-1} \frac{k \cos(2 \pi \omega k)}{L^2}\bigg| = O(1/L), \quad
\bigg|\frac{1}{L}\sum_{k = 1}^{L-1}\sum_{j = k}^{L-1}\frac{\cos(2 \pi \omega k)}{j}\bigg| = \\ \bigg|\frac{1}{L}\sum_{j = 1}^{L-1}\sum_{k = 1}^{j}\frac{\cos(2 \pi \omega k)}{j}\bigg| \le \bigg|\frac{1}{L}\sum_{j = 1}^{L-1}\frac{\hat c}{j}\bigg| = O \left(\frac{H_L}{L} \right),
\end{gather*}
где $\hat c$ --- константа. Для знаменателя нужно рассмотреть следующие суммы:
\begin{equation*}
\sum_{k=1}^K \hat s_{k,k} = N / L
\end{equation*}
по определению, а следующую составляющую просто оценить снизу:
\begin{equation*}
\sum_{k=1}^K \hat s_{k,k}\cos^2(2 \pi \omega (j + k - 1)) \ge \sum_{k=1}^K \frac{1}{L}\cos^2(2 \pi \omega (j + k - 1)) = O \left(\frac{K}{L} \right).
\end{equation*}
\end{proof}
 

\end{document}
