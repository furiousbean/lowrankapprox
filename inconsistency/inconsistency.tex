\documentclass[12pt,a4paper]{article}

\usepackage[text={14cm,20cm}]{geometry}
\usepackage{amsmath,amssymb,amsthm,amscd,amsfonts}
\usepackage[utf8x]{inputenc}

\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}


\usepackage{euscript}
\usepackage{relsize}
\usepackage{mathdots}
\usepackage{graphicx}
%\usepackage{epstopdf}
\usepackage{indentfirst}
\usepackage{empheq}
\usepackage{multirow}

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\usepackage[colorlinks, urlcolor=blue, pdfborder={0 0 0 [0 0]}]{hyperref}

\hyphenation{Struc-tu-red}
\hyphenation{Ran-do-mized}
\hyphenation{Ma-xi-mi-za-tion}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\tr}{tr}
\providecommand*{\BibDash}{}

\def\rank{\mathop{\mathrm{rank}}}
\def\rev{\mathop{\mathrm{rev}}}
\def\diag{\mathop{\mathrm{diag}}}
\DeclareSymbolFont{bbold}{U}{bbold}{m}{n}
\DeclareSymbolFontAlphabet{\mathbbold}{bbold}

\newtheorem{corollary}{Следствие}
\newtheorem{proposition}{Предложение}
\newtheorem{algorithm}{Алгоритм}
\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{remark}{Замечание}
\newtheorem{problem}{Задача}

%\usepackage{euscript}

\input{letters}

\input{newcommands}
%\sectionfont{\centering}

%\subsectionfont{\centering}
%\subsubsectionfont{\normalsize}
%\setcounter{page}{1}

\begin{document}


\title{Исследование информационной матрицы в задаче оценивания сигнала конечного ранга}
\author{Н.К.~Звонарев}


%\address{Санкт-Петербургский государственный университет,\\ 
%	Российская Федерация, 199034, Санкт-Петербург, Университетская наб., 7/9}
% \email{nikitazvonarev@gmail.com}

\maketitle

Рассмотрим задачу оценивания сигнала $\tsS_N$ длины $N$ из зашумлённого сигнала $\tsX_N = \tsS_N + \tsY_N$, где $\tsS_N$ --- сигнал конечного ранга $r$, $\tsY_N$ --- подстрока стационарного процесса авторегрессии длины $N$. Ряд конечного ранга $\tsS_N = (s_0, \ldots, s_{N-1})$ порядка $r$ означает, что 
\begin{equation*}
s_n = \sum_{j = 1}^{r} a_j s_{n-j}, \quad n = r, \ldots, N - 1;\  a_r\neq 0.
\end{equation*}
Известно, что сигналы такого вида представимы в следующем параметрическом виде:
\begin{equation*} \label{task:parametricform}
s_n = \sum_j P_j(n) \exp(\alpha_j n) \cos(2 \pi \omega_j n + \psi_j),
\end{equation*}
где $P_j(n)$ --- многочлены от $n$. Ограничимся частным случаем, когда каждый многочлен представляет из себя константу, а так же слегка изменим параметризацию, расписав $\omega_j$, то есть
\begin{equation} \label{task:redparametricform}
s_n = \sum_j \exp(\alpha_j n) (p_{j, 1} \cos(2 \pi \omega_j n) + p_{j, 2} \sin(2 \pi \omega_j n)).
\end{equation}
Такие ряды могут быть записаны в комплексной форме в следующем виде:
\begin{equation} \label{task:compparametricform}
s_n = \sum_{j=1}^r c_j \mu_j^n = \sum_{j=1}^r c_j \exp(\lambda_j n),
\end{equation}
где $\mu_j = e^{\lambda_j}$, $\operatorname{Im}(\lambda_j) \in (-\pi; \pi]$, все $\mu_j$ различны, комплексные $\mu_i$ встречаются вместе со своим комплексным дополнением, при этом для $\mu_j = \overline \mu_k$: $c_j = \overline c_k$.

Задача данной работы --- исследовать границу Рао-Крамера для несмещённых оценок параметров сигнала, чтобы оценить снизу их порядок сходимости при увеличивающеся к бесконечности длине ряда $N$. В первой части работы будет приведён приём, позволяющий избавиться от сложной вещественной параметризации и перейти к более простой для расчётов комплексной. Во второй части работы получены порядки сходимости оценок констант $p_{j, \cdot}$ в случае, когда все $\mu_j$ известны. В этом случае модель является линейной, и порядки сходимости оценок точные. В третьей части выведены границы снизу на порядки сходимости оценок констант $p_{j, \cdot}$ и частот $\lambda_j$. В приложении приведены необходимые для получения результата вспомогательные утверждения.

Получены следующие результаты: если ряд содержит экспоненциально-угасающую компоненту, для которой $|\mu_i| < 1$, то для неё не существует несмещённой состоятельной оценки ни для константы, ни для частоты. В случае, если известны частоты, то скорость сходимости констант равна $|\mu_j|^{-N}$, если $|\mu_j|>1$, и $N^{-1/2}$ для $|\mu_j|=1$. Если частоты неизвестны и $|\mu_j|=1$, то оценка снизу на скорость сходимости частоты равна $N^{-3/2}$, а константы --- $N^{-1/2}$.

\section{Вещественная и комплексная параметризация}
\subsection{Известные результаты}
В общем случае рассмотрим следующую задачу оценивания: дана выборка $X = M + \varepsilon$ длины $N$, где $\varepsilon$ --- многомерный случайный нормальный вектор с нулевым матожиданием и ковариационной матрицей $\Sigma_N$, $M = M(\Theta)$ --- детерминированный вектор средних, зависящий от параметра $\Theta \in \sfR^K$, в некоей окрестности $\Theta$ отображение $M(\Theta)$ является хотя бы один раз дифференцируемым. Требуется оценить параметр $\Theta = (\theta_1, \ldots, \theta_K)^\rmT$.

Определим информационную матрицу $\bfI = (\calI_{j, k}) \in \sfR^{K \times K}$, где элементы $\calI_{j, k} = \frac{\partial M^\rmT(\Theta)}{\partial \theta_j} \Sigma_N^{-1} \frac{\partial M(\Theta)}{\partial \theta_k}$. Заметим, что если определить скалярное произведение $\langle X, Y \rangle_{\Sigma^N} = X^\rmT \Sigma_N^{-1} Y$, то матрица $\bfI$ --- это матрица Грама для набора векторов $ \{\frac{\partial M(\Theta)}{\partial \theta_k}, k = 1,; \ldots, K\}$. Для удобства введём матрицу $\bfM \in \sfR^{N \times K}$, $\bfM = [\frac{\partial M(\Theta)}{\partial \theta_1}:\ldots:\frac{\partial M(\Theta)}{\partial \theta_K}]$.

В частном случае известная теорема Рао-Крамера утверждает следующее:
\begin{theorem} \label{th:raokramer}
	Рассмотрим несмещённую оценку $\widehat \Theta$ параметра $\Theta$. Тогда дисперсионная матрица $D \widehat \Theta \ge \bfI^{-1}$ в том смысле, что для любого $Z \in R^K$: $Z^T D \widehat \Theta Z \ge Z^T \bfI^{-1} Z$.
\end{theorem}

\subsection{Применение к задаче оценивания рядов}
В случае задачи оценивания сигнала \eqref{task:redparametricform}: $M = (s_0, \ldots, s_{N-1})$, вектор параметров $\Theta$ и его размерность зависит от выбора параметризации и набора известных данных. Во-первых, в данной работе рассматриваются два случая: когда частоты $\mu_j$ известны, и когда нет. В этих случаях $K = r$ и $K = 2r$ соответственно. Во-вторых, вид выбранной в \eqref{task:redparametricform} параметризации прямо зависит от того, лежит ли очередное $\mu_j$ на вещественной прямой или нет: если $\operatorname{Im}(\mu_j) = 0$, то соответствующие $p_{j, 2} = 0$ и $\omega_j = 0$. Это не очень удобно, поэтому вместо этого рассматривается общая компексная параметризация \eqref{task:compparametricform}, в которой параметрами являются $c_k$ для случая известных частот, и $c_k$ вместе с $\lambda_k$ для неизвестных частот. 

Проблема же комплексной параметризации состоит в том, что для неё нельзя сформулировать результат, подобный \ref{th:raokramer}. Однако, можно заметить, что между вещественным и комплексным набором параметров существует непрерывное взаимнооднозначное в некоторой окрестности параметров $c_k$ и $\lambda_k$ отображение $Q:C^K \to C^K$ между вещественной параметризацией \eqref{task:redparametricform} и \eqref{task:compparametricform} (заметим, что вещественная параметризация \eqref{task:redparametricform} однозначно продолжается на комплексные числа). Невырожденный якобиан $\bfQ$ отображения $Q$ позволяет выразить матрицу $\bfM = \bfN \bfQ$, где $\bfN$ --- матрица частных производных по комплексным параметрам $c_k$ в первом и $c_k$ с $\lambda_k$ во втором случае: 
\begin{equation}\label{eq:Nwolambdas}
\bfN = [\frac{\partial \tsS^\rmT}{\partial c_1}:\ldots:\frac{\partial \tsS^\rmT}{\partial c_r}]
\end{equation}
и 
\begin{equation}\label{eq:N}
\bfN = [\frac{\partial \tsS^\rmT}{\partial c_1}:\ldots:\frac{\partial \tsS^\rmT}{\partial c_r}\frac{\partial \tsS^\rmT}{\partial \lambda_1}:\ldots:\frac{\partial \tsS^\rmT}{\partial \lambda_r}]
\end{equation}
соответственно. После этого можно ввести комплексный аналог информационной матрицы $\bfI$ --- матрицу $\bfJ$, определённую следующим образом:
\begin{equation}\label{eq:J}
\bfJ = \bfN^* \Sigma_N^{-1} \bfN,
\end{equation}
где под $\bfN^*$ обозначено эрмитово транспонирование матрицы $\bfN$. Удобно расширить определение скалярного произведения $\langle X, Y \rangle_{\Sigma_N}$ на комплексный случай как $\langle X, Y \rangle_{\Sigma^N} = X^* \Sigma_N^{-1} Y$, чтобы $\bfJ$ стала матрицей Грама столбцов матрицы $\bfN$.

Очевидно, что $\bfI = \bfQ^* \bfJ \bfQ$, и, таким образом, матрица $\bfJ$ невырождена тогда и только тогда, когда невырождена матрица $\bfI$, а порядок элементов в матрицах $\bfI^{-1}$ и $\bfJ^{-1}$ совпадает.

\section{Случай известных частот $\lambda_j$}
Для начала выявим вид матрицы $\bfN$ из \eqref{eq:Nwolambdas}.

\begin{proposition}
	Элементы матрицы $\bfN = (n_{j, k})$ имеют следующий вид:
	\begin{equation*}
	n_{j, k} = \mu_k^{j-1}.
	\end{equation*}
	Таким образом, матрица $\bfN$ --- комплексная матрица Вандермондта размера $N \times r$ для набора $\{\mu_1, \ldots, \mu_r\}$.
\end{proposition}
\begin{proof}
	Очевидно.
\end{proof}

Также обратим внимание на следующий факт.
\begin{remark}
	Модель данных \eqref{task:redparametricform} при известных $\lambda_k$ является линейной. Таким образом, полученная снизу граница на дисперсию оценки $\bfI^{-1}$, согласно теореме Гаусса-Маркова, являются точной для оценки по взвешенному методу наименьших квадратов: 
	\begin{equation*}
	\widehat \Theta = \bfI^{-1}\bfM^\rmT \Sigma_N^{-1} \tsX_N^\rmT  = \bfQ^{-1}\bfJ^{-1} \bfN^* \Sigma_N^{-1} \tsX_N^\rmT.
	\end{equation*}
\end{remark}

Вначале докажем результат, касающийся состоятельности этой оценки, для случая белого шума, а затем для общего случая процесса авторегрессии порядка $p$.

\begin{theorem} \label{th:wninconsistency}
	Пусть $\Sigma_N^{-1} = \frac{1}{\sigma^2}{\mathbf{1}}$, где $\mathbf{1}$ --- единичная матрица порядка $N \times N$, что соответствует случаю белого шума с дисперсией $\sigma^2$.
	
	Представим все $\mu_j$ в полярном виде: $\mu_j = \rho_j e^{i \varphi_j}$, $\varphi_j \in (-\pi, \pi]$. Пусть показатели экспонент упорядочены по невозрастанию модуля $\mu_j$, т.е. $\rho_j \ge \rho_{j+1}$. Показатели экспонент в этом случае будут разделены на три (возможно пустые) подстроки: те, у которых $|\mu_j|>1$, те, у которых $|\mu_j| = 1$ и те, у которых $|\mu_j| < 1$. Пусть соответствующее количество экспонент будет равно $r_A$ для $|\mu_j|>1$, $r_B$ для $|\mu_j|=1$ и $r_C$ для $|\mu_j|<1$.
	\begin{enumerate}
		\item Рассмотрим отнормированную матрицу $\widetilde \bfN = \bfN \bfL$, $\bfL = \diag(l_1, \ldots, l_r)$, 
		\begin{equation*}
		l_j = \begin{cases}
		\frac{1}{\sqrt{\langle N_j, N_j \rangle_{\Sigma_N}}} e^{-i N \varphi_j}, & \rho_j > 1, \\
		\frac{1}{\sqrt{\langle N_j, N_j \rangle_{\Sigma_N}}}, & \rho_j \le 1,
		\end{cases}
		\end{equation*}
		$N_j$ --- $j$-й столбец матрицы $\bfN$.
		
		Тогда \begin{equation*}
		\lim_{N \to + \infty} \widetilde \bfN^* \Sigma_N^{-1} \widetilde \bfN = \left( \begin{array}{c|c|c}
		\widetilde \bfA & \mathbf{0} & \mathbf{0} \\ \hline
		\mathbf{0} & \mathbf{1}_{r_B} & \mathbf{0} \\ \hline
		\mathbf{0} & \mathbf{0} & \widetilde \bfC
		\end{array}  \right),
		\end{equation*}
		где матрица $\widetilde \bfA = (\tilde a_{j, k})$ --- невырожденная квадратная матрица порядка $r_A$, элементы которой равны
		\begin{equation*}
		\tilde a_{j, k} = \frac{\sqrt{\mu_j \overline{\mu_j} - 1}\sqrt{\mu_k \overline{\mu_k} - 1}}{\overline{\mu_j} \mu_k - 1},
		\end{equation*}
		$\mathbf{1}_{r_B}$ --- единичная квадратная матрица порядка $r_B$, $\widetilde \bfC = (\tilde c_{j, k})$ --- невырожденная квадратная матрица порядка $r_C$, элементы которой равны
		\begin{equation*}
		\tilde c_{j, k} = \frac{\sqrt{1 - \mu_{j+r_A+r_B} \overline{\mu_{j+r_A+r_B}}}\sqrt{1 - \mu_{k+r_A+r_B} \overline{\mu_{k+r_A+r_B}}}}{1 - \overline{\mu_{j+r_A+r_B}} \mu_{k+r_A+r_B}}.
	    \end{equation*}
	    
	    \item 
	    \begin{equation*}
	    \lim_{N \to + \infty} \bfJ^{-1} = \sigma^2 \left( \begin{array}{c|c}
	    \mathbf{0}_{r_A + r_B} & \mathbf{0} \\ \hline
	    \mathbf{0} & \bfC^{-1}
	    \end{array}  \right),
	    \end{equation*}
	    где $\mathbf{0}_{r_A + r_B}$ --- нулевая матрица порядка $r_A + r_B$, $\bfC = (c_{j,k})$ --- невырожденная квадратная матрица порядка $r_C$, элементы которой равны
	    \begin{equation*}
	    c_{j, k} = \frac{1}{1 - \overline{\mu_{j+r_A+r_B}} \mu_{k+r_A+r_B}}.
	    \end{equation*}
	\end{enumerate}
\end{theorem}
	
	\begin{proof}
		Не умаляя общности: пусть $\sigma^2 = 1$. Достаточно рассмотреть соответствующие пределы последовательностей. Это легко сделать, используя следующую формулу: $\sum_{j = 0}^{N-1} x^j = \frac{x^N-1}{x-1}$, если $x \ne 1$, и $N$ в противном случае.
		\begin{itemize}
			\item Пусть $\rho_j > 1$, $\rho_k > 1$. Тогда
			\begin{multline*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_j^t} \mu_k^t e^{i N (\varphi_j - \varphi_k)}}{\sqrt{\sum_{u=0}^{N-1} \mu_j^u \overline{\mu_j^u}} \sqrt{\sum_{u=0}^{N-1} \mu_k^u \overline{\mu_k^u}}} = \frac{(\rho_j \rho_k)^N - e^{i N(\varphi_j - \varphi_k)}}{\rho_j \rho_k e^{i (\varphi_k - \varphi_j)} - 1} \frac{\sqrt{\rho_j^2 - 1}}{\sqrt{\rho_j^{2N} - 1}} \cdot \\ \cdot \frac{\sqrt{\rho_k^2 - 1}}{\sqrt{\rho_k^{2N} - 1}} \to \tilde a_{j, k}.
			\end{multline*}
			
			\item Пусть $\rho_j > 1$, $\rho_k = 1$. Тогда
			\begin{equation*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_j^t} \mu_k^t e^{i N \varphi_j}}{\sqrt{\sum_{u=0}^{N-1} \mu_j^u \overline{\mu_j^u}} \sqrt{\sum_{u=0}^{N-1} \mu_k^u \overline{\mu_k^u}}} = \frac{(\rho_j )^N e^{i N \varphi_k} - e^{i N \varphi_j}}{\rho_j e^{i (\varphi_k - \varphi_j)} - 1} \frac{\sqrt{\rho_j^2 - 1}}{\sqrt{\rho_j^{2N} - 1}} \frac{1}{\sqrt{N}} \to 0.
			\end{equation*}
			
			\item Пусть $\rho_j > 1$, $\rho_k < 1$. Учитывая, что $\rho_j \rho_k < \rho_j$, получаем \footnote{Здесь нет случая, когда $\rho_k = 1/\rho_j$. Результат целиком аналогичен.}
			\begin{multline*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_j^t} \mu_k^t e^{i N \varphi_j}}{\sqrt{\sum_{u=0}^{N-1} \mu_j^u \overline{\mu_j^u}} \sqrt{\sum_{u=0}^{N-1} \mu_k^u \overline{\mu_k^u}}} = \frac{(\rho_j \rho_k)^N e^{i N \varphi_k} - e^{i N \varphi_j}}{\rho_j \rho_k e^{i (\varphi_k - \varphi_j)} - 1} \frac{\sqrt{\rho_j^2 - 1}}{\sqrt{\rho_j^{2N} - 1}} \cdot \\ \cdot \sqrt{\frac{1 - \rho_k^2}{1 - \rho_k^{2N}}} \to 0.
			\end{multline*}
			
			\item Пусть $\rho_j = 1$, $\rho_k = 1$, $j \ne k$. Тогда
			\begin{equation*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_j^t} \mu_k^t}{\sqrt{\sum_{u=0}^{N-1} \mu_j^u \overline{\mu_j^u}} \sqrt{\sum_{u=0}^{N-1} \mu_k^u \overline{\mu_k^u}}} = \frac{e^{i N (\varphi_k -\varphi_j)} - 1}{e^{i (\varphi_k - \varphi_j)} - 1} \frac{1}{\sqrt{N}} \frac{1}{\sqrt{N}} \to 0.
			\end{equation*}
			
			\item Пусть $\rho_j = 1$, $\rho_k < 1$. Тогда
			\begin{equation*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_j^t} \mu_k^t}{\sqrt{\sum_{u=0}^{N-1} \mu_j^u \overline{\mu_j^u}} \sqrt{\sum_{u=0}^{N-1} \mu_k^u \overline{\mu_k^u}}} = \frac{\rho_k^N e^{i N (\varphi_k -\varphi_j)} - 1}{\rho_k e^{i (\varphi_k - \varphi_j)} - 1}  \frac{1}{\sqrt{N}} \frac{\sqrt{1 - \rho_k^2}}{\sqrt{1 - \rho_k^{2N}}} \to 0.
			\end{equation*}
			
			\item Пусть $\rho_j < 1$, $\rho_k < 1$. Тогда
			\begin{multline*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_j^t} \mu_k^t}{\sqrt{\sum_{u=0}^{N-1} \mu_j^u \overline{\mu_j^u}} \sqrt{\sum_{u=0}^{N-1} \mu_k^u \overline{\mu_k^u}}} = \frac{1 - (\rho_j \rho_k)^N e^{i N (\varphi_k -\varphi_j)}}{1 - \rho_j \rho_k e^{i (\varphi_k - \varphi_j)}} \cdot \\ \cdot \frac{\sqrt{1 - \rho_j^2}}{\sqrt{1 - \rho_j^{2N}}} \frac{\sqrt{1 - \rho_k^2}}{\sqrt{1 - \rho_k^{2N}}} \to \frac{\sqrt{1 - \mu_{j} \overline{\mu_{j}}}\sqrt{1 - \mu_{k} \overline{\mu_{k}}}}{1 - \overline{\mu_{j}} \mu_{k}}.
			\end{multline*}
				
		\end{itemize}
		
		Матрицы $\widetilde \bfA$, $\widetilde \bfC$ и $\bfC$ --- Коши-подобные матрицы (Cauchy-like matrix), при этом все они являются невырожденными, см. \cite{Schechter1959}.
		
		Пункт 2 следует из пункта 1 теоремы с учётом применённой с помощью матрицы $\bfL$ нормировки.
	\end{proof}

Теперь докажем теорему в случае, когда шум представляет из себя стационарный процесс авторегрессии порядка $p$. Доказательство сводится к случаю белого шума \ref{th:wninconsistency} через предельное представление ковариационной матрицы.

Рассмотрим $\tsY = (y_t, \; t~\in~\sfN)$ --- стационарный гауссовский процесс авторегрессии  порядка $p$:
\begin{equation} \label{def:arp}
y_t = \sum_{j = 1}^p \beta_j y_{t - j} + \sigma \varepsilon_t,
\end{equation}
где $\beta_1, \ldots, \beta_p$ --- коэффициенты авторегрессии, $\sigma$ --- стандартное отклонение шума, $(\varepsilon_t, t \in \sfN)$ --- независимые нормально распределённые случайные величины с нулевым средним и единичной дисперсией.

Определим следующее: вектор $B = (1, -\beta_1, \ldots, -\beta_p)^\rmT \in \sfR^{p+1}$, $B = (b_1, \ldots, b_{p+1})^\rmT$ и комплексный полином $f(z) = 1 - \sum_{j=1}^p \beta_j z^j$, случайный вектор $\tsY_N = (y_1, \ldots, y_N)^\rmT$ длины $N \ge 2p + 1$ --- подстрока процесса $\tsY$ длины $N$ и $\Sigma_N$ --- его ковариационная матрица.

\begin{theorem} \label{th:arinconsistency}
	Пусть $\Sigma_N^{-1}$ --- обратная ковариационная матрица процесса случайного вектора $\tsY$.
	
	Представим все $\mu_j$ в полярном виде: $\mu_j = \rho_j e^{i \varphi_j}$, $\varphi_j \in (-\pi, \pi]$. Пусть показатели экспонент упорядочены по невозрастанию модуля $\mu_j$, т.е. $\rho_j \ge \rho_{j+1}$. Показатели экспонент в этом случае будут разделены на три (возможно пустые) подстроки: те, у которых $|\mu_j|>1$, те, у которых $|\mu_j| = 1$ и те, у которых $|\mu_j| < 1$. Пусть соответствующее количество экспонент будет равно $r_A$ для $|\mu_j|>1$, $r_B$ для $|\mu_j|=1$ и $r_C$ для $|\mu_j|<1$.
	\begin{enumerate}
		\item Рассмотрим отнормированную матрицу $\widetilde \bfN = \bfN \bfL$, $\bfL = \diag(l_1, \ldots, l_r)$, 
		\begin{equation*}
		l_j = \begin{cases}
		\frac{1}{\sqrt{\langle N_j, N_j \rangle_{\Sigma_N}}} e^{-i N \varphi_j}, & \rho_j > 1, \\
		\frac{1}{\sqrt{\langle N_j, N_j \rangle_{\Sigma_N}}}, & \rho_j \le 1,
		\end{cases}
		\end{equation*}
		$N_j$ --- $j$-й столбец матрицы $\bfN$.
		
		Тогда \begin{equation*}
		\lim_{N \to + \infty} \widetilde \bfN^* \Sigma_N^{-1} \widetilde \bfN = \left( \begin{array}{c|c|c}
		\widetilde \bfA & \mathbf{0} & \mathbf{0} \\ \hline
		\mathbf{0} & \mathbf{1}_{r_B} & \mathbf{0} \\ \hline
		\mathbf{0} & \mathbf{0} & \widetilde \bfC
		\end{array}  \right),
		\end{equation*}
		где матрица $\widetilde \bfA = (\tilde a_{j, k})$ --- невырожденная квадратная матрица порядка $r_A$, элементы которой равны
		\begin{equation*}
		\tilde a_{j, k} = \frac{\sqrt{\mu_j \overline{\mu_j} - 1}\sqrt{\mu_k \overline{\mu_k} - 1} }{(\overline{\mu_j} \mu_k - 1)} \frac{\overline{f(1/\mu_j)} f(1/\mu_k) }{|f(1/\mu_j)| |f(1/\mu_k)|},
		\end{equation*}
		$\mathbf{1}_{r_B}$ --- единичная квадратная матрица порядка $r_B$, $\widetilde \bfC = (\tilde c_{j, k})$ --- невырожденная квадратная матрица порядка $r_C$, элементы которой равны
		\begin{multline*}
		\tilde c_{j, k} = \frac{\sqrt{1 - \mu_{j+r_A+r_B} \overline{\mu_{j+r_A+r_B}}}\sqrt{1 - \mu_{k+r_A+r_B} \overline{\mu_{k+r_A+r_B}}}}{1 - \overline{\mu_{j+r_A+r_B}} \mu_{k+r_A+r_B}} \cdot \\ \cdot \frac{\overline{f(\mu_{j+r_A+r_B})} f(\mu_{k+r_A+r_B})}{|f(\mu_{j+r_A+r_B})| |f(\mu_{k+r_A+r_B})|}.
		\end{multline*}
		
		\item 
	    \begin{equation*}
	    \lim_{N \to + \infty} \bfJ^{-1} = \sigma^2 \left( \begin{array}{c|c}
	    \mathbf{0}_{r_A + r_B} & \mathbf{0} \\ \hline
	    \mathbf{0} & \bfC^{-1}
	    \end{array}  \right),
	    \end{equation*}
	    где $\mathbf{0}_{r_A + r_B}$ --- нулевая матрица порядка $r_A + r_B$, $\bfC = (c_{j,k})$ --- невырожденная квадратная матрица порядка $r_C$, элементы которой равны
		\begin{equation*}
		c_{j, k} = \frac{\overline{f(\mu_{j + r_A + r_B})} f(\mu_{k + r_A + r_B})}{1 - \overline{\mu_{j+r_A+r_B}} \mu_{k+r_A+r_B}}.
		\end{equation*}
	\end{enumerate}
\end{theorem}

\begin{proof}
	Чтобы не пересчитывать заново все пределы, заметим следующее:
	\begin{itemize}
		\item Если $X$ и $Y$ --- два вектора, лежащие в $\sfZ^N$, то $\langle X, Y \rangle_{\Sigma_N} = X^* \Sigma_N^{-1} Y = \rev(X)^* \Sigma_N^{-1} \rev(Y)$, где $\rev(\cdot)$ --- оператор, разворачивающий вектор наоборот. Это следует из того, что $\Sigma_N^{-1}$ симметрична относительно побочной диагонали.
		\item Выполняется равенство $\langle X, Y \rangle_{\Sigma_N} = X^* \Sigma_N^{-1} Y = X^* \bfU_N^T \bfU_N Y$, где $\bfU_N$ --- верхнетреугольная матрица в разложении Холецкого матрицы $\Sigma_N^{-1}$. С достаточно большого $N$ можно применять результат теоремы \ref{th:invcov_chol}.
	\end{itemize}
	
	Пусть $X = (1, \nu, \ldots, \nu^{N-1})^\rmT$, $Y = (1, \xi, \ldots, \xi^{N-1})^\rmT$. Рассмотрим два случая (не умаляя общности, пусть $\sigma = 1$):
	\begin{itemize}
		\item $|\nu| > 1$. Тогда последовательности надо развернуть, получаем следующее:
		$X^* \Sigma_N^{-1} Y = \rev(X)^* \Sigma_N^{-1} \rev(Y) =  \rev(X)^* \bfU_N^T \bfU_N  \rev(Y) =$ \\$\sum_{t=0}^{N-{2p} -2} (\sum_{j=1}^{p+1}b_j \overline{\nu^{N - t - j}}) (\sum_{j=1}^{p+1}b_j \mu^{N - t - j}) + O(1) = $\\ $\sum_{t=0}^{N-{2p} - 2} (\overline{\nu^{N - t - 1}} \overline{f(1/\nu)})  (\mu^{N - t - 1}f(1/\mu)) + O(1) =$\\ $\overline{f(1/\nu)} f(1/\mu)\sum_{t=0}^{N-1} (\overline{\nu^{N - t - 1}} ) (\mu^{N - t - 1}) + O(1) = $\\
		$\overline{f(1/\nu)} f(1/\mu)\sum_{t=0}^{N-1} \overline{\nu^t} \mu^t + O(1)$.
		\item $|\nu| \le 1, |\mu| \le 1$. Последовательности разворачивать не нужно, имеем следующее:
		$X^* \Sigma_N^{-1} Y = X^* \bfU_N^T \bfU_N Y = \sum_{t=0}^{N-{2p}-2} (\sum_{j=1}^{p+1}b_j \overline{\nu^{
				t + j - 1}}) \cdot$ \\$\cdot (\sum_{j=1}^{p+1}b_j \mu^{t + j - 1}) + O((|\nu||\mu|)^{N - 2p - 1}) =$  
		$\sum_{t=0}^{N-{2p} -2} (\overline{\nu^t f(\nu)})(\mu^t f(\mu)) +$\\ $+ O((|\nu||\mu|)^{N})$ = 
		$\overline{f(\nu)} f(\mu) \sum_{t=0}^{N-1} \overline{\nu^t} \mu^t + O((|\nu||\mu|)^{N})$.
	\end{itemize}
	
	Далее доказательство аналогично теореме \ref{th:invcov_chol}. Заметим, что все знаменатели остаются корректными, и невырожденность соответствующих матриц сохраняется: $f(1/\mu_j)$ не может быть равно $0$ при $|\mu_j| > 1$, и $f(\mu_j)$ не может быть равно $0$ при $|\mu_j| \le 1$, потому что по условию теоремы авторегрессионный шум является стационарным, а, значит, справедлива лемма \ref{th:stat}.
\end{proof}

\begin{remark}
	Теоремы \ref{th:wninconsistency} и \ref{th:arinconsistency} также дают оценку на скорость сходимости констант $c_j$ в \eqref{task:compparametricform} (или $p_{j, \cdot}$ в \eqref{task:redparametricform}): $O(\mu_j^{-N})$ для $|\mu_j| > 1$, $O(\frac{1}{\sqrt{N}})$ для $|\mu_j| = 1$ и $O(1)$ для $|\mu_j| < 1$.
\end{remark}

\section{Случай неизвестных частот $\lambda_j$}
Для начала выявим вид матрицы $\bfN$ из \eqref{eq:N}.

\begin{proposition}
	Элементы матрицы $\bfN = (n_{j, k})$ имеют следующий вид:
	\begin{equation*}
	n_{j, k} = \begin{cases}
	\mu_k^{j-1}, & 1 \le k \le r, \\
	c_k (j - 1) \mu_k^{j-1}, & r + 1 \le k \le 2r. \\	
	\end{cases}
	\end{equation*}
\end{proposition}
\begin{proof}
	Очевидно.
\end{proof}

Заметим, что всё определено корректно: если хотя бы одно число из пары $p_{j, 1}$, $p_{j, 2}$ из \eqref{task:redparametricform} ненулевое, то оба соответствующие ему $c_k$ и $c_{k+1} = \overline{c_k}$ из \eqref{task:compparametricform} ненулевые.

К сожалению, применить такой же подход к доказательству, как в предыдущей части, не представляется возможным из-за вырожденности отнормированной матрицы $\widetilde \bfN^* \Sigma_N^{-1} \widetilde \bfN$, если хотя бы одно из $|\mu_j|>1$. К тому же, полученный результат слишком громоздок. Вместо этого, рассмотрим по-отдельности случай, когда ряд состоит из возрастающих ($|\mu_j| > 1$), затухающих ($|\mu_j| < 1$) и стационарных ($|\mu_j| = 1$) компонент.

\subsection{Случай возрастающей компоненты}
Рассмотрим простейший случай: пусть ряд представляет из себя одну экспоненциальную компоненту $s_t = \mu^t$, $\mu>1$ --- вещественное, шум --- белый, и, не умаляя общности, с $\sigma = 1$. Матрица $\bfN$ имеет следующий вид:
\begin{equation*}
	n_{j, k} = \begin{cases}
	\mu^{j-1}, & k = 1, \\
	(j - 1) \mu^{j-1}, & k = 2. \\	
	\end{cases}
\end{equation*}

Матрицу $\widetilde \bfN$, аналогично теореме \ref{th:wninconsistency}, определим как:
\begin{equation*}
\widetilde \bfN = \bfN \cdot \left( \begin{array}{c|c}
(\sum_{j = 0}^{N-1} \mu^{2j})^{-1/2} & 0 \\ \hline
0 & (\sum_{j = 0}^{N-1} j^2 \mu^{2j})^{-1/2}
\end{array}  \right).
\end{equation*}

Пользуясь следующими формулами:
\begin{gather*}
\sum_{t = 0}^{N-1} t \mu^t = \frac{\mu + (N-1)\mu^{N+1} - N \mu^N}{(\mu-1)^2}, \\
\sum_{t = 0}^{N-1} t^2 \mu^t = \frac{-\mu^2 - \mu + N^2 \mu^N + (2N - 2 N^2 + 1) \mu^{N+1}  + (N^2 - 2N + 1) \mu^{N + 2} }{(\mu-1)^3},
\end{gather*}
получаем, что
\begin{equation*}
\lim_{N \to +\infty} \widetilde \bfN^* \widetilde \bfN = \left(\begin{array}{c|c}
1 & 1 \\ \hline
1 & 1
\end{array} \right),
\end{equation*}
которая, очевидно, является вырожденной матрицей. Поэтому метод теоремы \ref{th:wninconsistency} не работает. Вместо этого, обратим матрицу $\bfJ = \bfN^* \bfN$ с помощью метода Крамера. Верно следующее:
\begin{equation*}
	\det \bfJ = \frac{\mu^{4N + 2} + \mu^{2N}(-N^2 + (2N^2-2)\mu^2 - N^2\mu^4) + \mu^2}{(\mu-1)^4},
\end{equation*}
что означает, что при достаточно большом $N$ матрица $\bfJ$ невырождена. Таким образом, получаем следующие оценки на порядок обратной матрицы:
\begin{equation*}
\bfJ^{-1} = \left(\begin{array}{c|c}
O(\frac{N^2}{\mu^{2N}}) & O(\frac{N}{\mu^{2N}}) \\ \hline
O(\frac{N}{\mu^{2N}}) & O(\frac{1}{\mu^{2N}})
\end{array} \right),
\end{equation*}
из чего следует, что скорость сходимости константы падает по сравнению со случаем известной частоты, и ограничена снизу порядком $O(N\mu^{-N})$,
а скорость сходимости частоты ограничена снизу $O(\mu^{-N})$.

\subsection{Случай угасающих компонент}
Пусть все $|\mu_j| < 1$. Для простоты ограничимся случаем белого шума. Заметим, что в этом случае все элементы матрицы $\bfJ = \bfN^* \bfN$ имеют конечный предел, следовательно, если $\lim_{N \to +\infty} \bfJ^{-1}$ существует, то он не может быть нулевым. Из этого следует отсутствие состоятельности у несмещённой оценки и для случая неизвестных частот.

\subsection{Случай стационарных компонент}
Если все $|\mu_j| = 1$, то можно сформулировать результат, аналогичный теореме \ref{th:wninconsistency}.

\begin{theorem}\label{th:freqconsistency}
Пусть $\Sigma_N^{-1} = \frac{1}{\sigma^2}{\mathbf{1}}$, где $\mathbf{1}$ --- единичная матрица порядка $N \times N$, что соответствует случаю белого шума с дисперсией $\sigma^2$.

\begin{enumerate}
	\item Рассмотрим отнормированную матрицу $\widetilde \bfN = \bfN \bfL$, $\bfL = \diag(l_1, \ldots, l_r)$, 
	$l_j =\frac{1}{\sqrt{\langle N_j, N_j \rangle_{\Sigma_N}}}$, $N_j$ --- $j$-й столбец матрицы $\bfN$.
	
	Тогда \begin{equation*}
	\lim_{N \to + \infty} \widetilde \bfN^* \Sigma_N^{-1} \widetilde \bfN = \left( \begin{array}{c|c}
	\mathbf{1}_r & \frac{\sqrt{3}}{2} \mathbf{1}_r  \\ \hline
	\frac{\sqrt{3}}{2} \mathbf{1}_r & \mathbf{1}_r
	\end{array}  \right),
	\end{equation*}
	где матрица $\mathbf{1}_{r}$ --- единичная квадратная матрица порядка $r$.
	
	\item 
	\begin{equation*}
	\lim_{N \to + \infty} \bfJ^{-1} = \mathbf{0}_{2r},
	\end{equation*}
	где $\mathbf{0}_{2r}$ --- нулевая матрица порядка $2r$.
\end{enumerate}
\end{theorem}
\begin{proof}
	Доказательство теоремы строится аналогично доказательству теоремы \ref{th:wninconsistency} с использованием вышеупомянутых формул для сумм $\sum_{t=0}^{N-1} t x^t$ и $\sum_{t=0}^{N-1} t^2 x^t$.
	
	Значения для недиагональных клеток матрицы $\lim_{N \to + \infty} \widetilde \bfN^* \Sigma_N^{-1} \widetilde \bfN$ --- следствие из следующих формул:
	\begin{gather*}
	\sum_{t=0}^{N-1} t = \frac{N(N-1)}{2}, \\
	\sum_{t=0}^{N-1} t^2 = \frac{N(N-1)(2N-1)}{6},
	\end{gather*}
	причём эта матрица является невырожденной, так как перестановкой столбцов со строками сводится к блочно-диагональной с невырожденными блоками $\left( \begin{array}{c|c}
	1 & \sqrt{3}/2  \\ \hline
	\sqrt{3} / 2 & 1
	\end{array}  \right)$.
	Из этих же формул следует и пункт 2 теоремы.
\end{proof}

Аналогичный результат формулируется и для случая красного шума:
\begin{theorem}\label{th:freqrnconsistency}
	Пусть $\Sigma_N^{-1}$ --- обратная ковариационная матрица процесса случайного вектора $\tsY$.
	
	\begin{enumerate}
		\item Рассмотрим отнормированную матрицу $\widetilde \bfN = \bfN \bfL$, $\bfL = \diag(l_1, \ldots, l_r)$, 
		$l_j =\frac{1}{\sqrt{\langle N_j, N_j \rangle_{\Sigma_N}}}$, $N_j$ --- $j$-й столбец матрицы $\bfN$.
		
		Тогда \begin{equation*}
		\lim_{N \to + \infty} \widetilde \bfN^* \Sigma_N^{-1} \widetilde \bfN = \left( \begin{array}{c|c}
		\mathbf{1}_r & \frac{\sqrt{3}}{2} \mathbf{1}_r  \\ \hline
		\frac{\sqrt{3}}{2} \mathbf{1}_r & \mathbf{1}_r
		\end{array}  \right),
		\end{equation*}
		где матрица $\mathbf{1}_{r}$ --- единичная квадратная матрица порядка $r$.
		
		\item 
		\begin{equation*}
		\lim_{N \to + \infty} \bfJ^{-1} = \mathbf{0}_{2r},
		\end{equation*}
		где $\mathbf{0}_{2r}$ --- нулевая матрица порядка $2r$.
	\end{enumerate}
\end{theorem}

\begin{proof}
	Доказательство теоремы сводится к доказательству теоремы \ref{th:freqconsistency} так же, как доказательство \ref{th:arinconsistency} сводится к \ref{th:wninconsistency}.
	
	Пусть $X = (1, \nu, \ldots, \nu^{N-1})^\rmT$, $Y = (0, \nu, 2 \nu^2, 3 \nu^3 \ldots, (N - 1)\nu^{N-1})^\rmT$, $Z = (0, \xi, 2 \xi^2, 3 \xi^3 \ldots, (N - 1)\xi^{N-1})^\rmT$, $|\xi| = |\mu| = 1$. 
	Рассмотрим следующие формулы (не умаляя общности, пусть $\sigma = 1$):
	
	$X^* \Sigma_N^{-1} Z = X^* \bfU_N^T \bfU_N Z = $ \\
	$\sum_{t=0}^{N-{2p}-2} (\sum_{j=1}^{p+1}b_j \overline{\nu^{
	t + j - 1}}) (\sum_{j=1}^{p+1}b_j (t + j - 1) \mu^{t + j - 1}) + O(N) =$ \\ 
    $\sum_{t=0}^{N-{2p}-2} (\sum_{j=1}^{p+1}b_j \overline{\nu^{
		t + j - 1}}) (\sum_{j=1}^{p+1}b_j t \mu^{t + j - 1}) + \sum_{t=0}^{N-{2p}-2} (\sum_{j=1}^{p+1}b_j \overline{\nu^{
		t + j - 1}}) (\sum_{j=1}^{p+1}b_j (j-1) \mu^{t + j - 1}) + O(N) =$ 
	$\sum_{t=0}^{N-{2p} -2} t (\overline{\nu^t f(\nu)})(\mu^t f(\mu)) + O(\sum_{t=1}^{N-1} (\mu \nu)^t) + O(N)$ = \\
	$\overline{f(\nu)} f(\mu) \sum_{t=0}^{N-1} t \overline{\nu^t} \mu^t + O(\sum_{t=1}^{N-1} (\mu \nu)^t) + O(N)$.
	
	Аналогичным приёмом доказывается и следующая формула:
	$Y^* \Sigma_N^{-1} Z = Y^* \bfU_N^T \bfU_N Z = $ \\
	$\sum_{t=0}^{N-{2p}-2} (\sum_{j=1}^{p+1}b_j (t + j - 1) \overline{\nu^{
			t + j - 1}}) (\sum_{j=1}^{p+1}b_j (t + j - 1) \mu^{t + j - 1}) + O(N^2) =$ \\ 
	$\sum_{t=0}^{N-{2p}-2} (\sum_{j=1}^{p+1}b_j t \overline{\nu^{
			t + j - 1}}) (\sum_{j=1}^{p+1}b_j t \mu^{t + j - 1}) + O(\sum_{t=1}^{N-1} t (\mu \nu)^t) + O(N^2) =$ \\
	$\sum_{t=0}^{N-{2p} -2} t^2 (\overline{\nu^t f(\nu)})(\mu^t f(\mu)) + O(\sum_{t=1}^{N-1} t (\mu \nu)^t) + O(N^2)$ = \\
	$\overline{f(\nu)} f(\mu) \sum_{t=0}^{N-1} t^2 \overline{\nu^t} \mu^t + O(\sum_{t=1}^{N-1} t (\mu \nu)^t) + O(N^2)$.
	
	Из чего следует равенство всех пределов.
\end{proof}

\begin{remark}
	Теоремы \ref{th:freqconsistency} и \ref{th:freqrnconsistency} также дают оценку снизу на скорость сходимости констант $c_j$ и частот $\lambda_j$ в \eqref{task:compparametricform} (или $p_{j, \cdot}$ с $\omega_j$ и $\alpha_j$ в \eqref{task:redparametricform}):
	$O(\frac{1}{\sqrt{N}})$ для $c_j$ и $O(\frac{1}{\sqrt{N^3}})$ для $\lambda_j$.
\end{remark}

\begin{remark}
	Нормировка $\bfL$ в Теореме \ref{th:freqrnconsistency} также даёт представление о том, что среднеквадратическое отклонение оценки, связанной с частотой $\mu_j$, возрастает примерно в $1/|f(\mu_j)|$ при оценивании в случае авторегрессионного шума по сравнению с оцениванием в случае красного шума при одном и том же параметре шума $\sigma^2$.
\end{remark}

\section*{Заключение}
Была рассмотрена задача оценивания сигнала с точки зрения границы Рао-Крамера на диспресию несмещённых оценок её параметров. Отвергнута состоятельность несмещённых оценок при наличии в ряде затухающих компонент, т.е. тех, у которых $|\mu_j| < 1$ в \ref{task:compparametricform}. В то же время, найдена точная граница оценивания в случае известных частот, и граница снизу, если частоты неизвестны. Ключевые теоремы доказаны и в случае белого шума, и в случае стационарного авторегресионного шума порядка $p$.

\appendix
\section{Сведения из теории авторегрессионных процессов}

\begin{lemma}\label{th:stat}
	Процесс авторегресии $\tsY$, определённый в \eqref{def:arp}, является стационарным тогда и только тогда, когда все корни полинома $f(z)$ по модулю строго больше $1$: $f(z) = 0$ в том и только в том случае, если $|z| > 1$.
\end{lemma}

\begin{theorem}\label{th:invcov}
	Рассмотрим случайный вектор $\tsY_N = (y_1, \ldots, y_N)^\rmT$ длины $N \ge 2p + 1$ --- подстрока процесса $\tsY$ длины $N$ и $\Sigma_N$ --- его ковариационную матрицу. Тогда обратная ковариационная матрица $\Sigma_N^{-1} = (s_{l, j; N})$ равна:
	\begin{equation*}
	s_{l, j; N} = \begin{cases}
	0, & |l - j| > p, \\
	\frac{1}{\sigma^2}\sum_{k = 1}^{\min(l, N + 1 - j, p + 1 - |l - j|)}b_k b_{k + |l - j|}, & l \le j, \\
	\frac{1}{\sigma^2}\sum_{k = 1}^{\min(j, N + 1 - l, p + 1 - |l - j|)}b_k b_{k + |l - j|}, & l > j.
	\end{cases}
	\end{equation*}
\end{theorem}
\begin{proof}
	Утверждение теоремы --- переписанный в другом виде результат, полученный в работе \cite{Verbyla1985}.
\end{proof}

\begin{theorem}\label{th:invcov_chol}
	В терминах теоремы \ref{th:invcov}: рассмотрим обратную ковариационную матрицу $\Sigma_N^{-1}$, где $N \ge 2p + 2$ и её разложение Холецкого $\Sigma_N^{-1} = \bfU^T_N \bfU_N$, $\bfU_N$ --- верхнетреугольная матрица порядка $N$. Тогда матрица $\bfU_N$ имеет следующий вид:
	\begin{equation*}
	\bfU_N = \frac{1}{\sigma} \begin{pmatrix}
	b_1 & b_2 & \ldots & b_{p+1} &  &  &  &  \\ 
	& b_1 & b_2 & \ldots & b_{p+1} &  &  &  \\ 
	&  & \ddots & \ddots & \ddots & \ddots &  &  \\ 
	&  &  & b_1 & b_2 & \ldots & b_{p+1} &  \\ 
	&  &  &  & \multicolumn{4}{c}{\multirow{4}{*}{$\bfU_{2p+1}$}} \\
	&  &  &  & \multicolumn{4}{c}{\multirow{4}{*}{}} \\
	&  &  &  & \multicolumn{4}{c}{\multirow{4}{*}{}} \\
	&  &  &  & \multicolumn{4}{c}{\multirow{4}{*}{}}	
	\end{pmatrix},
	\end{equation*}
	где количество строчек в верхнем блоке равно $N - (2p + 1)$, $\bfU_{2p+1}$ --- разложение Холецкого матрицы $\Sigma_{2p+1}^{-1}$: $\Sigma_{2p+1}^{-1} = \bfU_{2p+1}^\rmT \bfU_{2p+1}$.
\end{theorem}
\begin{proof}
	Заметим, что разложение Холецкого для матрицы $\bfU_N$, $N \ge 2p + 2$, можно представить, как
	\begin{equation*}
	\bfU_N = \frac{1}{\sigma} \begin{pmatrix}
	b_1 & b_2 & \ldots & b_{p+1} &  \\ 
	& \multicolumn{4}{c}{\multirow{4}{*}{$\bfU_{N - 1}$}} \\
	& \multicolumn{4}{c}{\multirow{4}{*}{}} \\
	& \multicolumn{4}{c}{\multirow{4}{*}{}} \\
	& \multicolumn{4}{c}{\multirow{4}{*}{}}	
	\end{pmatrix},
	\end{equation*}	
	где $\bfU_{N-1}$ --- разложение Холецкого для матрицы $\Sigma_{N-1}^{-1}$. Это следствие вида матрицы $\Sigma_{N}$ из теоремы \ref{th:invcov} и алгоритма вычисления разложения Холецкого. Далее доказательство продолжается по индукции. \cite{stoica2005spectral}
\end{proof}

\bibliographystyle{gost705}
\bibliography{zvonarev}
\end{document}
