\documentclass[12pt,a4paper]{article}

\usepackage[text={14cm,20cm}]{geometry}
\usepackage{amsmath,amssymb,amsthm,amscd,amsfonts}
\usepackage[utf8x]{inputenc}

\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}


\usepackage{euscript}
\usepackage{relsize}
\usepackage{mathdots}
\usepackage{graphicx}
%\usepackage{epstopdf}
\usepackage{indentfirst}
\usepackage{empheq}
\usepackage{multirow}

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\usepackage[colorlinks, urlcolor=blue, pdfborder={0 0 0 [0 0]}]{hyperref}

\hyphenation{Struc-tu-red}
\hyphenation{Ran-do-mized}
\hyphenation{Ma-xi-mi-za-tion}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\tr}{tr}
\providecommand*{\BibDash}{}

\def\rank{\mathop{\mathrm{rank}}}
\def\rev{\mathop{\mathrm{rev}}}
\DeclareSymbolFont{bbold}{U}{bbold}{m}{n}
\DeclareSymbolFontAlphabet{\mathbbold}{bbold}

\newtheorem{corollary}{Следствие}
\newtheorem{proposition}{Предложение}
\newtheorem{algorithm}{Алгоритм}
\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{remark}{Замечание}
\newtheorem{problem}{Задача}

%\usepackage{euscript}

\input{letters}

\input{newcommands}
%\sectionfont{\centering}

%\subsectionfont{\centering}
%\subsubsectionfont{\normalsize}
%\setcounter{page}{1}

\begin{document}


\title{Исследование информационной матрицы в задаче оценивания сигнала конечного ранга}
\author{Н.К.~Звонарев}


%\address{Санкт-Петербургский государственный университет,\\ 
%	Российская Федерация, 199034, Санкт-Петербург, Университетская наб., 7/9}
% \email{nikitazvonarev@gmail.com}

\maketitle

Рассмотрим задачу оценивания сигнала $\tsS_N$ длины $N$ из зашумлённого сигнала $\tsX_N = \tsS_N + \tsY_N$, где $\tsS_N$ --- сигнал конечного ранга $r$, $\tsY_N$ --- подстрока стационарного процесса авторегрессии длины $N$. Ряд конечного ранга $\tsS_N = (s_0, \ldots, s_{N-1})$ порядка $r$ означает, что 
\begin{equation*}
s_n = \sum_{j = 1}^{r} a_j s_{n-j}, \quad n = r, \ldots, N - 1;\  a_r\neq 0.
\end{equation*}
Известно, что сигналы такого вида представимы в следующем параметрическом виде:
\begin{equation*} \label{task:parametricform}
s_n = \sum_j P_j(n) \exp(\alpha_j n) \cos(2 \pi \omega_j n + \psi_j),
\end{equation*}
где $P_j(n)$ --- многочлены от $n$. Ограничимся частным случаем, когда каждый многочлен представляет из себя константу, то есть
\begin{equation} \label{task:redparametricform}
s_n = \sum_j p_j \exp(\alpha_j n) \cos(2 \pi \omega_j n + \psi_j).
\end{equation}
Такие ряды могут быть записаны в комплексной форме в следующем виде:
\begin{equation} \label{task:compparametricform}
s_n = \sum_{j=1}^r c_j \mu_j^n = \sum_{j=1}^r c_j \exp(\lambda_j n),
\end{equation}
где $\mu_j = e^{\lambda_j}$, $\operatorname{Im}(\lambda_j) \in (-\pi; \pi]$, все $\mu_j$ различны, комплексные $\mu_i$ встречаются вместе со своим комплексным дополнением, при этом для $\mu_j = \overline \mu_k$: $c_j = \overline c_k$.

Задача данной работы --- исследовать границу Рао-Крамера для несмещённых оценок параметров сигнала, чтобы оценить снизу их порядок сходимости при увеличивающеся к бесконечности длине ряда $N$. В первой части работы будет приведён приём, позволяющий избавиться от сложной вещественной параметризации и перейти к более простой для расчётов комплексной. Во второй части работы получены порядки сходимости оценок констант $p_i$ в случае, когда все $\mu_j$ известны. В этом случае модель является линейной, и порядки сходимости оценок точные. В третьей части выведены границы снизу на порядки сходимости оценок констант $p_j$ и частот $\lambda_j$. В четвёртой части приведены необходимые для получения результата вспомогательные утверждения.

Получены следующие результаты: если ряд содержит экспоненциально-угасающую компоненту, для которой $|\mu_i| < 1$, то для неё не существует несмещённой состоятельной оценки ни для константы, ни для частоты. В случае, если известны частоты, то скорость сходимости констант равна $|\mu_j|^{-N}$, если $|\mu_j|>1$, и $N^{-1/2}$ для $|\mu_j|=1$. Если частоты неизвестны и $|\mu_j|=1$, то оценка снизу на скорость сходимости частоты равна $N^{-3/2}$, а константы --- $N^{-1/2}$.

\section{Вещественная и комплексная параметризация}
\subsection{Известные результаты}
В общем случае рассмотрим следующую задачу оценивания: дана выборка $X = M + \varepsilon$ длины $N$, где $\varepsilon$ --- многомерный случайный нормальный вектор с нулевым матожиданием и ковариационной матрицей $\Sigma_N$, $M = M(\Theta)$ --- детерминированный вектор средних, зависящий от параметра $\Theta \in \sfR^K$, в некоей окрестности $\Theta$ отображение $M(\Theta)$ является хотя бы один раз дифференцируемым. Требуется оценить параметр $\Theta = (\theta_1, \ldots, \theta_K)^\rmT$.

Определим информационную матрицу $\bfI = (\calI_{j, k}) \in \sfR^{K \times K}$, где элементы $\calI_{j, k} = \frac{\partial M^\rmT(\Theta)}{\partial \theta_j} \Sigma_N^{-1} \frac{\partial M(\Theta)}{\partial \theta_k}$. Заметим, что если определить скалярное произведение $\langle X, Y \rangle_{\Sigma^N} = X^\rmT \Sigma_N^{-1} Y$, то матрица $\bfI$ --- это матрица Грама для набора векторов $ \{\frac{\partial M(\Theta)}{\partial \theta_k}, k = 1,; \ldots, K\}$. Для удобства введём матрицу $\bfM \in \sfR^{N \times K}$, $\bfM = [\frac{\partial M(\Theta)}{\partial \theta_1}:\ldots:\frac{\partial M(\Theta)}{\partial \theta_K}]$.

В частном случае известная теорема Рао-Крамера утверждает следующее:
\begin{theorem} \label{th:raokramer}
	Рассмотрим несмещённую оценку $\widehat \Theta$ параметра $\Theta$. Тогда дисперсионная матрица $D \widehat \Theta \ge \bfI^{-1}$ в том смысле, что для любого $Z \in R^K$: $Z^T D \widehat \Theta Z \ge Z^T \bfI^{-1} Z$.
\end{theorem}

\subsection{Применение к задаче оценивания рядов}
В случае задачи оценивания сигнала \eqref{task:redparametricform}: $M = (s_0, \ldots, s_{N-1})$, вектор параметров $\Theta$ и его размерность зависит от выбора параметризации и набора известных данных. Во-первых, в данной работе рассматриваются два случая: когда частоты $\mu_j$ известны, и когда нет. В этих случаях $K = r$ и $K = 2r$ соответственно. Во-вторых, вид выбранной в \eqref{task:redparametricform} параметризации прямо зависит от того, лежит ли очередное $\mu_j$ на вещественной прямой или нет: если $\operatorname{Im}(\mu_j) = 0$, то соответствующие $\omega_j = 0$ и $\theta_j = 0$. Это не очень удобно, поэтому вместо этого рассматривается общая компексная параметризация \eqref{task:compparametricform}, в которой параметрами являются $c_k$ для случая известных частот, и $c_k$ вместе с $\lambda_k$ для неизвестных частот. 

Проблема же комплексной параметризации состоит в том, что для неё нельзя сформулировать результат, подобный \ref{th:raokramer}. Однако, можно заметить, что между вещественным и комплексным набором параметров существует непрерывное взаимнооднозначное в некоторой окрестности параметров $c_k$ и $\lambda_k$ отображение $Q:C^K \to C^K$ между вещественной параметризацией \eqref{task:redparametricform} и \eqref{task:compparametricform} (заметим, что вещественная параметризация \eqref{task:redparametricform} однозначно продолжается на комплексные числа). Невырожденный якобиан $\bfQ$ отображения $Q$ позволяет выразить матрицу $\bfM = \bfN \bfQ$, где $\bfN$ --- матрица частных производных по комплексным параметрам $c_k$ в первом и $c_k$ с $\lambda_k$ во втором случае: 
\begin{equation}\label{eq:Nwolambdas}
\bfN = [\frac{\partial \tsS^\rmT}{\partial c_1}:\ldots:\frac{\partial \tsS^\rmT}{\partial c_r}]
\end{equation}
и 
\begin{equation}\label{eq:N}
\bfN = [\frac{\partial \tsS^\rmT}{\partial c_1}:\ldots:\frac{\partial \tsS^\rmT}{\partial c_r}\frac{\partial \tsS^\rmT}{\partial \lambda_1}:\ldots:\frac{\partial \tsS^\rmT}{\partial \lambda_r}]
\end{equation}
соответственно. После этого можно ввести комплексный аналог информационной матрицы $\bfI$ --- матрицу $\bfJ$, определённую следующим образом:
\begin{equation}\label{eq:J}
\bfJ = \bfN^* \Sigma_N^{-1} \bfN,
\end{equation}
где под $\bfN^*$ обозначено эрмитово транспонирование матрицы $\bfN$. Удобно расширить определение скалярного произведения $\langle X, Y \rangle_{\Sigma_N}$ на комплексный случай как $\langle X, Y \rangle_{\Sigma^N} = X^* \Sigma_N^{-1} Y$, чтобы $\bfJ$ стала матрицей Грама столбцов матрицы $\bfN$.

Очевидно, что $\bfI = \bfQ^* \bfJ \bfQ$, и, таким образом, матрица $\bfJ$ невырождена тогда и только тогда, когда невырождена матрица $\bfI$, а порядок элементов в матрицах $\bfI^{-1}$ и $\bfJ^{-1}$ совпадает.

\section{Случай известных частот $\lambda_j$}
Для начала выявим вид матрицы $\bfN$ из \eqref{eq:Nwolambdas}.

\begin{proposition}
	Элементы матрицы $\bfN = (n_{j, k})$ имеют следующий вид:
	\begin{equation*}
	n_{j, k} = \mu_k^{j-1}.
	\end{equation*}
	Таким образом, матрица $\bfN$ --- комплексная матрица Вандермондта размера $N \times r$ для набора $\{\mu_1, \ldots, \mu_r\}$.
\end{proposition}
\begin{proof}
	Очевидно.
\end{proof}

Также обратим внимание на следующий факт.
\begin{remark}
	Модель данных \eqref{task:redparametricform} при известных $\lambda_k$ является линейной. Таким образом, полученная снизу граница на дисперсию оценки $\bfI^{-1}$, согласно теореме Гаусса-Маркова, являются точной для оценки по взвешенному методу наименьших квадратов: 
	\begin{equation*}
	\widehat \Theta = \bfI^{-1}\bfM^\rmT \Sigma_N^{-1} \tsX_N^\rmT  = \bfQ^{-1}\bfJ^{-1} \bfN^* \Sigma_N^{-1} \tsX_N^\rmT.
	\end{equation*}
\end{remark}

Вначале докажем результат, касающийся состоятельности этой оценки, для случая белого шума, а затем для общего случая процесса авторегрессии порядка $p$.

\begin{theorem} \label{th:wninconsistency}
	Пусть $\Sigma_N^{-1} = \frac{1}{\sigma^2}{\mathbf{1}}$, где $\mathbf{1}$ --- единичная матрица порядка $N \times N$, что соответствует случаю белого шума с дисперсией $\sigma^2$.
	
	Представим все $\mu_i$ в полярном виде: $\mu_j = \rho_j e^{i \varphi_j}$, $\varphi_j \in (-\pi, \pi]$. Пусть показатели экспонент упорядочены по невозрастанию модуля $\mu_j$, т.е. $\rho_j \ge \rho_{j+1}$. Показатели экспонент в этом случае будут разделены на три (возможно пустые) подпоследовательности: те, у которых $|\mu_j|>1$, те, у которых $|\mu_j| = 1$ и те, у которых $|\mu_j| < 1$. Пусть соответствующее количество экспонент будет равно $r_A$ для $|\mu_j|>1$, $r_B$ для $|\mu_j|=1$ и $r_C$ для $|\mu_j|<1$. Тогда
	\begin{enumerate}
		\item Рассмотрим отнормированную матрицу $\widetilde \bfN = \bfN \bfL$, $\bfL = \text{diag}(l_1, \ldots, l_r)$, 
		\begin{equation*}
		l_j = \begin{cases}
		\frac{1}{\sqrt{\langle N_j, N_j \rangle_{\Sigma_N}}} e^{-i N \varphi_j}, & \rho_j > 1, \\
		\frac{1}{\sqrt{\langle N_j, N_j \rangle_{\Sigma_N}}}, & \rho_j \le 1,
		\end{cases}
		\end{equation*}
		$N_j$ --- $j$-й столбец матрицы $\bfN$.
		
		Тогда \begin{equation*}
		\lim_{N \to + \infty} \widetilde \bfN^* \Sigma_N^{-1} \widetilde \bfN = \left( \begin{array}{c|c|c}
		\widetilde \bfA & \mathbf{0} & \mathbf{0} \\ \hline
		\mathbf{0} & \mathbf{1}_{r_B} & \mathbf{0} \\ \hline
		\mathbf{0} & \mathbf{0} & \widetilde \bfC
		\end{array}  \right),
		\end{equation*}
		где матрица $\widetilde \bfA = (\tilde a_{j, k})$ --- невырожденная квадратная матрица порядка $r_A$, элементы которой равны
		\begin{equation*}
		\tilde a_{j, k} = \frac{\sqrt{\mu_j \overline{\mu_j} - 1}\sqrt{\mu_k \overline{\mu_k} - 1}}{\overline{\mu_j} \mu_k - 1},
		\end{equation*}
		$\mathbf{1}_{r_B}$ --- единичная квадратная матрица порядка $r_B$, $\widetilde \bfC = (\tilde c_{j, k})$ --- невырожденная квадратная матрица порядка $r_C$, элементы которой равны
		\begin{equation*}
		\tilde c_{j, k} = \frac{\sqrt{1 - \mu_{j+r_A+r_B} \overline{\mu_{j+r_A+r_B}}}\sqrt{1 - \mu_{k+r_A+r_B} \overline{\mu_{k+r_A+r_B}}}}{1 - \overline{\mu_{j+r_A+r_B}} \mu_{k+r_A+r_B}}.
	    \end{equation*}
	    
	    \item 
	    \begin{equation*}
	    \lim_{N \to + \infty} \bfJ^{-1} = \sigma^2 \left( \begin{array}{c|c}
	    \mathbf{0}_{r_A + r_B} & \mathbf{0} \\ \hline
	    \mathbf{0} & \bfC^{-1}
	    \end{array}  \right),
	    \end{equation*}
	    где $\mathbf{0}_{r_A + r_B}$ --- нулевая матрица порядка $r_A + r_B$, $\bfC = (c_{j,k})$ --- невырожденная квадратная матрица порядка $r_C$, элементы которой равны
	    \begin{equation*}
	    c_{j, k} = \frac{1}{1 - \overline{\mu_{j+r_A+r_B}} \mu_{k+r_A+r_B}}.
	    \end{equation*}
	\end{enumerate}
\end{theorem}
	
	\begin{proof}
		Не умаляя общности: пусть $\sigma^2 = 1$. Достаточно рассмотреть соответствующие пределы последовательностей. Для этого требуется использовать формулу $\sum_{i = 0}^{N-1} x^i = \frac{x^N-1}{x-1}$, если $x \ne 1$, и $N$ в противном случае.
		\begin{itemize}
			\item Пусть $\rho_i > 1$, $\rho_j > 1$. Тогда
			\begin{multline*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_i^t} \mu_j^t e^{I N (\varphi_i - \varphi_j)}}{\sqrt{\sum_{k=0}^{N-1} \mu_i^k \overline{\mu_i^k}} \sqrt{\sum_{k=0}^{N-1} \mu_j^k \overline{\mu_j^k}}} = \frac{(\rho_i \rho_j)^N - e^{I N(\varphi_i - \varphi_j)}}{\rho_i \rho_j e^{I (\varphi_j - \varphi_i)} - 1} \frac{\sqrt{\rho_i^2 - 1}}{\sqrt{\rho_i^{2N} - 1}} \cdot \\ \cdot \frac{\sqrt{\rho_j^2 - 1}}{\sqrt{\rho_j^{2N} - 1}} \to \tilde a_{i, j}.
			\end{multline*}
			
			\item Пусть $\rho_i > 1$, $\rho_j = 1$. Тогда
			\begin{equation*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_i^t} \mu_j^t e^{I N \varphi_i}}{\sqrt{\sum_{k=0}^{N-1} \mu_i^k \overline{\mu_i^k}} \sqrt{\sum_{k=0}^{N-1} \mu_j^k \overline{\mu_j^k}}} = \frac{(\rho_i )^N e^{I N \varphi_j} - e^{I N \varphi_i}}{\rho_i e^{I (\varphi_j - \varphi_i)} - 1} \frac{\sqrt{\rho_i^2 - 1}}{\sqrt{\rho_i^{2N} - 1}} \frac{1}{\sqrt{N}} \to 0.
			\end{equation*}
			
			\item Пусть $\rho_i > 1$, $\rho_j < 1$. Учитывая, что $\rho_i \rho_j < \rho_i$, получаем \footnote{Здесь нет случая, когда $\rho_j = 1/\rho_i$. Результат целиком аналогичен.}
			\begin{multline*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_i^t} \mu_j^t e^{I N \varphi_i}}{\sqrt{\sum_{k=0}^{N-1} \mu_i^k \overline{\mu_i^k}} \sqrt{\sum_{k=0}^{N-1} \mu_j^k \overline{\mu_j^k}}} = \frac{(\rho_i \rho_j)^N e^{I N \varphi_j} - e^{I N \varphi_i}}{\rho_i \rho_j e^{I (\varphi_j - \varphi_i)} - 1} \frac{\sqrt{\rho_i^2 - 1}}{\sqrt{\rho_i^{2N} - 1}} \cdot \\ \cdot \sqrt{\frac{1 - \rho_j^2}{1 - \rho_j^{2N}}} \to 0.
			\end{multline*}
			
			\item Пусть $\rho_i = 1$, $\rho_j = 1$, $i \ne j$. Тогда
			\begin{equation*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_i^t} \mu_j^t}{\sqrt{\sum_{k=0}^{N-1} \mu_i^k \overline{\mu_i^k}} \sqrt{\sum_{k=0}^{N-1} \mu_j^k \overline{\mu_j^k}}} = \frac{e^{I N (\varphi_j -\varphi_i)} - 1}{e^{I (\varphi_j - \varphi_i)} - 1} \frac{1}{\sqrt{N}} \frac{1}{\sqrt{N}} \to 0.
			\end{equation*}
			
			\item Пусть $\rho_i = 1$, $\rho_j < 1$. Тогда
			\begin{equation*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_i^t} \mu_j^t}{\sqrt{\sum_{k=0}^{N-1} \mu_i^k \overline{\mu_i^k}} \sqrt{\sum_{k=0}^{N-1} \mu_j^k \overline{\mu_j^k}}} = \frac{\rho_j^N e^{I N (\varphi_j -\varphi_i)} - 1}{\rho_j e^{I (\varphi_j - \varphi_i)} - 1}  \frac{1}{\sqrt{N}} \frac{\sqrt{1 - \rho_j^2}}{\sqrt{1 - \rho_j^{2N}}} \to 0.
			\end{equation*}
			
			\item Пусть $\rho_i < 1$, $\rho_j < 1$. Тогда
			\begin{multline*}
			\sum_{t=0}^{N-1} \frac{\overline{\mu_i^t} \mu_j^t}{\sqrt{\sum_{k=0}^{N-1} \mu_i^k \overline{\mu_i^k}} \sqrt{\sum_{k=0}^{N-1} \mu_j^k \overline{\mu_j^k}}} = \frac{1 - (\rho_i \rho_j)^N e^{I N (\varphi_j -\varphi_i)}}{1 - \rho_i \rho_j e^{I (\varphi_j - \varphi_i)}} \cdot \\ \cdot \frac{\sqrt{1 - \rho_i^2}}{\sqrt{1 - \rho_i^{2N}}} \frac{\sqrt{1 - \rho_j^2}}{\sqrt{1 - \rho_j^{2N}}} \to \frac{\sqrt{1 - \mu_{i} \overline{\mu_{i}}}\sqrt{1 - \mu_{j} \overline{\mu_{j}}}}{1 - \overline{\mu_{i}} \mu_{j}}.
			\end{multline*}
				
		\end{itemize}
		
		Матрицы $\widetilde \bfA$, $\widetilde \bfC$ и $\bfC$ --- Коши-подобные матрицы (Cauchy-like matrix), при этом все они невырожденные, см. \cite{Schechter1959}.
		
		Пункт 2 следует из пункта 1 теоремы с учётом применённой с помощью матрицы $\bfL$ нормировки.
	\end{proof}

Теперь докажем теорему в общем виде, сведя вычисление пределов к случаю белого шума.

\begin{theorem} \label{th:arinconsistency}
	Представим все $\mu_i$ в полярном виде: $\mu_i = \rho_i e^{I \varphi_i}$, $\varphi_i \in (-\pi, \pi]$. Пусть показатели экспонент упорядочены по невозрастанию модуля $\mu_i$, т.е. $\rho_i \ge \rho_{i+1}$. Показатели экспонент в этом случае будут разделены на три (возможно пустые) подпоследовательности: те, у которых $|\mu_i|>1$, те, у которых $|\mu_i| = 1$ и те, у которых $|\mu_i| < 1$. Пусть соответствующее количество экспонент будет равно $r_A$ для $|\mu_i|>1$, $r_B$ для $|\mu_i|>1$ и $r_C$ для $|\mu_i|<1$. Тогда
	\begin{enumerate}
		\item Рассмотрим отнормированную матрицу $\widetilde \bfM_N = (\tilde m_{i, j})$: 
		\begin{equation*}
		\tilde m_{i, j} = \begin{cases}
		\frac{\mu_j^{i-1}}{\sqrt{\sum_{k=0}^{N-1} \mu_j^k \overline{\mu_j^k}}} e^{-I N \varphi_j}, & \rho_j > 1, \\
		\frac{\mu_j^{i-1}}{\sqrt{\sum_{k=0}^{N-1} \mu_j^k \overline{\mu_j^k}}}, & \rho_j \le 1.
		\end{cases}
		\end{equation*}
		
		Тогда \begin{equation*}
		\lim_{N \to + \infty} \widetilde \bfM_N^* \Sigma_N^{-1} \widetilde \bfM_N = \left( \begin{array}{c|c|c}
		\widetilde \bfA & \mathbf{0} & \mathbf{0} \\ \hline
		\mathbf{0} & \bfI / \sigma^2& \mathbf{0} \\ \hline
		\mathbf{0} & \mathbf{0} & \widetilde \bfC
		\end{array}  \right),
		\end{equation*}
		где матрица $\widetilde \bfA = (\tilde a_{i, j})$ --- невырожденная квадратная матрица порядка $r_A$, элементы которой равны
		\begin{equation*}
		\tilde a_{i, j} = \frac{1}{\sigma^2} \frac{\sqrt{\mu_i \overline{\mu_i} - 1}\sqrt{\mu_j \overline{\mu_j} - 1} \overline{f(1/\mu_i)} f(1/\mu_j) }{(\overline{\mu_i} \mu_j - 1) |f(1/\mu_i)| |f(1/\mu_j)|},
		\end{equation*}
		$\bfI$ --- единичная квадратная матрица порядка $r_B$, $\widetilde \bfC = (\tilde c_{i, j})$ --- невырожденная квадратная матрица порядка $r_C$, элементы которой равны
		\begin{multline*}
		\tilde c_{i, j} = \frac{1}{\sigma^2} \frac{\sqrt{1 - \mu_{i+r_A+r_B} \overline{\mu_{i+r_A+r_B}}}\sqrt{1 - \mu_{j+r_A+r_B} \overline{\mu_{j+r_A+r_B}}}}{1 - \overline{\mu_{i+r_A+r_B}} \mu_{j+r_A+r_B}} \cdot \\ \cdot \frac{\overline{f(\mu_{i+r_A+r_B})} f(\mu_{j+r_A+r_B})}{|f(\mu_{i+r_A+r_B})| |f(\mu_{j+r_A+r_B})|}.
		\end{multline*}
		
		\item 
		\begin{equation*}
		\lim_{N \to + \infty} (\bfM_N^* \Sigma_N^{-1} \bfM_N)^{-1} = \sigma^2 \left( \begin{array}{c|c}
		\mathbf{0}_{r_A + r_B} & \mathbf{0} \\ \hline
		\mathbf{0} & \bfC^{-1}
		\end{array}  \right),
		\end{equation*}
		где $\mathbf{0}_{r_A + r_B}$ --- нулевая матрица порядка $r_A + r_B$, $\bfC = (c_{i,j})$ --- невырожденная квадратная матрица порядка $r_C$, элементы которой равны
		\begin{equation*}
		c_{i, j} = \frac{\overline{f(\mu_{i + r_A + r_B})} f(\mu_{j + r_A + r_B})}{1 - \overline{\mu_{i+r_A+r_B}} \mu_{j+r_A+r_B}}.
		\end{equation*}
	\end{enumerate}
\end{theorem}

\begin{proof}
	Чтобы не пересчитывать заново все пределы, заметим следующее:
	\begin{itemize}
		\item Если $A$ и $B$ --- два вектора, лежащие в $\sfZ^N$, то $A^* \Sigma_N^{-1} B = \rev(A)^* \Sigma_N^{-1} \rev(B)$, где $\rev(\cdot)$ --- оператор, разворачивающий вектор наоборот. Следует из того, что $\Sigma_N^{-1}$ симметрична относительно побочной диагонали.
		\item Выполняется равенство $A^* \Sigma_N^{-1} B = A^* \bfU_N^T \bfU_N B$, где $\bfU_N$ --- верхнетреугольная матрица в разложении Холецкого матрицы $\Sigma_N^{-1}$. С достаточно большого $N$ можно применять результат теоремы \ref{th:invcov_chol}.
	\end{itemize}
	
	Пусть $A = (a_0, \ldots, a_{N - 1})^\rmT$, $a_i = \nu^i$, $B = (b_0, \ldots, b_{N - 1})^\rmT$, $b_i = \xi^i$. Рассмотрим два случая (не умаляя общности, пусть $\sigma = 1$):
	\begin{itemize}
		\item $|\nu| > 1$. Тогда последовательности надо развернуть, получаем следующее:
		$A^* \Sigma_N^{-1} B = \rev(A)^* \Sigma_N^{-1} \rev(B) =  \rev(A)^* \bfU_N^T \bfU_N  \rev(B) =$ \\$\sum_{i=0}^{N-{2p} -2} (\sum_{j=1}^{p+1}b_j \overline{\nu^{N - i - j}}) (\sum_{j=1}^{p+1}b_j \mu^{N - i - j}) + O(1) = $\\ $\sum_{i=0}^{N-{2p} - 2} (\overline{\nu^{N - i - 1}} \overline{f(1/\nu)})  (\mu^{N - i - 1}f(1/\mu)) + O(1) =$\\ $\overline{f(1/\nu)} f(1/\mu)\sum_{i=0}^{N-1} (\overline{\nu^{N - i - 1}} ) (\mu^{N - i - 1}) + O(1) = $\\
		$\overline{f(1/\nu)} f(1/\mu)\sum_{i=0}^{N-1} \overline{\nu^i} \mu^i + O(1)$.
		\item $|\nu| \le 1, |\mu| \le 1$. Последовательности разворачивать не нужно, имеем следующее:
		$A^* \Sigma_N^{-1} B = A^* \bfU_N^T \bfU_N B = \sum_{i=0}^{N-{2p}-2} (\sum_{j=1}^{p+1}b_j \overline{\nu^{
				i + j - 1}}) \cdot$ \\$\cdot (\sum_{j=1}^{p+1}b_j \mu^{i + j - 1}) + O((|\nu||\mu|)^{N - 2p - 1}) =$  
		$\sum_{i=0}^{N-{2p} -2} (\overline{\nu^i f(\nu)})(\mu^i f(\mu)) +$\\ $+ O((|\nu||\mu|)^{N})$ = 
		$\overline{f(\nu)} f(\mu) \sum_{i=0}^{N-1} \overline{\nu^i} \mu^i + O((|\nu||\mu|)^{N})$.
	\end{itemize}
	
	Далее доказательство аналогично теореме \ref{th:invcov_chol}. Заметим, что все знаменатели остаются корректными, и невырожденность соответствующих матриц сохраняется: $f(1/\mu_i)$ не может быть равно $0$ при $|\mu_i| > 1$, и $f(\mu_i)$ не может быть равно $0$ при $|\mu_i| \le 1$, потому что авторегрессионный шум является стационарным, а, значит, справедлива лемма \ref{th:stat}.
\end{proof}

\section{Следствия из теорем \ref{th:wninconsistency} и \ref{th:arinconsistency}}
\begin{enumerate}
	\item Даже зная параметры шума и сигнальные корни, невозможно получить состоятельную оценку сигнала, если в сигнале есть убывающая экспонента или экспоненциально-модулированная синусоида. Тем более странно ожидать состоятельность, если параметры шума и сигнальные корни требуется оценивать. Тем не менее, результат не отвергает возможную (при реальном оценивании) состоятельность сигнала, если модули всех его сигнальных корней $|\mu_i| \ge 1$.
	
	\item Теоремы также дают оценки снизу на скорость сходимости при реальном оценивании: $O(\mu_i^{-N})$ для $|\mu_i| > 1$, $O(\frac{1}{\sqrt{N}})$ для $|\mu_i| = 1$ и $O(1)$ для $|\mu_i| < 1$. Быстрее нереально.
	
	\item Результат не зависит от того, стоит задача выделения из сигнала плюс белого шума или авторегрессионной стационарной последовательности конечного порядка $p$. \cite{ZhigljavskyGolyandinaGryaznov2016}
\end{enumerate}

\section{Сведения из теории авторегрессионных процессов}

Рассмотрим $\tsY = (y_t, \; t~\in~\sfN)$ --- стационарный гауссовский процесс авторегрессии  порядка $p$:
\begin{equation} \label{def:arp}
y_t = \sum_{j = 1}^p \beta_j y_{t - j} + \sigma \varepsilon_t,
\end{equation}
где $\beta_1, \ldots, \beta_p$ --- коэффициенты авторегрессии, $\sigma$ --- стандартное отклонение шума, $(\varepsilon_t, t \in \sfN)$ --- независимые нормально распределённые случайные величины с нулевым средним и единичной дисперсией.

Определим следующее: вектор $B = (1, -\beta_1, \ldots, -\beta_p)^\rmT \in \sfR^{p+1}$, $B = (b_1, \ldots, b_{p+1})^\rmT$ и комплексный полином $f(z) = 1 - \sum_{j=1}^p \beta_j z^j$.

\begin{lemma}\label{th:stat}
	Процесс авторегресии $\tsY$ является стационарным тогда и только тогда, когда все корни полинома $f(z)$ по модулю строго больше $1$: $f(z) = 0$ в том и только в том случае, если $|z| > 1$.
\end{lemma}

\begin{theorem}\label{th:invcov}
	Рассмотрим случайный вектор $\tsY_N = (y_1, \ldots, y_N)^\rmT$ длины $N \ge 2p + 1$ --- подстрока процесса $\tsY$ длины $N$, $\Sigma_N$ --- его ковариационная матрица. Тогда обратная ковариационная матрица $\Sigma_N^{-1} = (s_{l, j; N})$ равна:
	\begin{equation*}
	s_{l, j; N} = \begin{cases}
	0, & |l - j| > p, \\
	\frac{1}{\sigma^2}\sum_{k = 1}^{\min(l, N + 1 - j, p + 1 - |l - j|)}b_k b_{k + |l - j|}, & l \le j, \\
	\frac{1}{\sigma^2}\sum_{k = 1}^{\min(j, N + 1 - l, p + 1 - |l - j|)}b_k b_{k + |l - j|}, & l > j.
	\end{cases}
	\end{equation*}
\end{theorem}
\begin{proof}
	Утверждение теоремы --- переписанный в другом виде результат, полученный в работе \cite{Verbyla1985}.
\end{proof}

\begin{theorem}\label{th:invcov_chol}
	В терминах теоремы \ref{th:invcov}: рассмотрим обратную ковариационную матрицу $\Sigma_N^{-1}$, где $N \ge 2p + 2$ и её разложение Холецкого $\Sigma_N^{-1} = \bfU^T_N \bfU_N$, $\bfU_N$ --- верхнетреугольная матрица порядка $N$. Тогда матрица $\bfU_N$ имеет следующий вид:
	\begin{equation*}
	\bfU_N = \frac{1}{\sigma} \begin{pmatrix}
	b_1 & b_2 & \ldots & b_{p+1} &  &  &  &  \\ 
	& b_1 & b_2 & \ldots & b_{p+1} &  &  &  \\ 
	&  & \ddots & \ddots & \ddots & \ddots &  &  \\ 
	&  &  & b_1 & b_2 & \ldots & b_{p+1} &  \\ 
	&  &  &  & \multicolumn{4}{c}{\multirow{4}{*}{$\bfU_{2p+1}$}} \\
	&  &  &  & \multicolumn{4}{c}{\multirow{4}{*}{}} \\
	&  &  &  & \multicolumn{4}{c}{\multirow{4}{*}{}} \\
	&  &  &  & \multicolumn{4}{c}{\multirow{4}{*}{}}	
	\end{pmatrix},
	\end{equation*}
	где количество строчек в верхнем блоке равно $N - (2p + 1)$, $\bfU_{2p+1}$ --- разложение Холецкого матрицы $\Sigma_{2p+1}^{-1}$: $\Sigma_{2p+1}^{-1} = \bfU_{2p+1}^\rmT \bfU_{2p+1}$.
\end{theorem}
\begin{proof}
	Заметим, что разложение Холецкого для матрицы $\bfU_N$, $N \ge 2p + 2$, можно представить, как
	\begin{equation*}
	\bfU_N = \frac{1}{\sigma} \begin{pmatrix}
	b_1 & b_2 & \ldots & b_{p+1} &  \\ 
	& \multicolumn{4}{c}{\multirow{4}{*}{$\bfU_{N - 1}$}} \\
	& \multicolumn{4}{c}{\multirow{4}{*}{}} \\
	& \multicolumn{4}{c}{\multirow{4}{*}{}} \\
	& \multicolumn{4}{c}{\multirow{4}{*}{}}	
	\end{pmatrix},
	\end{equation*}	
	где $\bfU_{N-1}$ --- разложение Холецкого для матрицы $\Sigma_{N-1}^{-1}$. Это следствие вида матрицы $\Sigma_{N}$ из теоремы \ref{th:invcov} и алгоритма вычисления разложения Холецкого. Далее доказательство продолжается по индукции. \cite{stoica2005spectral}
\end{proof}

\bibliographystyle{gost705}
\bibliography{zvonarev}
\end{document}
