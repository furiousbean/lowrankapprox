\documentclass[12pt,a4paper,fleqn,leqno]{article}
\usepackage{mathtext}
\usepackage{cmap}
\usepackage[utf8x]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{amsmath,amssymb,amsthm,amscd,amsfonts}
\usepackage{euscript}
\usepackage{relsize}
\usepackage{mathdots}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{caption2}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{titlesec}
\usepackage{sicpro_rus}
\usepackage{mathtext}%русские буквы в формулах

\usepackage[colorlinks, urlcolor=blue, pdfborder={0 0 0 [0 0]}]{hyperref}

\hyphenation{Struc-tu-red}
\hyphenation{Ran-do-mized}
\hyphenation{Ma-xi-mi-za-tion}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\tr}{tr}
\providecommand*{\BibDash}{}

\def\rank{\mathop{\mathrm{rank}}}
\newtheorem{corollary}{Следствие}
\newtheorem{proposition}{Предложение}
\newtheorem{algorithm}{Алгоритм}
\newtheorem{lemma}{Лемма}

\input{letters}

\input{newcommands}

\sectionfont{\centering}

\subsectionfont{\centering}
\subsubsectionfont{\normalsize}
\setcounter{page}{1}


\author{Nikita Zvonarev}
\title{Iterative algorithms for Weighted Finite Rank Time Series  Approximation}
\begin{document}
\noindent УДК 519.246.8+519.254

\begin{center}{
\fontsize{18pt}{23pt}\selectfont\bf%
  \MakeUppercase{
 Iterative algorithms for Weighted Finite Rank Time Series Approximation
}}
\end{center}

\begin{center}{\bpv\bmv {Н.К.~Звонарев}\\
\footnotesize\it Санкт-Петербургский государственный университет,\\
Математико-механический факультет
\\
\rm
Россия, 198504, Санкт-Петербург, Петродворец, Университетский пр., 28\\
E-mail: \textcolor {blue}{\underline{nikitazvonarev@gmail.com}}}
\end{center}
\begin{center}{\bpv {Н.Э.~Голяндина}\\
\footnotesize\it Санкт-Петербургский государственный университет,\\
Математико-механический факультет
\\
\rm
Россия, 198504, Санкт-Петербург, Петродворец, Университетский пр., 28\\
E-mail: \textcolor {blue}{\underline{nina@gistatgroup.com}}}
\end{center}
\hspace{1.25cm}\begin{minipage}{12.16cm}\bpv\bpv\bmv \noindent
\footnotesize{\bf Keywords:}\/ Time series, Cadzow iterations, Time Series of Finite Rank, Weighted Total Least Squares, Oblique SVD-Factorization, Singular Spectrum Analysis

\bpv\bpv\noindent  The problem of time series approximation by series of finite rank is considered in the paper. This is actual problem in signal processing tasks, partially, for extracting a signal in analysis of noisy signals. After applying weigthed least-squares method, the optimization problem arises, which do not have an exact solution. One of the numeric local minimum search method (Cadzow iterations) is well known. However, Cadzow iterations may work with specific weights only, which decreases while approaching edges of time series. In addition, it is naturally to take equal weights corresponding to usual Euclidean metric. Therefore, several new methods are suggested and investigated to achieve equal or approximately equal weights. Questions of convergency, computational complexity and accuracy are considered for proposed methods. Methods are compared on the numeric example.

\end{minipage}\bls\bmv

\section{Introduction}
Consider the problem of extracting a signal $\tsS = (s_1, \ldots, s_N)$ from an observed noisy signal $\tsX = \tsS + \tsN$, where $\tsS$ has some fixed structure, exactly, $\tsS$ is governed by some \emph{linear recurrence relation} (LRR) of order $r$:
\begin{equation*}
s_n = \sum_{i = 1}^{r} a_i s_{n-i}, \quad n = r + 1, \ldots, N.
\end{equation*}
Generally, governed-by-LRR series may be written in a parametric form \\ $s_n = \sum_i P_i(n) \exp(\alpha_i n) \cos(2 \pi \omega_i n + \psi_i)$. However, parametric approach for the problem does not lead to good estimation of parameters due to their big amount and instability of estimators.

It is known that methods based on signal subspace estimation (subspace-based methods) works well. The basic idea of these methods is as follows: let us fix window length $L$, $1 \le L \le N$, $K = N - L + 1$, and build a trajectory matrix for series $\tsS$:
\begin{equation*}
\bfS = \begin{pmatrix}
s_1 & s_2 & \ldots & s_K \\
s_2 & s_3 & \ldots & s_{K + 1} \\
\vdots & \vdots & \vdots & \vdots \\
s_L & s_{L + 1} & \ldots & s_N
\end{pmatrix}.
\end{equation*}
Note that $\bfS\in \calH$, where $\calH$ is the set of Hankel matrixes with equal values on their anti-diagonals $i+j=\mathrm{const}$.
If series are governed by minimal LRR of order $r$, $r < \min(L, K)$, then $\rank \bfS = r < L$. So, $\bfS$ is a Hankel matrix of low-rank $r$.

Let $\bfX$ be a trajectory matrix of series $\tsX$. Then the problem of estimation of $\tsS$ could be considered as the problem of approximation of matrix $\bfX$ by a Hankel matrix of rank not larger than $r$:
\begin{equation}\label{introd_task}
\|\bfX - \bfY\|^2_\rmF \to \min_{\substack{\rank \bfY \le r \\ \bfY \in \calH}}
\end{equation}

A lot of papers are dedicated to this problem, e.g., \cite{Cadzow1988, Markovsky2011, Usevich.Markovsky2014, Gillard.Zhigljavsky2013} and many other works, where the problem is called Structured Low-Rank Approximation. Approaches of solving the problem are iterative, e.g., Cadzow iterations consist of alternating projections to the set of Hankel matrices and matrices of rank not larger than $r$. The target function is not unimodal in such class of problems, and convergency to global minimum is not guaranteed; despite this, problem \eqref{introd_task} is considered to be well-researched, though it has many open answers yet.

Note that problem \eqref{introd_task} is equivalent to the problem
\begin{equation}\label{introd_task_2}
\sum_{i = 1}^N w_i(x_i - y_i)^2 \to \min_{\substack{\tsY: \rank \bfY \le r \\ \bfY \in \calH}},
\end{equation}
where
\begin{equation}
\label{eq:w}
w_i = \begin{cases}
i & \text{for $i = 1, \ldots, L-1,$}\\
L & \text{for $i = L, \ldots, K,$}\\
N - i + 1 & \text{for $i = K + 1, \ldots, N.$}
\end{cases}.
\end{equation}

Weights on edges of series are less than in the center, i.e. \eqref{introd_task} is a weighted least squares problem for series.

The aim of this paper is to consider methods which solve problem \eqref{introd_task_2} with various weights instead of $w_i$ and compare methods in terms of precision of the signal's $\tsS$ estimation. All described methods are iterative. If an estimate of the signal, which is not governed by LRR, is the point of interest then only the first iteration could be taken with aim of reduction of computational complexity. So, described methods are compared by precision of the signal estimation in the first iteration and in the limit. Note than known Singular Spectrum Analysis (SSA) \cite{Broomhead.King1986, Vautard.etal1992, Elsner.Tsonis1996, Golyandina.etal2001, Ghil.etal2002, Golyandina.Zhigljavsky2012} could be
described as one iteration of Cadzow iterations.

Structure of paper is as follows:  In Section~\ref{sec:lowrank_appr} a problem of approximating a matrix by a Hankel rank-deficient matrix is considered. Common structure of iterative alternating projection algorithms is described, ways of builduing projectors are given, the convergency theorem is proved.

In Section~\ref{sec:ts_matrices} the connection between problems of approximation of time series and matrices is described, relation between weights in weighted least squares problems is also given. Chapter~\ref{sec:alg} is dedicated to suggested time series approximation algorithms. In Section~\ref{sec:simul} numeric comparison of algorithms on typical example is carried out.

The paper is finished with short conclusions and discussion of further work in Section~\ref{sec:concl}. The result of separability of constant and sinusoidal signal, which has connection with convergency speed of some described algorithms, is proved in appendix.

\section{Approximation by rank-deficient Hankel matrices}
\label{sec:lowrank_appr}
\subsection{Common algotihm}
Consider a problem of approximation of a matrix $\bfX$ by Hankel rank-deficient matrix with respect to some (semi)norm $\|\cdot\|$ in this Section. Define by $\spaceR^{L\times K}$ space of matrices of size $L \times K$, $\calM_r\subset \spaceR^{L\times K}$ is space of matrices of rank not larger than $r$,
$\calH \subset \spaceR^{L\times K}$ is the set of Hankel matrices.
Note that  $\calM_r$ is not nor linear, nor convex set. However, $\calM_r$ is multiplicative, i.e.
if $\bfZ\in \calM_r$, then $a\bfZ\in \calM_r$ for any $a$.
Space $\calH$ is linear.

The problem is
\be
\label{eq:gen_task}
\|\bfX - \bfY\| \to \min_\bfY, \mbox{\ where\ } \bfY \in \calH \cap \calM_r.
\ee

For presenting the algorithm's scheme for this problem, let us introduce projectors to proper subspaces with respect to $\|\cdot\|$ norm, $\Pi_{\calM_r}$ --- projector to $\calM_r\subset \spaceR^{L\times K}$,
$\Pi_{\calH}$ --- projector to $\calH$.
Both projectors are ortogonal because multiplicativity is sufficient for  orthogonality of space. Note that result of projection to subspace of rank-deficient matrices could not be defined uniquely, but further we suppose that in the case of ambiguity any value from all possible is chosen.

\begin{proposition} \label{pythaprop}
Let $\calX$ be Hilbert space, $\calM$ is multiplicative subset, $\Pi_\calM$ --- projection operator to $\calM$. Then for any $x \in \calX$ Pythagorean theorem is true: $\|x\|^2 = \|x - \Pi_\calM x\|^2 + \|\Pi_\calM x\|^2$.
\end{proposition}
\begin{proof5}{\ref{pythaprop}}
By using multiplicativity, the set $\calM$ could be represented as $\calM = \bigcup\limits_{l \in \calL}l$, where $\calL$ is the set of all lines lying in $\calM$ and passes through $0$, and $l \cap m = 0$ for any $l, m \in \calL$, $l \neq m$. Then projection could be described as follows: to begin with, we choose a line $l$ such that $\dist(x, l) \rightarrow \min\limits_{l \in \calL}$, then $y = \Pi_\calM x$ is orthogonal projection $x$ to a line $l$, which is linear subspace. Proposition is proved.
\end{proof5}

For solving problem \eqref{eq:gen_task} iterative alternating projections method could be used:
\be
   \bfY_{k+1}=\Pi_\calH \Pi_{\calM_r} \bfY_{k}, \mbox{\ where\ } \bfY_{0}=\bfX.
\ee

Let us prove the theorem about convergency speed of given method.

\begin{theorem}
\label{th:converg}
\begin{enumerate}
Let space $\calM_r$ be closed in topology gererated by norm $\|\cdot\|$. Then
\item $\|\bfY_k - \Pi_{\calM_r}\bfY_k\| \to 0$ as $k \to +\infty$, $\|\Pi_{\calM_r}\bfY_k - \bfY_{k+1}\| \to 0$ as $k \to +\infty$.
\item There exists convergent subsequence of matrices $\bfY_{i_1}, \bfY_{i_2}, \ldots$ such that its limit $\bfY^*$ lies in $\calM_r \cap \calH$.
\end{enumerate}
\end{theorem}
\begin{proof2}{\ref{th:converg}}
Let us use unequalities \cite{Chu.etal2003}
\begin{equation}
\label{chuprop}
\|\bfY_k - \Pi_{\calM_r} \bfY_k\| \ge \|\Pi_{\calM_r} \bfY_k - \bfY_{k + 1}\| \ge \|\bfY_{k+1} - \Pi_{\calM_r} \bfY_{k + 1}\|.
\end{equation}

\begin{enumerate}
\item According to unequalities \eqref{chuprop}, sequences $\|\bfY_k - \Pi_{\calM_r} \bfY_k\|$, $k = 1, 2, \ldots$ and $\|\Pi_{\calM_r} \bfY_k - \bfY_{k + 1}\|$, $k = 1, 2, \ldots$ are non-increasing. It is obvious that they are limited below by zero. So they have equal limit $c$ according to \eqref{chuprop}.

Prove that $c = 0$. Assume contrary: $d > 0$ exists such that for any $k = 1, 2, \ldots$: $\|\bfY_k - \Pi_{\calM_r} \bfY_k\| > d$, $\|\Pi_{\calM_r} \bfY_k - \bfY_{k + 1}\| > d$. According to ~\ref{pythaprop} proposition, the following is true:
\begin{gather*}
\|\bfY_k\|^2 = \|\Pi_{\calM_r} \bfY_k\|^2 + \|\bfY_k - \Pi_{\calM_r} \bfY_k\|^2 =\\ \|\bfY_k - \Pi_{\calM_r} \bfY_k\|^2 + \|\Pi_{\calM_r} \bfY_k - \bfY_{k + 1}\|^2 + \|\bfY_{k + 1}\|^2.
\end{gather*}
Thus, $\|\bfY_{k+1}\|^2 < \|\bfY_k\|^2 - 2d^2$. Expanding the inequality the same way further, see that for any $j = 1, 2, \ldots$: $\|\bfY_{k+j}\|^2 < \|\bfY_k\|^2 - 2 j d^2$. Choose any $k$, e.g. $k = 1$, and $j = \lceil \|\bfY_k\|^2 / (2d^2) \rceil + 1$. Then $\|\bfY_{k+j}\|^2 < 0$, which is not possible.
\item Consider a sequence $(\Pi_{\calM_r} \bfY_k)$, $k = 1, 2, \ldots$. It is bounded because $\|\Pi_{\calM_r} \bfZ\| \le \|\bfZ\|$ and $\|\Pi_{\calH} \bfZ\| \le \|\bfZ\|$ for any $\bfZ \in \sfR^{L \times K}$ (it is correct by Proposition \ref{pythaprop}, for example). Then a convergent subsequence $(\Pi_{\calM_r} \bfY_{i_k})$ could be chosen, $\bfY^*$ is its limit, wherein $\|\Pi_{\calM_r} \bfY_{i_k} - \bfY_{i_k + 1}\| = \|\Pi_{\calM_r} \bfY_{i_k} - \Pi_\calH \Pi_{\calM_r} \bfY_{i_k}\| \to 0$ as $k \to + \infty$. Taking into consideration that $\|\bfZ - \Pi_\calH \bfZ\|$ is a composition of continuous mappings, obtain that $\|\bfY^* - \Pi_\calH \bfY^*\| = 0$, and knowing that $\calM_r$ is closed set, obtain that $\bfY^* \in \calM_r \cap \calH$. The last thing to note is that sequence $(\Pi_\calH \Pi_{\calM_r} \bfY_{i_k})$ is convergent because $\Pi_\calH$ is continuous mapping, and its limit is equal to $\bfY^*$. Find that $\bfY_{i_k + 1}$ is required subsequence.
\end{enumerate}
\end{proof2}

Norms generated by weighted Frobenius inner product in form
\be
\label{eq:w_inner_prod}
\langle\bfY, \bfZ\rangle_M = \sum_{l = 1}^L \sum_{k = 1}^K m_{l, k} y_{l, k} z_{l, k}
\ee
are considered below.

It is known that $\calM_r$ is closed with respect to usual Frobenius norm, therefore, Theorem~\ref{th:converg} is also true in the case of weigthed Frobenius norm for any $m_{l,k} > 0$.


\subsection{Evaluation of projectors}

Consider a norm $\|\cdot\|_\bfM$ generated by \eqref{eq:w_inner_prod}.

\paragraph{Projector $\Pi_\calH$.} It is easy to show that $\Pi_\calH$
could be evaluated explicitly using following Proposition.

\begin{proposition}
For $\widehat{\bfY}=\Pi_\calH \bfY$
\begin{equation*}
\hat{y}_{ij} = \frac{\sum_{l,k: l+k=i+j} m_{l,k} y_{l,k}}{\sum_{l,k: l+k=i+j} m_{l,k}}.
\end{equation*}
\end{proposition}

It is not possible to get explicit form of $\Pi_{\calM_r}$ in general case.
Consider following cases.

\paragraph{Case of explicit form of projector $\Pi_{\calM_r}$.} To begin with consider a case when projector could be evaluated explicitly.
Let all weights $m_{ij}=1$. Denote $\Pi_r=\Pi_{\calM_r}$ in this speial case.
It is well-known that projector $\Pi_{r} \bfY$ could be evaluated as a sum of first components of matrix's $\bfY$ Singular Value Decomposition: let $\bfY = \bfU \mathbf{\Sigma} \bfV^\rmT$ where $\bfU$ is an orthogonal matrix of size $L \times L$, $\mathbf{\Sigma}$ is a quasi-diagonal matrix of size $L \times K$ with non-negative diagonal elements in non-increasing order, $\bfV$ is an orthogonal matrix of size $K \times K$. More definitely, denote by $L\le K$, $\Sigma = (\sigma_1, \ldots, \sigma_L)$ a vector consisting of diagonal elements of matrix $\mathbf{\Sigma}$. Denote by $\mathbf{\Sigma}_r = (\sigma^r_{l k})$ following matrix:
\begin{equation*}
\sigma^r_{i j} = \begin{cases}
\sigma_i & \text{if $i = j, i \le r,$}\\
0 & \text{otherwise}.
\end{cases}
\end{equation*}
Then projection could be evaluated as follows: $\Pi_{r} \bfY  = \bfU \mathbf{\Sigma}_r \bfV^\rmT$.
Next Proposition describes the case when evaluation of a projector is reduced to applying the projector  $\Pi_r$.

\begin{proposition}
\label{prop:projS}
Let $\bfC$ be symmetric semidefinite matrix of size $K \times K$ such that $\|\bfY\|_\bfM = \tr(\bfY \bfC \bfY^\rmT)$.
Let us also suppose that the column space of a matrix $\bfY$ lies in the column space of a matrix $\bfC$.
Then
\be
\label{eq:PiMr}
\Pi_{\calM_r} \bfY = (\Pi_r \bfB) (\bfO_\bfC^{\rmT})^\dagger,
\ee
where $\bfO_\bfC$ is a matrix such that $\bfC = \bfO_\bfC^{\rmT}\bfO_\bfC$,
$\bfB = \bfY \bfO_\bfC^{\rmT}$, $(\bfO_\bfC^{\rmT})^\dagger$ denotes  Moore-Penrose pseudoinverse to the matrix $\bfO_\bfC^{\rmT}$.
\end{proposition}
\begin{proof5}{\ref{prop:projS}}
The proof is a direct consequence of the fact that considered norm is generated by oblique inner product in row space of a matrix $\bfY$, watch details in \cite{Golyandina2013}.
\end{proof5}

\begin{remark}
Note that Proposition~\ref{prop:projS} conditions could be satisfied unless matrix $\bfC$ is diagonal.
\end{remark}

\paragraph{Projector $\Pi_{\calM_r}$ in general case.}
Since the projector could not be found explicitly, iterative algorithms are used in the general case.
One of these algorithms is described in \cite{Srebro2003}. Denote by $\odot$ element-wise matrix product.

\begin{algorithm}
\label{alg:weightedSVD}
\textbf{Input}: initial matrix $\bfY$, rank $r$, weight matrix $\bfM$,
stop criterion STOP.

\textbf{Result}:
Matrix $\widehat\bfY$ as an estimate of $\Pi_{\calM_r} \bfY$.

\begin{enumerate}
\item
$\bfY_0 = \bfY$, $k=0$.
\item
$\bfY_{k+1} = \Pi_r(\bfY \odot \bfM + \bfY_{k} \odot (\bfU -  \bfM))$, where
$\bfU \in \sfR^{L \times K}$,  $\bfU = \begin{pmatrix}
1 & \cdots & 1 \\
\vdots & \ddots & \vdots \\
1 & \cdots & 1
\end{pmatrix}$, $k\leftarrow k+1$.
\item
If STOP then $\widehat\bfY = \bfY_k$.
\end{enumerate}
\end{algorithm}

Note that in the case when $m_{ij}$ is equal to 0 or 1 this algorithm is EM-algorithm \cite{Srebro2003},
hence properties of EM-algorithms are carried out and it converges to local minimum in the problem of projector search. Nominally, it does not make a sence which values are in matrix at positions of zeros. However, it could be significant for algorithm's convergency.

\section{Time series and matrix approximation problem}
\label{sec:ts_matrices}
\subsection{Problem statement for time series}
Consider time series $\tsX = (x_1, \ldots, x_N)$ of length $N \ge 3$. Let us fix a window length $L$, $1 < L < N$, denote $K = N - L + 1$. Also consider a sequence of vectors:
\begin{equation}\label{l_lagged}
X_i = (x_i, \ldots, x_{i + L - 1})^\rmT, \qquad i = 1, \ldots, K.
\end{equation}
Define by $L$-trajectory matrix of series $\tsX$ a matrix $\bfX = [X_1:\ldots:X_K]$.

\begin{definition}
Suppose that $0 \le r \le L$. Then it is said that series $\tsX$ \emph{have $L$-rank $r$} if its $L$-trajectory matrix's $\bfX$ rank equals $r$.
\end{definition}

Note that series $\tsX$ could have $L$-rank $r$ only when
\begin{equation}
r \le \min(L, N-L+1). \label{min_condition}
\end{equation}
It is said that the window length $L$ is \emph{acceptable} for fixed $r$ if it satisfies condition \eqref{min_condition}.

Further we suppose that $L$ is not larger than $K$ because transposition does not change the situation and row rank of the trajectory matrix is equal to its column rank.

Let $\sfX_N$ be the set of all time series of length $N$, $\sfX_N^r$ is the set of all time series of length $N$ of $L$-rank not larger than $r$. For given time series $\tsX \in \sfX_N$, window length $L$, $1 < L < N$, and $r$ satisfying condition \eqref{min_condition}, consider a problem:
\begin{equation} \label{L-rank_task}
f_q(\tsY) \to \min_{\tsY \in \sfX_N^r}, \quad f_q(\tsY) = \sum \limits_{i=1}^N q_i(x_i - y_i)^2,
\end{equation}
where $y_i$ is $i$-th measurement of series $\tsY$ and $q_1, \ldots, q_N$ are some non-negative weights,
$q_i \ge 0$, $i = 1, \ldots, N$. The greatest point of interest is the case when a target function $f(\tsY) = \rho^2(\tsX, \tsY)$ is a square of Euclid distance in $\sfR^N$. It concides with $f_q(\tsY)$ when $q_i = 1$, $i = 1, \ldots, N$.


Let $\tsX$ be time series of length $N$, $\bfX \in \calH$ --- its trajectory matrix where $\calH$ is the set of all Hankel matrices of size $L \times K$. Then there exists a one-to-one mapping $\calT$ between $\sfX_N$ and $\calH$ which could be written as
\begin{equation*}
\calT(\tsX) = \bfX: \hat x_{l, k} = x_{l + k - 1}, \quad \bfX = (\hat x_{l,k}), \quad \tsX = (x_1, \ldots, x_N).
\end{equation*}

Hence the one-to-one mapping between space of series and Hankel matrices,
problem~\eqref{L-rank_task} could be expressed in terms of matrices.

\paragraph{Adjustment.} For a numeric search of solution of optimization problem \eqref{eq:gen_task} following fact is necessary.
Consider $\calX$ as Hilbert space with inner product $\langle\cdot, \cdot\rangle$, $\calM$ is multiplicative subset. Let an element $x$ lie in $\calX$, $y$ lie in $\calM$. Consider a projection $y^*$ of an element $x$ to a line $l = \{\alpha y:\, \alpha \in \sfR\}$:. Then an adjustment $y^*$ is
\begin{equation}
\label{eq:adjust}
y^* = \calA(y)= y \frac{\langle x, y\rangle}{\langle y, y\rangle}.
\end{equation}
Herewith $\|x - y^*\| \le \|x - y\|$ is carried out.

Thus if some estimate $y=\tsY \in \sfX_N^r$ to the solution of problem \eqref{L-rank_task} of approximation $x=\tsX \in \sfX_N$ exists, then it could be adjusted (at least not made worse) by applying \eqref{eq:adjust} and obtaining series $\tsY^*=\calA(\tsY)$ which is called as \emph{$\tsY$ adjustment}.

\subsection{Equivalent target functions of problem \eqref{L-rank_task}}
In space of time series a target function is given explicitly using (semi)inner product
\begin{equation}
\label{eq:norm_ser}
    \langle\tsY,\tsZ\rangle_q = \sum_{i = 1}^N q_i y_i z_i,
\end{equation}
where $q_i$ are positive (non-negative) weights.

Consider two (semi)inner products in space of matrices which are expansions of usual Frobenius inner product.

Denote
\begin{equation}
\label{eq:norm1M}
    \langle\bfY,\bfZ\rangle_{1,\bfM} = \sum_{i = 1}^L \sum_{j=1}^K m_{i,j} y_{i,j} z_{i,j}.
\end{equation}
for a matrix $\bfM$ with positive (non-negative) elements and
\begin{equation}
\label{eq:norm2S}
    \langle\bfY,\bfZ\rangle_{2,\bfC} = \tr(\bfY \bfC \bfZ^\rmT)
\end{equation}
for positive definite (or positive negative in a case of seminorm) matrix $\bfC$.

Note that if a matrix $\bfM$ fully consist of ones, i.e. $m_{i.j}=1$,
and if a $\bfC$ is identity matrix, then both inner products coincide with usual Frobenius inner product.

\begin{proposition}
\label{prop:equiv_tasks}
1. Let $\bfY = \calT(\tsY)$,  $\bfZ = \calT(\tsZ)$. Then $\langle\tsY,\tsZ\rangle_q= <\bfY,\bfZ>_{1,\bfM}$ iff
\begin{equation}\label{qi_mi}
q_i = \sum_{\substack{1 \le l \le L \\ 1 \le k \le K \\ l+k-1=i}} m_{l,k}.
\end{equation}

2. For a diagonal matrix $\bfC=\diag(c_1,\ldots,c_K)$, $\langle\bfY,\bfZ\rangle_{1,\bfM}= \langle\bfY,\bfZ\rangle_{2,\bfC}$ iff
\begin{equation}\label{sk_mlk}
m_{l,k}=c_k.
\end{equation}
\end{proposition}
\begin{proof5}{\ref{prop:equiv_tasks}}
To prove the first part note that
\begin{equation*}
\langle \bfY, \bfZ \rangle_{1,\bfM} = \sum_{i = 1}^L \sum_{j = 1}^K m_{i,j} y_{i + j - 1} z_{i + j - 1},
\end{equation*}
Proof of the second part is a consequence of the fact that for a diagonal matrix $\bfC$
\begin{equation*}
\langle \bfY, \bfZ \rangle_{2,\bfC} = \sum_{l=1}^L \sum_{k=1}^K c_k y_{l,k} z_{l, k}.
\end{equation*}
\end{proof5}

\begin{corollary}
\label{cor:base_weights}
If all weights $m_{i,j}=1$, then weights $q_i$ are equal to $w_i$ given in \eqref{eq:w}.
\end{corollary}

Note that second matrix norm with a diagonal matrix $\bfC$ is a particular case of first norm. However, the value of writing first norm in a form of second norm is that approximation by rank-deficient matrices with respect to the first norm is a complex problem if not all $m_{i,j}$ are equal, but approximation with respect to the second norm is a natural problem which could be solved using oblique Singular Value Decomposition.

\begin{remark}
\label{rem:2tasks}
Thus, if Condition~\eqref{qi_mi} is carried out and all weights $q_i$ and $m_{i,j}$ are non-zero, then problem~\eqref{L-rank_task}
is equivalent to the problem
\begin{equation*}
\label{rank_task}
    f_\bfM(\bfY) \to \min_{\bfY \in \calM_r \cap \calH}, \quad f_\bfM(\tsY) = \|\bfX-\bfY\|_\bfM.
\end{equation*}
\end{remark}

\section{Algorithms}
\label{sec:alg}
All discussed algorithms for solving problem~\eqref{L-rank_task} are given in this Section.
In a model of series $\tsX=\tsS+\tsN$, where $\tsS$ is time series of finite rank $r$, $\tsN$ is a noise, a result of algorithm is considered as an estimate of signal $\tsS$.

\subsection{Cadzow iterations}
The aim of this algorithm is approximation of trajectory matrix with respect to norm $\|\cdot\|_\bfM$ with weights $m_{ij}=1$ (i.e. it solves problem \eqref{introd_task}) corresponding to problem~\eqref{introd_task_2} with weights given in \eqref{eq:w} by Corollary \ref{cor:base_weights}. The algorithm was proposed in \cite{Cadzow1988}. The drawback of this algorithm is non-uniformity of weights $w_i$: they are larger in center than in edges of time series. Note that window length decrease leads to more uniform weights.
	
\begin{algorithm}[Cadzow iterations]
\textbf{Input}: Time series $\tsX$, window length $L$, rank $r$,
stop criterion STOP1 (eg., given quantity of iterations).

\textbf{Result}:
Series $\widehat\tsS$ as an estimate of $\tsX$ by finite rank series of order $r$.

\begin{enumerate}
\item
$\bfY_0 = \calT \tsX$, $k=0$.
\item
$\bfY_{k+1} = \Pi_\calH  \Pi_{\calM_r} \bfY_{k}$, $k\leftarrow k+1$.
\item
If STOP1 then $\widehat\tsS = \calT^{-1} \bfY_k$.
\end{enumerate}
\end{algorithm}


\subsection{Weighted Cadzow iterations}

Let all weights $q_{i}=1$. Then equivalent matrix weights could be presented as
\begin{equation}
\label{Mw}
   m_{l, k} = \frac{1}{q_{l + k - 1}}
\end{equation}
according to Proposition ~\ref{prop:equiv_tasks}.

\begin{algorithm}[Weighted Cadzow iterations]
\textbf{Input}: Time series $\tsX$, window length $L$, rank $r$,
stop criteria STOP1 for outer iterations and STOP2 for inner iterations.

\textbf{Result}:
Series $\widehat\tsS$ as an estimate of $\tsX$ by finite rank series of order $r$.

\begin{enumerate}
\item
$\bfY_0 = \calT \tsX$, $k=0$.
\item
Obtaining $\widehat\bfZ$ using algorithm ~\ref{alg:weightedSVD} applied to $\bfY_k$ for estimation of $\Pi_{\calM_r} \bfY_{k}$ with stop criterion STOP2.
\item
$\bfY_{k+1} = \Pi_\calH  \widehat\bfZ$, $k\leftarrow k+1$.
\item
If STOP1 then $\widehat\tsS = \calT^{-1} \bfY_k$.
\end{enumerate}
\end{algorithm}

\subsection{Extended Cadzow iterations}

The formulation of a problem in the case of this algorithm is quite different.
Formally, series are extended to both sides on $L-1$ measurements with some values having zero weights, i.e. new measurements are considered as gaps.
Thus, length of extended series $\widetilde\tsX$ is $N+2L-2$, and its trajectory matrix's $\widetilde\bfX$ size is $L$ by $N+L-1$.

For extended series a common scheme with weights $m_{i,j}=\calT \tsI$ is applied, where series $\tsI$ have ones in the place of given series and zeroes in positions of gaps, i.e.
\begin{equation*}
m_{i,j} = \begin{cases}
1 & 1 \le i+j-L \le N, \\
0 & \text{otherwise.}
\end{cases}
\end{equation*}

\begin{algorithm}[Extended Cadzow iterations]
\textbf{Input}: Time series $\tsX$, window length $L$, rank $r$,
stop criteria STOP1 for outer iterations and STOP2 for inner iterations,
left and right extention values, $\tsL_{L-1}$ и $\tsR_{L-1}$.

\textbf{Result}:
Series $\widehat\tsS$ as an estimate of $\tsX$ by finite rank series of order $r$.

\begin{enumerate}
\item
$\bfY_0 = \calT \widetilde\tsX$, where $\widetilde\tsX=(\tsL_{L-1}, \tsX, \tsR_{L-1})$, $k=0$.
\item
Obtaining $\widehat\bfZ$ using algorithm ~\ref{alg:weightedSVD} applied to $\bfY_k$ for estimation of $\Pi_{\calM_r} \bfY_{k}$ with stop criterion STOP2.
\item
$\widetilde\bfY_{k+1} = \Pi_\calH  \widehat\bfZ$, $k\leftarrow k+1$.
\item
If STOP1 then $\widehat\tsS = \calT^{-1} \bfY_k$, where $\bfY_k$ consists of matrix's $\widetilde\bfY_{k}$ columns from $L$-th to $N$-th.
\end{enumerate}
\end{algorithm}

%\begin{remark}
%Несмотря на то, что при добавлении точек формально получается задача \eqref{L-rank_task} с частично нулевыми весами, сходимость $\bfY_k$
%по подпоследовательностям имеет место. Это следует из того, что при построении $\bfY_k$ добавленные точки с нулевыми весами
%не участвуют.
%\end{remark}

\subsection{Oblique Cadzow iterations}

These algorithms could be applied if Proposition~\ref{prop:projS} conditions are carried out.

\begin{algorithm}[Oblique Cadzow iterations]
\label{alg:obliqueCadzow}
\textbf{Input}: Time series $\tsX$, window length $L$, rank $r$, matrix $\bfC=\diag(c_1,\ldots, c_K)$, where $K=N-L+1$,
stop criteria STOP1.

\textbf{Result}:
Series $\widehat\tsS$ as an estimate of $\tsX$ by finite rank series of order $r$.

\begin{enumerate}
\item
$\bfY_0 = \calT \tsX$, $k=0$.
\item
$\bfY_{k+1} = \Pi_\calH  \Pi_{\calM_r} \bfY_{k}$, $k\leftarrow k+1$, where
$\Pi_{\calM_r}$ is given by \eqref{eq:PiMr}.
\item
If STOP1 then $\widehat\tsS = \calT^{-1} \bfY_k$.
\end{enumerate}
\end{algorithm}

Since the original problem is approximation of time series with equal weights, a problem of finding a proper matrix $\bfC$ is considered.
It is found that there is no such matrix, so a few variants are considered below.

\subsubsection{Cadzow ($\alpha$) iterations}
\label{sec:cadzow_alpha}
Let us find a matrix $\bfC$ such that seminorm $\|\cdot\|_{\bfC}$ is corresponded with respect to distance with uniform weights $f_q(\tsY) = f(\tsY)$ taken a part in \eqref{L-rank_task}, i.e. Proposition~\ref{prop:projS} conditions and equality \eqref{qi_mi} under $q_i = 1$, $i=1,\ldots,N$ are carried out for $\bfM$.

Let us use the following lemma.

\begin{lemma}[\cite{Gillard2014}]
\label{zhiglemma}
Let $\tsX \in \sfX_N$, $\bfX = \calT(\tsX) \in \sfR^{L \times K}$. If $h = N/L$ is integer, then $\tr(\bfX \bfC \bfX^\rmT) = \tsX^\rmT \tsX$ where $\bfC$ is a diagonal matrix with diagonal elements are as follows:
\begin{equation*}
c_k = \begin{cases}
1, & \text{if} \quad k = jL+1 \quad \text{for some} \quad j = 0, \ldots, h-1, \\
0, & \text{otherwise}
\end{cases}.
\end{equation*}
\end{lemma}

This approach contains the important trouble --- zeroes are placed in a diagonal of matrix $\bfC$. Thus, matrix $\bfC$ rank deliberately less than $K$. Changing diagonal zeroes to some small $\alpha$ is suggested in \cite{Gillard2014} for solving the trouble.

Let
\begin{equation}\label{zhigweights}
c_k = \begin{cases}
1, & \text{если} \quad k = jL+1 \quad \text{for some} \quad j = 0, \ldots, h-1, \\
\alpha, & \text{otherwise,}
\end{cases}
\end{equation}
It fixes the trouble of rank, but makes convergecy of method for solving problem \eqref{L-rank_task} slower, as it could be seen further.

Let us denote Cadzow($\alpha$) iterations by algorithm~\ref{alg:obliqueCadzow} with matrix $\bfC$  given in \eqref{zhigweights}.

\paragraph{Degenerate case $\alpha=0$.}

Following $\bfM$ is obtained using equality \eqref{sk_mlk} with $\bfC$ with $\alpha=0$:
\begin{equation*}
\bfM = \begin{pmatrix}
1 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & \cdots & 1 \\
1 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & \cdots & 1 \\
\vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots & \cdots & \cdots & 1 \\
1 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & \cdots & 1
\end{pmatrix}.
\end{equation*}
So, distance is taken using only $h$ columns instead of $K$, while multiplicating by $\bfO_\bfC^{\rmT}$ sets to zero $K - h$ columns of $\bfC$.

\begin{remark}
Optimization problem with $\alpha=0$ corresponds to search of arbitrary (not necessary Hankel) matrix of rank not larger than $r$, which is nearest with respect to Frobenius norm to matrix
\be
\label{eq:traj_noinersect}
\begin{pmatrix}
x_1&x_{L+1}&\cdots&x_{K}\\
\vdots&\vdots&\cdots&\vdots\\
x_L&x_{2L}&\cdots&x_N
\end{pmatrix}.
\ee
This problem differs from problem of approximation by finite rank series.

Under $\alpha=1$ Cadzow($\alpha$) iterations coincide with usual Cadzow iterations.

\end{remark}


\subsubsection{Cadzow $\widehat\bfC$ iterations}
\label{sec:cadzow_hat}
Let us approach to the problem in terms of matrices: find matrix $\bfC$ such that obtained norm $\|\cdot\|_{\bfC}$ is nearest to norm $\|\cdot\|_{\bfM}$ with matrix $\bfM$ obtained in \eqref{Mw}. As it was mentioned,
only diagonal $\bfC$ is acceptable. Consider a set $\sfZ^{L \times K} \subset \sfR^{L \times K}$ --- matrices which elements are equal in columns. Then a reasonable decision is matrix $\bfZ \in \sfR^{L \times K}$, $\bfZ=(z_{l,k})$, $z_{l,k} = c_k$ such that
\begin{equation*}
\|\bfM - \bfZ\| \to \min_{\bfZ \in \sfZ^{L \times K}}.
\end{equation*}
The solution is averaging of matrix $\bfM$ by columns. As a result, obtained matrix $\hat \bfC$ consists of following diagonal elements:
\begin{equation}\label{my_s}
\hat c_k = \frac{1}{L}\sum_{l=1}^L m_{l, k}.
\end{equation}

Denote algorithm~\ref{alg:obliqueCadzow} with matrix $\bfC=\widehat\bfC$ by Cadzow $\widehat\bfC$ iterations.

\subsubsection{Correspondence between algorithms and weights $q_i$ in \eqref{L-rank_task}}

Define weights $q(\alpha)$ and $\hat q_i$ generated by matrix $\bfC$ in Cadzow ($\alpha$) and Cadzow $\widehat\bfC$ iterations respectively.

Following statements are true.

\begin{proposition}\label{zhigconseq}
Let $h = N/L$ be integer, matrix $\bfC$ is diagonal with diagonal elements given in \eqref{zhigweights},
where $0 \le \alpha \le 1$. Then weights $q_i(\alpha)$ defined in \eqref{qi_mi} are as follows:
\begin{equation*}
q_i (\alpha) = \begin{cases}
1 + (i - 1) \alpha & \text{для $i = 1, \ldots, L-1,$}\\
1 + (L - 1) \alpha & \text{для $i = L, \ldots, K-1,$}\\
1 + (N - i) \alpha & \text{для $i = K, \ldots, N.$}
\end{cases}
\end{equation*}
\end{proposition}
\begin{proof5}{\ref{zhigconseq}}
It is enough to sum $c_k$ by size of $i$-th anti-diagonal times.
\end{proof5}


\begin{proposition} \label{myweightstat}
Matrix weights $\hat c_k$ defined in \eqref{my_s} equals
\begin{equation*}
\hat c_k = \begin{cases}
\frac{1}{L}\left(\frac{k}{L} + \sum_{j=k}^{L-1} \frac{1}{j} \right),& k = 1, \ldots, L-1, \\
\hat c_{N - k + 1, N - k + 1}, & k = K - L + 2, \ldots K, \\
1/L, &\text{otherwise}.
\end{cases}
\end{equation*}
\end{proposition}

\begin{proof5}{\ref{myweightstat}}
It is enough to substitude $m_{l,k}$ defined in \eqref{Mw} to \eqref{my_s}.
\end{proof5}

To illustrate weights $\hat{q_i}$, let us formulate a proposition with simplifying conditions.

\begin{proposition} \label{myserweightstat}
Let $N \ge 4(L-1)$. Then weights $\hat{q_i}$ defined in \eqref{qi_mi}
are as follows:
\begin{equation*}
\hat{q_i} = \begin{cases}
\frac{i(i+1)}{2 L^2} + \frac{i}{L}(1 + H_{L-1} - H_i), &1 \le i \le L-1, \\
1 + \frac{2iL-i-i^2}{2L^2} + \frac{L-i}{L}(H_{L-1} - H_{i - L}), & L \le i \le 2L-1, \\
\hat{q}_{N-i+1}, &N-2L+2 \le i \le N, \\
1, &\text{otherwise},
\end{cases}
\end{equation*}
where $H_0 = 0$, and $H_i = \sum_{j=1}^i 1/j$ is $i$-th harmonic number.
\end{proposition}

\begin{proof5}{\ref{myserweightstat}}
For $1 \le i \le L-1$ we have
\begin{gather*}
\hat{q}_i = \sum_{j=1}^i \hat{c}_j = \sum_{j=1}^i \frac{1}{L}\left(\frac{j}{L} + \sum_{k=j}^{L-1}1/k\right)\! =
\frac{i(i+1)}{2L^2}+\frac{1}{L} \sum_{k = 1}^{L-1} \sum_{j=1}^{\min(k,i)} 1/k =\\= \frac{i(i+1)}{2L^2}+\frac{1}{L} \sum_{k = 1}^{L-1} \frac{\min(k,i)}{k} = \frac{i(i+1)}{2 L^2} + \frac{i}{L}(1 + H_{L-1} - H_i).
\end{gather*}
For $L \le i \le 2L-1$, by changing the order of summation too, we obtain
\begin{gather*}
\hat{q}_i = \sum_{j = 1}^L \hat{c}_{i-L+j} = \sum_{j = i - L + 1}^{L - 1} \hat{c}_j + \frac{i - L + 1}{L} =\\
=\frac{i - L + 1}{L} + \frac{1}{L^2} \sum_{j = i - L + 1}^{L-1}j + \frac{1}{L} \sum_{j = i-L + 1}^{L-1} \sum_{k=j}^{L-1}1/k =\\
=\frac{i - L + 1}{L} + \frac{2iL - =i - i^2}{2L^2} + \frac{1}{L} \sum_{k = i - L + 1}^{L - 1} \sum_{j = i - L + 1}^k 1/k =\\
=1 + \frac{2iL-i-i^2}{2L^2} + \frac{L-i}{L}(H_{L-1} - H_{i - L}).
\end{gather*}
Weights on the right part of series are calculated using symmetry, and for center it is necessary to sum $\hat{c}_k = 1/L$ $L$ times.
\end{proof5}

Normalized weights $q_i(\alpha)$ (so that sum equals 1) under $\alpha = 1$ (usual Cadzow iterations), $\alpha = 0$ (equal $q_i$), $\alpha = 0.1$,
 and $\hat{q}_i$ under $N = 40$, $L = 8$ are shown at Picture~\ref{img_weights}.
\begin{figure}[!h] \begin{center}
\includegraphics[width = \textwidth]{weights.pdf}\caption{Веса ряда}\label{img_weights}
\end{center}\end{figure}

\subsection{Comments to algorithms. Comparison}

Following methods are considered and compared further: Weighted Cadzow iterations, Extended Cadzow iterations, Cadzow ($\alpha$) iterations, $0< \alpha \leq 1$, coinciding with usual Cadzow iterations under $\alpha=1$,
and Cadzow $\widehat \bfC$ iterations.
Note that window length $L$ is a parameter for all following methods.

\begin{itemize}
\item
All methods are iterative, and, generally, they are not obliged to converge to global extremum in Least Squares problem (theoretically, even convergency by subsequences could be satisfied only; however, there is convergency by sequences in all numeric examples made). So, comparison of various methods applied to same problem is comprehended.
\item
Convergency of methods by sequences is carried out under following consumption: inner projecting problems in Weighted and Extended Cadzow iterations are solved exactly. This is a direct corollary from Theorem \ref{th:converg} statement for all methods excluding Extended Cadzow iterations, which have partially zero weights. But note that matrices $\tilde \bfY_k$ consisting of columns of matrices $\Pi_{\calM_r} \bfY_k$ from $L$-th to $N$-th and having posisive weight of each element are matrices of rank not larger than $r$, and their Frobenius norm is not larger than seminorm of original matrix: $\|\tilde \bfY_k\|_F \le \|\Pi_{\calM_r} \bfY_k\|_\bfM$, so it is bounded sequence which contains subsequence converging to needed set.
\item
Precision of estimation of excluded signal $\bfS$ is our point of interest, not a minimum in Least Squares problem. It is possible that excessively near approximation of given series could lead to overfitting, so quality of signal estimation could decrease.
\item
Weighted and Extended Cadzow iterations solves problem \eqref{L-rank_task} with equal weights $q_i$. Other methods work with weights with various degree of non-uniformity.
\item
Though, each iteration in Weighted and Extended Cadzow distinguishes by higher computational complexity because it uses one more iterative algorithm on each primary iteration.
\item
Computational complexity is described by complexity of one iteration and amount of iterations too. So, convergency speed is important from this point of view.
\item
It is known that computational complexity and application variety of one iteration of usual Cadzow iterations is important in real problems. Exactly, one iteration of Cadzow iterations is well-known Singular Spectrum Analysis (SSA) method, which could solve significally bigger variety of tasks than iterative method. Therefore, precision of signal estimation done by one iteration in all described methods is the point of interest too.
\item
Separability, which defines the ability of method to (approximately) find a signal using given series, is the one of SSA method concepts. So, separability is closely connected with precision of first iteration of method. Hence, it is naturally to assume that precision of first iteration is connected with method's convergency speed. Therefore, questions about separability are related to convergency speed of iterative methods.
\item
The connection between separability and window length $L$ is well researched for SSA method, see \cite{Golyandina2010}. Namely, optimal length window is near to half of a series length. Small window length $L$ tends to bad separability. Influence of parameter $\alpha$ in the class of Cadzow ($\alpha$) iterations is researched in Appendix (\ref{sec:app}) on the example of constant and harmonic signal. There it is shown that small value of $\alpha$ tends to bad separability, though it corresponds with approximately equal weights $q_i$ in problem \eqref{L-rank_task}. Generally, following effect becomes clear: parameters corresponding to more uniform weights tend to worst separability.
\item
It is possible that good convergency speed and precision of estimation of signal are properties that could not be satisfied simultaneously because of contradiction between uniformity of weights and separability, and slow convergency could tend to better solution of optimization problem too.
\end{itemize}

\begin{remark}
\label{rem:adjust}
Adjustment \eqref{eq:adjust} could be applied to $\widehat\tsS$ in all algorithms where $\|\cdot\|$ is usual Euclidean norm not depending to algorithms because norm corresponds to problem \eqref{L-rank_task} with equal weights $q_i$. Define by adjusted algorithms obtained algorithms.
For example, result of $k$-th iteration of Cadzow iterations could be written as $\widehat\tsS_k = \calT^{-1}(\Pi_\calH \Pi_{\calM_r})^k \calT \tsX$. Then result of $k$-th iteration of adjusted Cadzow iterations is $\widehat\tsS_k^*=\calA(\widehat\tsS_k)$.
\end{remark}

\section{Numeric examples}
\label{sec:simul}
Numeric results which confirm conclusions and considerations listed before are shown in this Section. Comparison was done in the case of extacting a sine and exponentially-modulated sine signals.
Since results are same at all, only result for sine signal is presented.

Following signal is taken:
\begin{equation*}
\tsS = (s_{1}, \ldots, s_N), \qquad s_{i} = 5\sin{\frac{2 \pi k}{6}}, \quad k = 1, \ldots, N, \quad N = 40
\end{equation*}
and signal $\tsX = \tsS + \tsN$ is considered, where  $\tsN$ is gaussian white noise with mean equals $0$ and variance equals $1$. Precision of signal estimation is done by extracting a root from mean of series measurements and 1000 realizations of mean square error (MSE) of series $\widehat\tsS$ from signal $\tsS$. This measure is called RMSE (root mean square error) of signal estimate. Comparison is done on same realizations of given series. Comparison results are significant at 5\% level of significance.

Firstly, consider a class of Oblique Cadzow iterations including usual Cadzow iterations. Picture \ref{img_cadzowspeed2} shows speed of convergency for various $\alpha$ and two various window lengths $L$ simultaneously. There are iteration number on X axis, and RMSE of signal estimate divided by number of measurements in series on Y axis.

\begin{figure}[!hhh]
\begin{center}
\includegraphics[width = \textwidth]{cadzowspeed_2.pdf}
\caption{RMSE of signal estimate depending by number of iterations.}
\label{img_cadzowspeed2}
\end{center}
\end{figure}

The best method in the limit is the one with slowest convergency speed, exactly, it is Cadzow (0.1) with window length $L=8$. Also, weights corresponding to this method are most uniform in Example.
Note that precision of all described methods does not differ strongly, from 0.33 ($\alpha=0.1$, $L=8$) in the best case to 0.37 ($\alpha=1$, $L=20$) in the worst case. However, error 0.38 is achieved at first iteration in the first case, while it takes 4--5 iterations to achieve error 0.38 in the second case.

Consider error distributions by series measurements in detail including Extended and Weighted Cadzow iterations too. ``Number of iterations equal 100'' is taken as stop criterion STOP1 (obtained results could be interpreted as limit), and stop criterion STOP2 for inner iterations is as follows:
 $\frac{\|\bfY_k - \bfY_{k+1}\|^2}{LK} < 10^{-4}$. Initial left and right extended values $\tsL_{L-1}$ и $\tsR_{L-1}$ in Extended Cadzow iterations are obtained using Vector SSA forecast \cite[chapter 2.3.1]{Golyandina.etal2001}.

Let us take window length $L=20$. There are measurement number on X axis and RMSE from real signal value at this measurement on Y axis at Pictures~\ref{fig:s1_it1}~and~\ref{fig:s1_it100}. Picture~\ref{fig:s1_it1} shows errors at first iteration, Picture~\ref{fig:s1_it100} shows errors at 100-th iteration. It is clear that Extended Cadzow iterations is the most precise method in both cases. Cadzow and Cadzow with $\widehat\bfC$ iterations are best at first iteration among methods without inner iterations. The best method in the limit (after 100-th iteration, results does not change significally further) is Cadzow with $\alpha=0.1$, and it is not surprising according to Picture~\ref{img_cadzowspeed2}.

\begin{figure}[!hhh]
\begin{center}
\includegraphics[width = 13cm]{s1_it1.pdf}
\caption{RMSE of signal estimate at each measurement at first iteration.}
\label{fig:s1_it1}
\end{center}
\end{figure}

\begin{figure}[!hhh]
\begin{center}
\includegraphics[width = 13cm]{s1_it100.pdf}
\caption{RMSE of signal estimate at each measurement at 100-th iteration.}
\label{fig:s1_it100}
\end{center}
\end{figure}

Consider Table~\ref{fintable} which shows method results, namely, RMSE as the measure of deviation from signal and initial series at first and 100-th iteration. $k$ is a number of iterations in Table, $\tsS$ means signal, $\tsX$ --- initial series; $L=20$. This Table confirms conclusions about comparison of methods by precision of signal estimation. Also it is seen that quality of initial series approximation does not always correspond with quality of signal estimation. For example, overfitting is clearly present for Cadzow ($0.1$) iterations at first iteration. However, methods are ordered identically by precision of approximation and precision of signal estimation in the limit.

Same experiment is done with adjusted algorithms (see remark~\ref{rem:adjust}), results are written in Table~\ref{fintable_improved}. It is clear that adjustment does not affect significally (Pictures are not brought for adjusted methods because adjustment does not influence noticeably by sight). Adjustment improves approximation of initial series in all cases by its construction, but influence to precision of signal approximation is ambiguous (it improves precision at 100th iteration; results are various at first iteration).

\begin{table}[!hhh]
\begin{center}
\caption{Comparison of methods by RMSE.}\label{fintable}
\begin{tabular}{|c|c|c|c|c|}
\hline
$P$: & $\tsS$, $k = 1$ & $\tsX$, $k = 1$ & $\tsS$, $k = 100$ & $\tsX$, $k = 100$  \\
\hline
Cadzow, $\alpha = 1$ & 0.3758 & 0.9195 & 0.3782 & 0.9664 \\
\hline
Cadzow, $\alpha = 0.1$ & 0.4329 & 0.7040 & 0.3311 & 0.9506 \\
\hline
Cadzow $\hat{\bfC}$ & 0.3655 & 0.8925 & 0.3559 & 0.9583 \\
\hline
Weighted Cadzow & 0.3644 & 0.8891 & 0.3455 & 0.9549 \\
\hline
Extended Cadzow & 0.3361 & 0.9030 & 0.3189 & 0.9471 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[!hhh]
	\begin{center}
		\caption{Comparison of adjusted methods by RMSE.}\label{fintable_improved}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			$P$: & $\tsS$, $T = 1$ & $\tsX$, $T = 1$ & $\tsS$, $T = 100$ & $\tsX$, $T = 100$  \\
			\hline
			Cadzow, $\alpha = 1$ & 0.3714 & 0.9175 & 0.3667 & 0.9622 \\
			\hline
			Cadzow, $\alpha = 0.1$ & 0.4385 & 0.7023 & 0.3276 & 0.9493 \\
			\hline
			Cadzow $\hat{\bfC}$ & 0.3626 & 0.8909 & 0.3478 & 0.9555 \\
			\hline
			Weighted Cadzow & 0.3640 & 0.8883 & 0.3380 & 0.9523 \\
			\hline
			Extended Cadzow & 0.3370 & 0.9030 & 0.3184 & 0.9469 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\section{Conclusion}
\label{sec:concl}
Known and new iterative algorithms for approximation of series by finite rank series with aim of estimating a signal from noisy signal by Least Squares method were considered in this Paper.

Large class of algorithms was reviewed with aim of achieving equal weights in Least Squares method. Equal weights were achieved by using inner iterations only, which converges to local minimum only and makes algorithms time-consuming. Usage of methods without inner iterations leads to approximately equal weights only.

Convergency of outer iterations by subsequences was proved for reviewed class of algorithms.

Results about precision and convergency speed of considered methods were obtained by simulation on the example of noisy sine signal. Results shows that the most precise is the most time-consuming method. Questions about relation of convergency speed, computational complexity and precision of methods were reviewed. Also, emphasis was placed on precision of estimate made by one iteration of methods.

Further work includes extended numeric and analytical research of methods, and achievement of more specific recomendations about relation of computational complexity and precision.

%\bibliographystyle{plain}
\bibliographystyle{gost2008}
\bibliography{zvonarev}
\addcontentsline{toc}{section}{References}

\section{Appendix: separability of constant and harmonic signal for Oblique Cadzow iterations}
\label{sec:app}

Let us introduce another characteristic of algorithms showing ability of decomposing time series into additive parts. Basic application of described algorithms is the problem of signal estimation, so that characteristic is important for obtaining a more precise estimate as much as possible.

Let $\bfC \in \sfR^{K \times K}$ be symmetric semidefinite matrix, $\tsX_1$ и $\tsX_2$ ---  two different time series of length $N$, $\bfX^1$, $\bfX^2$ --- their trajectory matrices. Then define \emph{correlation coefficient of $i$-th and $j$-th column} by:
\begin{equation}\label{col_corr}
\rho^c_{i,j} = \frac{(X^1_i, X^2_j)}{\|X^1_i\| \|X^2_j\|},
\end{equation}
where $X^k_i$ is $i$-th column of matrix $\bfX^k$, $k = 1, 2$, $(\cdot, \cdot)$ is euclidian inner product, $\|\cdot\|$ is euclidean norm. Define \emph{correlation coefficient of $i$-th and $j$-th row} by:
\begin{equation}\label{row_corr}
\rho^r_{i,j} = \frac{(X^{1,i}, X^{2,j})_\bfC}{\|X^{1,i}\|_\bfC \|X^{2,j}\|_\bfC},
\end{equation}
where $X^{k,i}$ is $i$-th row of matrix $\bfX^k$, $k = 1, 2$, and $(\cdot, \cdot)_\bfC$ is oblique inner product generated by matrix $\bfC$ in $\sfR^K$ defined as follows: $(X, Y)_\bfC = X \bfC Y^\sfT$, because $X$ and $Y$ are row-vectors, $\| \cdot \|_\bfC$ is the norm with respect to this inner product. Let us say that series $\tsX_1$ and $\tsX_2$ are \emph{weak $\varepsilon$---separatable} if
\begin{equation}\label{weak_sep_eq}
\rho = \max\Big(\max_{1 \le i,j \le K}|\rho^c_{i,j}|, \max_{1 \le i,j \le L}|\rho^r_{i,j}|\Big) < \varepsilon.
\end{equation}
We are interested in order of $\varepsilon$ for various matrices $\bfC$ and series $\tsX_1 = (c, c, \ldots)$ --- some constant series and $\tsX_2 = (\cos(2 \pi \omega k), k = 1, 2, \ldots)$, and also for various $L$ and $K$ under assumption that only first $N = L + K - 1$ measurements of series are taken. When $\bfC$ is identity matrix, the answer is known: $\varepsilon$ have order $1/\min(L,K)$, i.e. speed of convergency have order $1/N$ for $L$ proportional to $N$.
This result could be found in \cite[Chapter 6.1]{Golyandina.etal2001}. This result relates to the precision of first iteration of Cadzow iterations.

Order of separability of Cadzow ($\alpha$) iterations, introduced in Chapter~\ref{sec:cadzow_alpha}, is considered in following proposition.

\begin{proposition}
\label{prop:separ1}
Let $\tsX_1 = (c, c, \ldots)$ be some constant and $\tsX_2 = (\cos(2 \pi \omega k), k = 1, 2, \ldots)$, where $0<\omega <0.5$, $L,K\ra \infty$ such that $h = h_L = N/L$, where $N=L+K-1$, is integer, and $\bfC$ defined in Cadzow ($\alpha$) iterations, i.e.  $\bfC$ is diagonal matrix with following diagonal elements:
\begin{equation*}
c_k = \begin{cases}
1, & \text{если} \quad k = jL+1 \quad \text{for some} \ j = 0, \ldots, h-1,\\
\alpha, & \text{otherwise},
\end{cases}
\end{equation*}
where $0 \le \alpha \le 1$. Then $\rho$ have order $\max(\frac{1}{L}, \frac{(1-\alpha)C_{L,K}+\alpha}{(1-\alpha)N/L+\alpha K})$, where order of $C_{L,K}$
could vary from $O(1)$ to $O(N/L)$ depending on how $K$ tends to infinity.
\end{proposition}

\begin{proof5}{\ref{prop:separ1}}
It is necessary to evaluate an order of following values:
\begin{gather*}
\rho^c_{i,j} = \frac{\sum_{k=j}^{j + L - 1} \cos(2 \pi \omega k)}{\sqrt{L (\sum_{k=j}^{j + L - 1} \cos^2(2 \pi \omega k))}},\\ \rho^r_{i,j} = \frac{\sum_{k=1}^K c_k\cos(2 \pi \omega (j + k - 1))}{\sqrt{(\sum_{k=1}^K c_k) (\sum_{k=1}^K c_k\cos^2(2 \pi \omega (j + k - 1)))}}.
\end{gather*}
Let us use following facts to prove the proposition:
\begin{gather*}
\sum_{k=1}^n \cos(ak + b) = \csc(a/2) \sin(an / 2) \cos \left(\frac{an + a + 2b}{2} \right), \\
\sum_{k=1}^n \cos^2(ak + b) = \frac{1}{4}(\csc(a) \sin(2an + a + 2b) -\\ - \csc(a)\sin(a + 2b) + 2n),
\end{gather*}
for any real $a, b$ and positive integer $n$.
Thus when series $\tsX_2$ is not constant, numerator in $\rho^c_{i,j}$ have order $O(1)$, and denominator --- $O(L)$.
First part is proved, and its proof is exactly analogous to the case when $\bfC$ is identity matrix.

To prove second part choose a sum for $k$ such that $c_k=1$ separately:
\begin{gather*}
\sum_{k=1}^K c_k\cos(2 \pi \omega (j + k - 1)) = (1-\alpha) \sum_{\substack{1 \le k \le K: \\ c_k = 1}}\cos(2 \pi \omega (j + k - 1)) +\\ +\sum_{1 \le k \le K}\alpha \cos(2 \pi \omega (j + k - 1)) = (1-\alpha) C_{L,K} + \alpha\, O(1),
\\
%\end{gather*}
%\begin{gather*}
\sum_{k=1}^K c_k = (1-\alpha) N/L + \alpha K,
\\
%\end{gather*}
%\begin{gather*}
\sum_{k=1}^K c_k\cos^2(2 \pi \omega (j + k - 1)) = (1-\alpha)\sum_{\substack{1 \le k \le K: \\ c_k = 1}}\cos^2(2 \pi \omega (j + k - 1)) +\\ +\sum_{1 \le k \le K }\alpha \cos^2(2 \pi \omega (j + k - 1)) = (1-\alpha) O(N/L) + \alpha\, O(K).
\end{gather*}
In the worst case when $L\omega$ is integer we obtain that $\cos(2 \pi \omega (j + k - 1))$ is a constant not depending from $j$ and $k$, and then $C_{L,K}$ has order $O(N/L)$.
\end{proof5}

Thus even in the worst case when $C_{L,K}$ has order $O(1)$, separability of constant and sine signal becomes worse than in usual Cadzow iterations: for $\alpha$ close to 0, the most optimal chose for $L$ is $L \approx \sqrt{N}$, so order of separability equal to $1/\sqrt{N}$ is obtained.

Now let us consider Cadzow with $\widehat\bfC$ iterations introduced in Chapter~\ref{sec:cadzow_hat}.

\begin{proposition}
\label{prop:separ2}
Let $\tsX_1 = (c, c, \ldots)$ be some constant signal and $\tsX_2 = (\cos(2 \pi \omega k), k = 1, 2, \ldots)$, where $0<\omega <0.5$, $L,K\ra \infty$ and $\bfC$ defined in Cadzow with $\widehat\bfC$ iterations.
 Then $\rho$ have order $\max \left(1/L, \frac{H_L}{\sqrt{NK}} \right)$ as $L, K \to \infty$, where $H_L$ is $L$-th harmonic number.
\end{proposition}

\begin{proof5}{\ref{prop:separ2}}
It is necessary to evaluate an order of following values:
\begin{gather*}
\rho^c_{i,j} = \frac{\sum_{k=j}^{j + L - 1} \cos(2 \pi \omega k)}{\sqrt{L (\sum_{k=j}^{j + L - 1} \cos^2(2 \pi \omega k))}}, \\ \rho^r_{i,j} = \frac{\sum_{k=1}^K \hat c_k\cos(2 \pi \omega (j + k - 1))}{\sqrt{(\sum_{k=1}^K \hat c_k) (\sum_{k=1}^K \hat c_k\cos^2(2 \pi \omega (j + k - 1)))}}.
\end{gather*}
 The order of $\rho^c_{i,j}$ was already obtained in proof of Proposition~\ref{prop:separ1}, so let us just move to $\rho^r_{i,j}$. Consider the correlation of first rows only --- proof for remained rows is absolutely analogous. Consider a numerator $\rho^r_{1,1}$:
\begin{gather*}
\sum_{k=1}^K \hat c_k\cos(2 \pi \omega k) = \sum_{k=1}^{L-1} \hat c_k\cos(2 \pi \omega k) + \sum_{k=L}^{K - L + 1} \frac{\cos(2 \pi \omega k)}{L} +\\+ \sum_{k=K - L + 2}^{K} \hat c_k\cos(2 \pi \omega k) = I_1 + I_2 + I_3,
\end{gather*}
which is divided by three parts. For $I_2$ an estimate $O(1/L)$ is carried out, and the proof for $I_3$ is analogic to proof for $I_1$:
\begin{gather*}
|I_1|=\bigg|\sum_{k=1}^{L-1}\frac{1}{L}\left(\frac{k}{L} + \sum_{j=k}^{L-1} \frac{1}{j} \right) \cos(2 \pi \omega k)\bigg| =\\= \bigg|\sum_{k=1}^{L-1} \frac{k \cos(2 \pi \omega k)}{L^2} +  \frac{1}{L}\sum_{k = 1}^{L-1}\sum_{j = k}^{L-1}\frac{\cos(2 \pi \omega k)}{j}\bigg| \le \\ \le
\bigg|\sum_{k=1}^{L-1} \frac{k \cos(2 \pi \omega k)}{L^2}\bigg| + \bigg|\frac{1}{L}\sum_{k = 1}^{L-1}\sum_{j = k}^{L-1}\frac{\cos(2 \pi \omega k)}{j}\bigg|.
\end{gather*}
Using a fact that
\begin{gather*}
\sum_{k=1}^n k \cos(ak + b) = -\frac{1}{4}\csc^2(a/2)(-(n+1)\cos(an+b) + \\ + n\cos(an + a + b) + \cos b),
\end{gather*}
obtain that:
\begin{gather*}
\bigg|\sum_{k=1}^{L-1} \frac{k \cos(2 \pi \omega k)}{L^2}\bigg| = O(1/L), \quad
\bigg|\frac{1}{L}\sum_{k = 1}^{L-1}\sum_{j = k}^{L-1}\frac{\cos(2 \pi \omega k)}{j}\bigg| = \\ =\bigg|\frac{1}{L}\sum_{j = 1}^{L-1}\sum_{k = 1}^{j}\frac{\cos(2 \pi \omega k)}{j}\bigg| \le \bigg|\frac{1}{L}\sum_{j = 1}^{L-1}\frac{d}{j}\bigg| = O \left(\frac{H_L}{L} \right),
\end{gather*}
where $d$ is some constant.

Following sums are considered for the denominator:
\begin{equation*}
\sum_{k=1}^K \hat c_k = N / L
\end{equation*}
by definition, and the following part is just estimated lower:
\begin{equation*}
\sum_{k=1}^K \hat c_k\cos^2(2 \pi \omega (j + k - 1)) \ge \sum_{k=1}^K \frac{1}{L}\cos^2(2 \pi \omega (j + k - 1)) = O \left(\frac{K}{L} \right).
\end{equation*}
\end{proof5}


\end{document}
