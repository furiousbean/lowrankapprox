\documentclass[sii]{ipart}

\usepackage{amsmath,amssymb,amsthm,amscd,amsfonts}
\usepackage[numbers,square]{natbib}
\RequirePackage{hyperref}
\usepackage{graphicx}

% will be filled by editor:
\pubyear{2015}
\volume{0}
\issue{0}
\firstpage{1}
\lastpage{1}
%\arxiv{}

% put your definitions there:
\startlocaldefs
\hyphenation{Struc-tu-red}
\hyphenation{Ran-do-mized}
\hyphenation{Ma-xi-mi-za-tion}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\tr}{tr}
\providecommand*{\BibDash}{}

\def\rank{\mathop{\mathrm{rank}}}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{algorithm}{Algorithm}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\input{letters}

\input{newcommands}
\endlocaldefs


\begin{document}

\begin{frontmatter}

% "Title of the Paper"
\title{Iterative algorithms for weighted finite-rank time-series approximation}
\runtitle{Iterative algorithms for weighted finite-rank time-series approximation}

% indicate corresponding author with \corref{}
% \author{\fnms{John} \snm{Smith}\thanksref{t2}\corref{}\ead[label=e1]{smith@foo.com}\ead[label=e2,url]{www.foo.com}}
% \thankstext{t2}{Thanks to somebody}
% \address{line 1\\ line 2\\ \printead{e1}\\ \printead{e2}}

\author{\fnms{Nikita} \snm{Zvonarev}\ead[label=e1]{nikitazvonarev@gmail.com}}
\address{Department of Statistical Modelling\\
	Faculty of Mathematics and Mechanics\\
	St. Petersburg State University\\
	Universitetskij pr 28, 198504, St.Petersburg, Russia\\ \printead{e1}}
\and
\author{\fnms{Nina} \snm{Golyandina}\ead[label=e2]{nina@gistatgroup.com}}
\address{Department of Statistical Modelling\\
	Faculty of Mathematics and Mechanics\\
	St. Petersburg State University\\
	Universitetskij pr 28, 198504, St.Petersburg, Russia\\ \printead{e2}}

\runauthor{N. Zvonarev and N. Golyandina}

\begin{abstract}
The problem of time series approximation by series of finite rank is considered from the viewpoint of signal extraction. For signal estimation,
a weighted least-squares method is applied to the trajectory matrix of the considered time series. Matrix weights are chosen to obtain equal or approximately equal weights in the equivalent problem of time series least-squares approximation.
Several new methods are suggested and examined together with the Cadzow's iterative method. Questions of convergence, computational complexity and accuracy are considered for the proposed methods. The methods are compared on numeric examples.
\end{abstract}

%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\begin{keyword}
\kwd{time series}
\kwd{Cadzow iterations}
\kwd{time series of finite rank}
\kwd{weighted total least squares}
\kwd{oblique SVD-factorization}
\kwd{singular spectrum analysis}
\end{keyword}

% history:
\received{\smonth{1} \syear{0000}}

%\tableofcontents

\end{frontmatter}

\section{Introduction}
Consider the problem of extracting a signal $\tsS~=~(s_1, \ldots, s_N)$ from an observed noisy series $\tsX = \tsS + \tsN$, where $\tsS$ is governed by a \emph{linear recurrence relation} (LRR) of order $r$:
\begin{equation*}
s_n = \sum_{i = 1}^{r} a_i s_{n-i}, \quad n = r + 1, \ldots, N.
\end{equation*}
Generally, governed-by-LRR series may be written in a parametric form
\begin{equation} \label{parametricform}
s_n = \sum_i P_i(n) \exp(\alpha_i n) \cos(2 \pi \omega_i n + \psi_i),
\end{equation}
where $P_i(n)$ are polynomials of $n$. However, parametric regression approach for the problem does not lead to accurate estimation of parameters due to their large amount and instability of estimators.

It is known that methods based on signal subspace estimation (subspace-based methods) works well \cite{Broomhead.King1986, Vautard.etal1992, Elsner.Tsonis1996, Golyandina.etal2001}. These subspace-based methods use the following approach. Let us fix a window length $L$, $1 \le L \le N$, set $K = N - L + 1$, and build the trajectory matrix for the series $\tsS$:
\begin{equation*}
\bfS = \begin{pmatrix}
s_1 & s_2 & \ldots & s_K \\
s_2 & s_3 & \ldots & s_{K + 1} \\
\vdots & \vdots & \vdots & \vdots \\
s_L & s_{L + 1} & \ldots & s_N
\end{pmatrix}.
\end{equation*}
Note that $\bfS\in \calH$, where $\calH$ is the set of Hankel matrices with equal values on their anti-diagonals $i+j=\mathrm{const}$.
If $\tsS$ is governed by a minimal LRR of order $r$, $r < \min(L, K)$, then $\rank \bfS = r$ and therefore $\bfS$ is a Hankel matrix of low-rank $r$. The column space of $\bfS$, that is, the  signal subspace, provides estimates of $\alpha_i$ and $\omega_i$ in \eqref{parametricform} by the ESPRIT method \cite{Roy.Kailath1989, Golyandina.Zhigljavsky2012} applied to $\tsS$.

Let $\bfX$ be the trajectory matrix of the series $\tsX$. Then the problem of estimation of $\tsS$ and the signal subspace can be considered as a problem of approximation of the matrix $\bfX$ by a Hankel matrix of rank not larger than $r$:
\begin{equation}\label{introd_task}
\|\bfX - \bfY\|^2_\rmF \to \min_{\substack{\rank \bfY \le r \\ \bfY \in \calH}},
\end{equation}
where $\|\cdot\|_\rmF$ is the Frobenius norm.

A lot of papers are devoted to this problem, e.g., \cite{Cadzow1988, Markovsky2011, Usevich.Markovsky2014, Gillard.Zhigljavsky2013} among them, where the problem is called Structured Low-Rank Approximation. Approaches to solving the problem are iterative; e.g., the Cadzow's iterative method \cite{Cadzow1988} consists of alternating projections to the set of Hankel matrices and matrices of rank not larger than $r$. The target function is not unimodal in such class of problems, and convergence to global minimum is not guaranteed; despite this, problem \eqref{introd_task} is considered to be well-researched, though it has many open questions yet.

Note that problem \eqref{introd_task} is equivalent to the problem of weighted approximation of the series $\tsX = (x_1, \ldots, x_N)$:
\begin{equation}\label{introd_task_2}
\sum_{i = 1}^N w_i(x_i - y_i)^2 \to \min_{\substack{\tsY: \rank \bfY \le r \\ \bfY \in \calH}},
\end{equation}
where
\begin{equation}
\label{eq:w}
w_i = \begin{cases}
i & \text{for $i = 1, \ldots, L-1,$}\\
L & \text{for $i = L, \ldots, K,$}\\
N - i + 1 & \text{for $i = K + 1, \ldots, N.$}
\end{cases}.
\end{equation}

Weights at both ends of the series are smaller than that in the center, i.e. the ordinary least-square problem \eqref{introd_task} for matrices corresponds to a weighted least-squares problem for series.

The aim of this paper is to consider methods which solve problem \eqref{introd_task_2} with equal weights instead of $w_i$ and then to compare the constructed methods in terms of accuracy of the signal estimation. All described methods are iterative.
If one is interested in a signal estimate, which is not necessarily governed by an LRR, then the first iteration can be taken as a low-cost estimator of the signal.
Hence, the described methods are compared by accuracy of the signal estimation at the first iteration and in the limit. Note that Singular Spectrum Analysis (SSA) \cite{Broomhead.King1986, Vautard.etal1992, Elsner.Tsonis1996, Golyandina.etal2001, Ghil.etal2002, Golyandina.Zhigljavsky2012} applied to the problem of signal estimation can be represented as the first iteration of the Cadzow method.

The structure of the paper is as follows.  In Section~\ref{sec:lowrank_appr}, a problem of approximation of a matrix by a Hankel rank-deficient matrix is considered. Common structure of iterative alternating projection algorithms is described, approaches to construction of the projectors are given, the convergence theorem is proved.

In Section~\ref{sec:ts_matrices}, relation between the problems of approximation of time series and of their trajectory matrices is described, relationship between weights in equivalent weighted least-squares problems is also given. Section~\ref{sec:alg} contains the suggested time-series approximation algorithms. In Section~\ref{sec:simul}, a numeric comparison of algorithms on a typical example is performed.

The paper is summarized and conclusions are drawn in Section~\ref{sec:concl}. Supplementary results on SSA separability, which has a connection with convergence rate, are proved in Appendix~\ref{sec:app}.

\section{Approximation by rank-deficient Hankel matrices}
\label{sec:lowrank_appr}
\subsection{Common scheme of iterations}
Consider a problem of approximation of a matrix $\bfX$ by a Hankel rank-deficient matrix with respect to some (semi)norm $\|\cdot\|$. Define by $\spaceR^{L\times K}$ the space of matrices of size $L \times K$, $\calM_r\subset \spaceR^{L\times K}$ is the set of matrices of rank not larger than $r$,
$\calH \subset \spaceR^{L\times K}$ is the set of Hankel matrices.
Note that  $\calM_r$ is not nor linear, nor convex set. However, $\calM_r$ is closed with respect to scalar multiplication, i.e.
if $\bfZ\in \calM_r$, then $a\bfZ\in \calM_r$ for any $a$.
The space $\calH$ is linear.

The problem is
\be
\label{eq:gen_task}
\|\bfX - \bfY\| \to \min_\bfY, \mbox{\ where\ } \bfY \in \calH \cap \calM_r.
\ee

To present the algorithm's scheme for this problem, let us introduce the projectors to the subspaces $\calM_r$ and $\calH$ with respect to norm $\|\cdot\|$: $\Pi_{\calM_r}$ is the projector to $\calM_r$,
$\Pi_{\calH}$ is the projector to $\calH$.
Note that the projection to $\calM_r$ can not be defined uniquely, but further we suppose that in the case of ambiguity any possible value is chosen. The projector to $\calH$ is evidently orthogonal, while $\Pi_{\calM_r}$ is orthogonal due to the following proposition.

\begin{proposition} \label{pythaprop}
	Let $\calX$ be Hilbert space, $\calM\in \calX$ be a subset closed with respect to scalar multiplication, $\Pi_\calM$ be the projection operator to $\calM$. Then for any $x \in \calX$ the Pythagorean theorem is true: $\|x\|^2~=~\|x~-~\Pi_\calM x\|^2~+~\|\Pi_\calM x\|^2$.
\end{proposition}
\begin{proof}
	By conditions, the set $\calM$ can be represented as $\calM = \bigcup\limits_{l \in \calL}l$, where $\calL$ is the set of all lines lying in $\calM$ and passing through $0$. Then the projection can be described as follows: to begin with, we choose a line $l$ such that $\dist(x, l) \rightarrow \min\limits_{l \in \calL}$; then, $y = \Pi_\calM x$ is the orthogonal projection of $x$ to the line $l$, which is a linear subspace. Proposition is proved.
\end{proof}

The iterative method of alternating projections for the problem \eqref{eq:gen_task} is given by the following iteration step:
\be
\label{eq:iter}
\bfY_{k+1}=\Pi_\calH \Pi_{\calM_r} \bfY_{k}, \mbox{\ where\ } \bfY_{0}=\bfX.
\ee

Consider the convergence of the sequence~\eqref{eq:iter}.

\begin{theorem}
	\label{th:converg}
	\begin{enumerate}
		Let the space $\calM_r$ be closed in the topology generated by norm $\|\cdot\|$. Then
		\item $\|\bfY_k - \Pi_{\calM_r}\bfY_k\| \to 0$ as $k \to +\infty$, $\|\Pi_{\calM_r}\bfY_k - \bfY_{k+1}\| \to 0$ as $k \to +\infty$.
		\item There exists a convergent subsequence of matrices $\bfY_{i_1}, \bfY_{i_2}, \ldots$ such that its limit $\bfY^*$ belongs to $\calM_r \cap \calH$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let us use inequalities from \cite[ineq. (4.1)]{Chu.etal2003}:
	\begin{multline}
	\label{chuprop}
	\|\bfY_k - \Pi_{\calM_r} \bfY_k\| \ge \|\Pi_{\calM_r} \bfY_k - \bfY_{k + 1}\| \ge \\ \|\bfY_{k+1} - \Pi_{\calM_r} \bfY_{k + 1}\|.
	\end{multline}
	
	\begin{enumerate}
		\item According to inequalities \eqref{chuprop}, the sequences $\|\bfY_k~-~\Pi_{\calM_r} \bfY_k\|$, $k = 1, 2, \ldots$, and $\|\Pi_{\calM_r} \bfY_k - \bfY_{k + 1}\|$, $k = 1, 2, \ldots$, are non-increasing. It is obvious that they are limited below by zero. Therefore, they have the same limit $c$ due to \eqref{chuprop}.
		
		Let us prove that $c = 0$ assuming the opposite $c > 0$. Then there exists $d > 0$ such that $\|\bfY_k - \Pi_{\calM_r} \bfY_k\| > d$ and $\|\Pi_{\calM_r} \bfY_k - \bfY_{k + 1}\| > d$ for any $k = 1, 2, \ldots$. In accordance to Proposition~\ref{pythaprop}, the following equalities are valid:
		\begin{multline*}
		\|\bfY_k\|^2 = \|\Pi_{\calM_r} \bfY_k\|^2 + \|\bfY_k - \Pi_{\calM_r} \bfY_k\|^2 =\\ \|\bfY_k - \Pi_{\calM_r} \bfY_k\|^2 + \|\Pi_{\calM_r} \bfY_k - \bfY_{k + 1}\|^2 + \|\bfY_{k + 1}\|^2.
		\end{multline*}
		Thus, $\|\bfY_{k+1}\|^2 < \|\bfY_k\|^2 - 2d^2$. Expanding this inequality by the same way, we obtain that $\|\bfY_{k+j}\|^2 < \|\bfY_k\|^2 - 2 j d^2$ for any $j = 1, 2, \ldots$. Choose $k = 1$, and $j = \lceil \|\bfY_k\|^2 / (2d^2) \rceil + 1$. Then $\|\bfY_{k+j}\|^2 < 0$, what is impossible. Thus, $c=0$.
		\item Consider the sequence $(\Pi_{\calM_r} \bfY_k,\, k = 1, 2, \ldots)$. It is bounded because $\|\Pi_{\calM_r} \bfZ\| \le \|\bfZ\|$ and $\|\Pi_{\calH} \bfZ\| \le \|\bfZ\|$ for any $\bfZ \in \sfR^{L \times K}$ (by Proposition \ref{pythaprop}). Then a convergent subsequence $(\Pi_{\calM_r} \bfY_{i_k})$ can be chosen; denote by $\bfY^*\in\calM_r$ its limit, wherein $\|\Pi_{\calM_r} \bfY_{i_k} - \bfY_{i_k + 1}\| = \|\Pi_{\calM_r} \bfY_{i_k} - \Pi_\calH \Pi_{\calM_r} \bfY_{i_k}\| \to 0$ as $k \to + \infty$. Taking into consideration that $\|\bfZ - \Pi_\calH \bfZ\|$ is a composition of continuous mappings, we obtain that $\|\bfY^* - \Pi_\calH \bfY^*\| = 0$, $\bfY^* \in \calM_r \cap \calH$. Finally, $\Pi_\calH$ is continuous mapping and therefore the sequence $(\Pi_\calH \Pi_{\calM_r} \bfY_{i_k})$ converges to $\bfY^*$. Thus, $\bfY_{i_k + 1}$ is a required subsequence.
	\end{enumerate}
\end{proof}

Below we will consider the norms generated by weighted Frobenius inner products in the form, which is parameterized by a matrix $\bfM$ with non-negative entries $m_{i,j}$: 
\be
\label{eq:w_inner_prod}
\langle\bfY, \bfZ\rangle_M = \sum_{l = 1}^L \sum_{k = 1}^K m_{l, k} y_{l, k} z_{l, k}.
\ee

It is well known that $\calM_r$ is closed with respect to the conventional Frobenius norm. It can be proved that $\calM_r$ is closed with respect to any weighted norm \eqref{eq:w_inner_prod} when $m_{l,k} > 0$ and therefore the conclusion of Theorem~\ref{th:converg} holds in the case of positive weights.


\subsection{Evaluation of projections}

Let us consider the weighted norm $\|\cdot\|_\bfM$ generated by \eqref{eq:w_inner_prod}, that is, $\|\bfX\|^2 = \|\bfX\|^2_\bfM = \sum_{l = 1}^L \sum_{k = 1}^K m_{l, k} x^2_{l, k}$.

\subsubsection{Projector $\Pi_\calH$.} It is easy to show that $\Pi_\calH$
can be evaluated explicitly using the following proposition.

\begin{proposition}
	For $\widehat{\bfY}=\Pi_\calH \bfY$
	\begin{equation*}
	\hat{y}_{ij} = \frac{\sum_{l,k:\, l+k=i+j} m_{l,k} y_{l,k}}{\sum_{l,k:\, l+k=i+j} m_{l,k}}.
	\end{equation*}
\end{proposition}

It is impossible to derive explicit form of $\Pi_{\calM_r}$ in the case of arbitrary weights.
Consider one specific case and suggest an iterative approach to the general case.

\subsubsection{Case of explicit form of projector $\Pi_{\calM_r}$.}
\label{sec:obliqueSVD}
For equal weights $m_{ij}=1$, denote $\Pi_r=\Pi_{\calM_r}$.
It is well-known that the projector $\Pi_{r} \bfY$ can be evaluated as a sum of $r$ leading components of the singular value decomposition (SVD) of the matrix $\bfY$: let $\bfY = \bfU \mathbf{\Sigma} \bfV^\rmT$ be the SVD, where $\bfU$ is an orthogonal matrix of size $L \times L$, $\mathbf{\Sigma}$ is a quasi-diagonal matrix of size $L \times K$ with non-negative diagonal elements in non-increasing order, and $\bfV$ is an orthogonal matrix of size $K \times K$. More precisely, let $L\le K$ and denote $\Sigma = (\sigma_1, \ldots, \sigma_L)$ the vector consisting of diagonal elements of matrix $\mathbf{\Sigma}$. Denote by $\mathbf{\Sigma}_r = (\sigma^r_{l k})$ the following matrix:
\begin{equation*}
\sigma^r_{i j} = \begin{cases}
\sigma_i & \text{if $i = j, i \le r,$}\\
0 & \text{otherwise}.
\end{cases}
\end{equation*}
Then the projection can be evaluated as $\Pi_{r} \bfY  = \bfU \mathbf{\Sigma}_r \bfV^\rmT$.
The next proposition describes the case when evaluation of the projector is reduced to application of the projector $\Pi_r$.

\begin{proposition}
	\label{prop:projS}
	Let $\bfC$ be a symmetric positive semidefinite matrix of size $K \times K$ such that $\|\bfZ\|_\bfM^2 = \tr(\bfZ \bfC \bfZ^\rmT)$ for any matrix $\bfZ\in \spaceR^{L\times K}$.
	Suppose that the column space of a matrix $\bfY$ lies in the column space of the matrix $\bfC$.
	Then
	\be
	\label{eq:PiMr}
	\Pi_{\calM_r} \bfY = (\Pi_r \bfB) (\bfO_\bfC^{\rmT})^\dagger,
	\ee
	where $\bfO_\bfC$ is a matrix such that $\bfC = \bfO_\bfC^{\rmT}\bfO_\bfC$,
	$\bfB = \bfY \bfO_\bfC^{\rmT}$, $(\bfO_\bfC^{\rmT})^\dagger$ denotes  Moore-Penrose pseudoinverse to the matrix $\bfO_\bfC^{\rmT}$.
\end{proposition}
\begin{proof}
	The proof is a direct consequence of the fact that the considered norm is generated by an oblique inner product in the row space of $\bfY$, see details in \cite{Golyandina2013}.
\end{proof}

\begin{remark}
\label{rem:diagC}
	The conditions $\|\bfZ\|_\bfM^2 = \tr(\bfZ \bfC \bfZ^\rmT)$ of Proposition~\ref{prop:projS} can be fulfilled only if $\bfC$ is diagonal.
\end{remark}

\subsubsection{Projector $\Pi_{\calM_r}$ in general case.}
Since the projector can not be found explicitly for arbitrary weights $m_{ij}$, iterative algorithms are used in the general case.
One of these algorithms is described in \cite{Srebro2003}. Denote by $\odot$ the element-wise matrix product.

\begin{algorithm}
	\label{alg:weightedSVD}
	\textbf{Input}: initial matrix $\bfY$, rank $r$, weight matrix $\bfM$,
	stop criterion STOP.
	
	\textbf{Result}:
	Matrix $\widehat\bfY$ as an estimate of $\Pi_{\calM_r} \bfY$.
	
	\begin{enumerate}
		\item
		$\bfY_0 = \bfY$, $k=0$.
		\item
		$\bfY_{k+1} = \Pi_r(\bfY \odot \bfM + \bfY_{k} \odot (\bfQ -  \bfM))$, where
		$\bfQ \in \sfR^{L \times K}$ is the matrix of all ones; 
%,  $\bfQ = \begin{pmatrix}
%		1 & \cdots & 1 \\
%		\vdots & \ddots & \vdots \\
%		1 & \cdots & 1
%		\end{pmatrix}$;
        \quad $k\leftarrow k+1$.
		\item
		If STOP, then $\widehat\bfY = \bfY_k$; else go to 2.
	\end{enumerate}
\end{algorithm}

Note that in the case, when $m_{ij}$ are equal to either 0 or 1, this algorithm is an EM-algorithm \cite{Srebro2003};
hence, properties of EM-algorithms are carried out and the sequence $\bfY_k$ converges to local minimum. Formally, it does not matter what values are in $\bfY$ at positions of zero weights. However, these values can influence the algorithm's convergence rate and the limiting values.

\section{Time series and problem of matrix approximation}
\label{sec:ts_matrices}
\subsection{Problem statement for time series}
Consider a time series $\tsX = (x_1, \ldots, x_N)$ of length $N \ge 3$. Let us fix a window length $L$, $1 < L < N$, denote $K = N - L + 1$. Also consider a sequence of \emph{$L$-lagged vectors}:
\begin{equation}\label{l_lagged}
X_i = (x_i, \ldots, x_{i + L - 1})^\rmT, \qquad i = 1, \ldots, K.
\end{equation}
Define by \emph{$L$-trajectory matrix} of series $\tsX$ the matrix $\bfX = [X_1:\ldots:X_K]$.

Suppose that $0 \le r \le L$. We say that the series $\tsX$ \emph{has $L$-rank $r$} if its $L$-trajectory matrix $\bfX$ has rank $r$.
%\footnote{Needed to be made as a definition}

Note that the series $\tsX$ can have $L$-rank $r$ only when
\begin{equation}
r \le \min(L, K). \label{min_condition}
\end{equation}
%We say that the window length $L$ is \emph{admittable} for fixed $r$ if it satisfies condition \eqref{min_condition}.

Further we suppose that $L$ is not larger than $K$, since the problems of approximation of $\bfX$ and $\bfX^\rmT$ coincide.

Let $\sfX_N$ be the set of time series of length $N$, $\sfX_N^r$ be the set of time series of length $N$ which has $L$-rank not larger than $r$. For a given time series $\tsX \in \sfX_N$, a window length $L$, $1 < L < N$, and $r$ satisfying condition \eqref{min_condition}, consider the problem:
\begin{equation} \label{L-rank_task}
f_q(\tsY) \to \min_{\tsY \in \sfX_N^r}, \quad f_q(\tsY) = \sum \limits_{i=1}^N q_i(x_i - y_i)^2,
\end{equation}
where $\tsY = (y_1, \ldots, y_N)$ and $q_1, \ldots, q_N$ are some non-negative weights,
$q_i \ge 0$, $i = 1, \ldots, N$. A one of reasonable target functions is the squared Euclidean distance in $\sfR^N$. It coincides with $f_q(\tsY)$ when $q_i = 1$, $i = 1, \ldots, N$.

\paragraph*{Adjustment.} For a numeric solution of optimization problem \eqref{L-rank_task} the following fact is important.
Consider a Hilbert space $\calX$ with inner product $\langle\cdot, \cdot\rangle$ and a subset $\calM \subset \calX$ closed with respect to scalar multiplication. Let an element $x$ belong to $\calX$, $y$ belong to $\calM$. Consider a projection $y^*$ of an element $x$ to a line $l = \{\alpha y:\, \alpha \in \sfR\}\subset \calM$. Then for the adjusted element $y^*$
\begin{equation}
\label{eq:adjust}
y^* = \calA(y)= y \frac{\langle x, y\rangle}{\langle y, y\rangle}
\end{equation}
the inequality $\|x - y^*\| \le \|x - y\|$ holds.

Thus, if an estimate $y=\tsY \in \sfX_N^r$ of the solution of problem \eqref{L-rank_task} for approximation of $x=\tsX \in \sfX_N$ is obtained, then it can be adjusted by applying \eqref{eq:adjust} to obtain the series $\tsY^*=\calA(\tsY)$, which is called \emph{adjustment of $\tsY$}.

\subsection{Equivalent target functions}

Let $\tsX \in \sfX_N$ be a time series of length $N$, $\bfX \in \calH$. Then there exists a one-to-one mapping $\calT$ between $\sfX_N$ and $\calH$, which can be written as
\begin{multline*}
\calT(\tsX) = \bfX, \text{where} \; \hat x_{l, k} = x_{l + k - 1}, \\ \bfX = (\hat x_{l,k}), \quad \tsX = (x_1, \ldots, x_N).
\end{multline*}

Due to the one-to-one mapping between the spaces of series and of Hankel matrices, problem~\eqref{L-rank_task} can be expressed in terms of matrices.

In the space $\sfX_N$ of time series, the target function \eqref{L-rank_task} is given explicitly using a (semi)inner product
\begin{equation}
\label{eq:norm_ser}
\langle\tsY,\tsZ\rangle_q = \sum_{i = 1}^N q_i y_i z_i,
\end{equation}
where $q_i$ are positive (non-negative) weights.

Consider two (semi)inner products in the space $\sfR^{L \times K}$ of matrices which are extension of the conventional Frobenius inner product.

Denote, as before,
\begin{equation}
\label{eq:norm1M}
\langle\bfY,\bfZ\rangle_{1,\bfM} = \langle\bfY,\bfZ\rangle_{\bfM} = \sum_{l = 1}^L \sum_{k=1}^K m_{l,k} y_{l,k} z_{l,k}.
\end{equation}
for a matrix $\bfM$ with positive (non-negative) elements and also
\begin{equation}
\label{eq:norm2S}
\langle\bfY,\bfZ\rangle_{2,\bfC} = \tr(\bfY \bfC \bfZ^\rmT)
\end{equation}
for a positive (semi)definite symmetric matrix $\bfC$.

Note that if the matrix $\bfM$ is of all ones, i.e. $m_{i,j}=1$,
and if $\bfC$ is the identity matrix, then both inner products coincide with the standard Frobenius inner product.

\begin{proposition}
	\label{prop:equiv_tasks}
	1. Let $\bfY = \calT(\tsY)$,  $\bfZ = \calT(\tsZ)$. Then $\langle\tsY,\tsZ\rangle_q= \langle \bfY,\bfZ \rangle_{1,\bfM}$ if and only if
	\begin{equation}\label{qi_mi}
	q_i = \sum_{\substack{1 \le l \le L \\ 1 \le k \le K \\ l+k-1=i}} m_{l,k}.
	\end{equation}
	
	2. The equality $\langle\bfY,\bfZ\rangle_{1,\bfM}= \langle\bfY,\bfZ\rangle_{2,\bfC}$ is valid if and only if
the matrix $\bfC=\diag(c_1,\ldots,c_K)$ and 
	\begin{equation}\label{sk_mlk}
	m_{l,k}=c_k.
	\end{equation}
\end{proposition}
\begin{proof}
	To prove the first statement, note that
	\begin{equation*}
	\langle \bfY, \bfZ \rangle_{1,\bfM} = \sum_{i = 1}^L \sum_{j = 1}^K m_{i,j} y_{i + j - 1} z_{i + j - 1}.
	\end{equation*}
The proof of the second statement is a consequence of the fact that only for a diagonal matrix $\bfC$ the corresponding inner product has a form appropriate to \eqref{eq:norm1M} (see also Remark~\ref{rem:diagC}):
	\begin{equation*}
	\langle \bfY, \bfZ \rangle_{2,\bfC} = \sum_{l=1}^L \sum_{k=1}^K c_k y_{l,k} z_{l, k}.
	\end{equation*}
\end{proof}

\begin{corollary}
	\label{cor:base_weights}
	If $m_{i,j}=1$, $i =1, \ldots, L$, $j = 1, \ldots, K$, then the equivalent series weights $q_i$, $i = 1, \ldots, N$, given by \eqref{qi_mi} are equal to $w_i$ given by \eqref{eq:w}.
\end{corollary}

Note that the matrix norm $\|\cdot\|_{2, \bfC}$ with a diagonal matrix $\bfC$ is a particular case of the norm $\|\cdot\|_{1, \bfM}$. 
However, this particular case is of special interest, since the corresponding approximation problem can be solved by means of the ordinary SVD, see Section~\ref{sec:obliqueSVD}.

\begin{remark}
	\label{rem:2tasks}
	If condition~\eqref{qi_mi} is carried out and all weights $q_i$ and $m_{i,j}$ are positive, then problem~\eqref{L-rank_task}
	is equivalent to the problem
	\begin{equation}
	\label{rank_task}
	f_\bfM(\bfY) \to \min_{\bfY \in \calM_r \cap \calH}, \quad f_\bfM(\bfY) = \|\bfX-\bfY\|_{1,\bfM}.
	\end{equation}
\end{remark}

\section{Algorithms}
\label{sec:alg}
In this section we suggest a range of algorithms for solving problem~\eqref{L-rank_task}.
In the model of series $\tsX=\tsS+\tsN$, where $\tsS$ is a time series of finite rank $r$ and $\tsN$ is a noise, results of the algorithms serve as  estimates of the signal $\tsS$.

\subsection{Cadzow iterations}
The aim of the Cadzow algorithm \cite{Cadzow1988} is the least-squares approximation \eqref{rank_task} of the trajectory matrix of a series with respect to the norm $\|\cdot\|_{1, \bfM}$ with the weights $m_{ij}=1$ (i.e. the algorithm solves problem \eqref{introd_task}, which, by Corollary \ref{cor:base_weights}, corresponds to problem~\eqref{introd_task_2} (or, the same, to problem ~\eqref{L-rank_task} with the weights $q_i=w_i$ given by \eqref{eq:w}). The drawback of this algorithm consists in the unequal series weights $w_i$: they are larger in the center than at both ends of the time series. Note that smaller window lengths leads to more uniform weights.

\begin{algorithm}[Cadzow iterations]
	\textbf{Input}: Time series $\tsX$, window length $L$, rank $r$,
	stop rule STOP1 (e.g., given by quantity of iterations).
	
	\textbf{Result}:
	Approximation $\widehat\tsS$ of time series $\tsX$ by finite-rank series of rank $r$.
	
	\begin{enumerate}
		\item
		$\bfY_0 = \calT \tsX$, $k=0$.
		\item
		$\bfY_{k+1} = \Pi_\calH  \Pi_{\calM_r} \bfY_{k}$, $k\leftarrow k+1$.
		\item
		If STOP1, then $\widehat\tsS = \calT^{-1} \bfY_k$; else go to 2.
	\end{enumerate}
\end{algorithm}


\subsection{Weighted Cadzow iterations}

Let $q_{i}=1$, $i = 1, \ldots, N$, be chosen in \eqref{L-rank_task}. According to Proposition ~\ref{prop:equiv_tasks},  problem \eqref{L-rank_task} is equivalent to problem \eqref{rank_task} with weights
\begin{equation}
\label{Mw}
m_{l, k} = \frac{1}{w_{l + k - 1}},
\end{equation}
where $w_i$ are introduced in \eqref{eq:w}.

\begin{algorithm}[Weighted Cadzow iterations]\label{alg:WCIt}
	\textbf{Input}: Time series $\tsX$, window length $L$, rank $r$,
	stop rules STOP1 for outer iterations and STOP2 for inner iterations.
	
	\textbf{Result}:
	Approximation $\widehat\tsS$ of time series $\tsX$ by finite-rank series of rank $r$.
	
	\begin{enumerate}
		\item
		$\bfY_0 = \calT \tsX$, $k=0$.
		\item
		Obtain $\widehat\bfZ$ using Algorithm ~\ref{alg:weightedSVD} applied to $\bfY_k$ for estimation of $\Pi_{\calM_r} \bfY_{k}$ with stop criterion STOP2.
		\item
		$\bfY_{k+1} = \Pi_\calH  \widehat\bfZ$, $k\leftarrow k+1$.
		\item
		If STOP1, then $\widehat\tsS = \calT^{-1} \bfY_k$; else go to 2.
	\end{enumerate}
\end{algorithm}

\subsection{Extended Cadzow iterations}
Let us introduce the Extended Cadzow algorithm, which presents a different approach to problem \eqref{L-rank_task} with equal weights than the Weighted Cadzow algorithm does.
Formally, let the series $\tsX$ be extended to both sides on $L-1$ measurements with some values having zero weights, i.e., the added measurements are considered as gaps.
Thus, the length of the extended series $\widetilde\tsX$ is $N+2L-2$, and size of its trajectory matrix $\widetilde\bfX$ is $L$ by $N+L-1$ (instead of $N-L+1$ for the non-extended trajectory matrix).

For the extended series, Algorithm~\ref{alg:weightedSVD} with weights $m_{i,j}=\calT \tsI$ is applied to $\widetilde\bfX$, where the series $\tsI$ has ones in the place of the series $\tsX$ and zeroes in positions of gaps, i.e.
\begin{equation*}
m_{i,j} = \begin{cases}
1 & 1 \le i+j-L \le N, \\
0 & \text{otherwise.}
\end{cases}
\end{equation*}

\begin{algorithm}[Extended Cadzow iterations]\label{alg:ECIt}
	\textbf{Input}: Time series $\tsX$, window length $L$, rank $r$,
	stop criteria STOP1 for outer iterations and STOP2 for inner iterations,
	left and right extension values, $\tsL_{L-1}$ и $\tsR_{L-1}$.
	
	\textbf{Result}:
	Approximation $\widehat\tsS$ of time series $\tsX$ by finite-rank series of rank $r$.
	
	\begin{enumerate}
		\item
		$\widetilde\bfY_0 = \calT \widetilde\tsX$, where $\widetilde\tsX=(\tsL_{L-1}, \tsX, \tsR_{L-1})$, $k=0$.
		\item
		Obtain $\widehat\bfZ$ using Algorithm ~\ref{alg:weightedSVD} applied to $\widetilde\bfY_k$ for estimation of $\Pi_{\calM_r} \widetilde\bfY_{k}$ with stop criterion STOP2.
		\item
		$\widetilde\bfY_{k+1} = \Pi_\calH  \widehat\bfZ$, $k\leftarrow k+1$.
		\item
		Construct $\bfY_k$ consisting of the columns of the matrix $\widetilde\bfY_{k}$, from $L$-th to $N$-th ones. If STOP1, then $\widehat\tsS = \calT^{-1} \bfY_k$; else go to 2.
	\end{enumerate}
\end{algorithm}

%\begin{remark}
%Несмотря на то, что при добавлении точек формально получается задача \eqref{L-rank_task} с частично нулевыми весами, сходимость $\bfY_k$
%по подпоследовательностям имеет место. Это следует из того, что при построении $\bfY_k$ добавленные точки с нулевыми весами
%не участвуют.
%\end{remark}

\subsection{Oblique Cadzow iterations}
\label{sec:ObliqueCadzow}
Algorithms considered in this section generalize the conventional Cadzow algorithm based on Euclidean inner product to the use of an oblique inner product given by a matrix $\bfC$.
These algorithms can be applied if the conditions of Proposition~\ref{prop:projS} hold.

\begin{algorithm}[Oblique Cadzow iterations]
	\label{alg:obliqueCadzow}
	\textbf{Input}: Time series $\tsX$, window length $L$, rank $r$, matrix $\bfC=\diag(c_1,\ldots, c_K)$, where $K=N-L+1$,
	stop criteria STOP1.
	
	\textbf{Result}:
	Approximation $\widehat\tsS$ of time series $\tsX$ by finite-rank series of rank $r$.
	
	\begin{enumerate}
		\item
		$\bfY_0 = \calT \tsX$, $k=0$.
		\item
		$\bfY_{k+1} = \Pi_\calH  \Pi_{\calM_r} \bfY_{k}$, $k\leftarrow k+1$, where
		$\Pi_{\calM_r}$ is given by \eqref{eq:PiMr}.
		\item
		If STOP1, then $\widehat\tsS = \calT^{-1} \bfY_k$; else go to 2.
	\end{enumerate}
\end{algorithm}

To solve the problem \eqref{L-rank_task} of approximation of time series with equal weights $q_i$, a proper matrix $\bfC$ should be chosen. It is found that there is no such full-rank matrix; therefore, a few variants providing approximately equal weights are considered below.

\subsubsection{Cadzow($\alpha$) iterations}
\label{sec:cadzow_alpha}
The following lemma describes a case, when the conditions of Proposition~\ref{prop:equiv_tasks} are fulfilled for equivalence of \eqref{L-rank_task} with equal weights $q_i$ and \eqref{rank_task}.

\begin{lemma}[\cite{Gillard2014}]
	\label{zhiglemma}
	Let $\tsX \in \sfX_N$, $\bfX = \calT(\tsX) \in \sfR^{L \times K}$. If $h = N/L$ is integer, then for $q_i\equiv 1$,  $\|\tsX\|^2_q = \|\bfX\|^2_{2, \bfC}$, where $\bfC=\diag(c_1,\ldots,c_K)$ with diagonal elements 
	\begin{equation*}
	c_k = \begin{cases}
	1, & \text{if} \; k = jL+1 \;\text{for some} \; j = 0, \ldots, h-1, \\
	0, & \text{otherwise}.
	\end{cases}
	\end{equation*}
\end{lemma}

This approach has an essential drawback. Since zeroes are placed at the diagonal of the diagonal matrix $\bfC$, $\bfC$ has rank $h$, which is considerably smaller than $K$. The change of the diagonal zeroes to some small $\alpha$ is suggested in \cite{Gillard2014} to improve rank-deficiency.

Let
\begin{multline}\label{zhigweights}
c_k = c_k(\alpha) =\\ \begin{cases}
1, & \text{if} \; k = jL+1 \; \text{for some} \; j = 0, \ldots, h-1, \\
\alpha, & \text{otherwise.}
\end{cases}
\end{multline}
Then the matrix $\bfC(\alpha)=\diag(c_1(\alpha),\ldots,c_K(\alpha))$ with the diagonal given by \eqref{zhigweights} is of full rank.
However, the corresponding series weights are not equal.

Let us denote Cadzow($\alpha$) the iterations performed by Algorithm~\ref{alg:obliqueCadzow} with the diagonal matrix $\bfC=\bfC(\alpha)$.
	Note that for $\alpha=1$ the matrix $\bfC(\alpha)$ is the identity matrix and the Cadzow($\alpha$) iterations coincide with the conventional Cadzow iterations.

\paragraph*{Degenerate case $\alpha=0$.}

Equality \eqref{sk_mlk} provides the form of a matrix $\bfM$ to obtain $\|\cdot\|_{1, \bfM} = \|\cdot\|_{2, \bfC}$ in the case $\alpha = 0$:
\begin{equation}
\label{eq:degenerateM}
\bfM = \begin{pmatrix}
1 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & \cdots & 1 \\
1 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & \cdots & 1 \\
\vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots & \cdots & \cdots & 1 \\
1 & 0 & 0 & \cdots & 0 & 1 & 0 & \cdots & \cdots & 1
\end{pmatrix}.
\end{equation}
%So, distance is taken using only $h$ columns instead of $K$, while multiplicating by $\bfO_\bfC^{\rmT}$ sets to zero $K - h$ columns of $\bfC$.

\begin{remark}
	The optimization problem \eqref{rank_task} with the matrix $\bfM$ given by \eqref{eq:degenerateM} corresponds to the search of an arbitrary (not necessary Hankel) matrix of rank not larger than $r$, which is closest in the Frobenius norm to the matrix
	\be
	\label{eq:traj_noinersect}
	\begin{pmatrix}
		x_1&x_{L+1}&\cdots&x_{K}\\
		\vdots&\vdots&\cdots&\vdots\\
		x_L&x_{2L}&\cdots&x_N
	\end{pmatrix}.
	\ee
	This problem quite differs from problem \eqref{L-rank_task} of approximation by finite-rank series. Therefore, the Cadzow($0$) algorithm does not solve the problem \eqref{L-rank_task}.
\end{remark}


\subsubsection{Cadzow-$\widehat{\bfC}$ iterations}
\label{sec:cadzow_hat}
Let us correct the rank-deficiency of $\bfC(0)$ by another way.

To obtain equal series weights $q_i\equiv 1$ in \eqref{L-rank_task}, we
should choose the weight matrix $\bfM$ in \eqref{rank_task} with weights $m_{i,j}$ defined by \eqref{Mw}.
Generally, there is no matrices $\bfC$ providing the equivalent norm
$\|\cdot\|_{2,\bfC}=\|\cdot\|_{1,\bfM}$, since the matrix $\bfC$ should
be diagonal and therefore the matrix $\bfM$ should have columns consisting of equal elements (see Proposition~\ref{prop:equiv_tasks}).

To obtain approximately equal weights, the following approach is suggested.
Consider the set $\sfZ \subset \sfR^{L \times K}$ of matrices with columns consisting of equal elements and find $\what{\bfM}$ such that
\begin{equation*}
\|\bfM - \widehat\bfM\| \to \min_{\widehat\bfM \in \sfZ},
\end{equation*}
where $\|\cdot\|$ is the Frobenius norm.

The solution $\widehat\bfM$ is evidently constructed as the averaging of the matrix $\bfM$ by columns. As a result, the resultant matrix $\widehat\bfC$ such that $\|\cdot\|_{2,\widehat\bfC}=\|\cdot\|_{1,\widehat\bfM}$ has the form $\widehat\bfC = \text{diag}(\hat c_1, \ldots, \hat c_K)$, where
\begin{equation}\label{my_s}
\hat c_k = \frac{1}{L}\sum_{l=1}^L m_{l, k}.
\end{equation}

%where $m_{l, k}$ are defined in \eqref{Mw}.


We call Algorithm~\ref{alg:obliqueCadzow} with the matrix $\bfC=\widehat\bfC$ Cadzow-$\widehat\bfC$ iterations.

\subsubsection{Weights $q_i$ in \eqref{L-rank_task} produced by the algorithms}
Since the norm $\|\cdot\|_{2, \bfC}$ with $\bfC(\alpha)$ or $\widehat\bfC$ in place of $\bfC$ does not correspond to equal series weights,
let us find $q_i(\alpha)$ and $\hat{q}_i$ from the equalities  
$\|\bfY\|_{2,\widehat\bfC}=\|\tsY\|_{\hat{q}}$ and $\|\bfY\|_{2,\bfC(\alpha)}=\|\tsY\|_{q(\alpha)}$.
Formulas for calculation are provided by Proposition~\ref{prop:equiv_tasks}.

The following statements are valid.

\begin{proposition}\label{prop:zhigconseq}
	Let $h = N/L$ be integer, $\bfC(\alpha) = \diag(c_1(\alpha), \ldots, c_K(\alpha))$, where $c_i(\alpha)$ are given by \eqref{zhigweights}, $0 \le \alpha \le 1$. Then the weights $q_i(\alpha)$ have the form
	\begin{equation*}
	q_i (\alpha) = \begin{cases}
	1 + (i - 1) \alpha & \text{для $i = 1, \ldots, L-1,$}\\
	1 + (L - 1) \alpha & \text{для $i = L, \ldots, K-1,$}\\
	1 + (N - i) \alpha & \text{для $i = K, \ldots, N.$}
	\end{cases}
	\end{equation*}
\end{proposition}
\begin{proof}
	The proof is a straightforward consequence of Proposition~\ref{prop:equiv_tasks}. 
\end{proof}

To illustrate the form of the weights $\hat{q_i}$, let us formulate propositions with simplifying conditions.
\begin{proposition} \label{myweightstat}
	Let $N \ge 3(L-1)$. Then the diagonal matrix weights $\hat c_k$ defined in \eqref{my_s} are equal to
	\begin{equation*}
	\hat c_k = \begin{cases}
	\frac{1}{L}\left(\frac{k}{L} + \sum_{j=k}^{L-1} \frac{1}{j} \right),& 1 \le k \le L-1, \\
	1/L, & L \le k \le K - L + 1,\\
	\hat c_{K - k + 1}, & K - L + 2 \le k \le K.
 	          \end{cases}
	\end{equation*}
\end{proposition}

\begin{proof}
	To prove the proposition, it is sufficient to substitute $m_{l,k}$ defined in \eqref{Mw} to \eqref{my_s}.
\end{proof}

\begin{proposition} \label{myserweightstat}
	Let $N \ge 4(L-1)$. Define
	\begin{equation*}
	\hat{u}_i = \begin{cases}
	\frac{i(i+1)}{2 L^2} + \frac{i}{L}(1 + H_{L-1} - H_i), &1 \le i \le L-1, \\
	1 + \frac{2iL-i-i^2}{2L^2} + \frac{L-i}{L}(H_{L-1} - H_{i - L}), & L \le i \le 2L-1,
	\end{cases}
	\end{equation*}
	where $H_0 = 0$, and $H_i = \sum_{j=1}^i 1/j$ is the $i$-th harmonic number.
	 Then the weights $\hat{q}_i$ have the form:
	\begin{equation*}
	\hat{q}_i = \begin{cases}
	\hat{u}_i, &1 \le i \le 2L-1, \\
		1, &2L \le i \le N-2L+1,\\
\hat{u}_{N-i+1}, &N-2L+2 \le i \le N. \\
	\end{cases}
	\end{equation*}
\end{proposition}

\begin{proof}
	For $1 \le i \le L-1$, we have
	\begin{multline*}
	\hat{q}_i = \sum_{j=1}^i \hat{c}_j = \sum_{j=1}^i \frac{1}{L}\left(\frac{j}{L} + \sum_{k=j}^{L-1}\frac{1}{k}\right)\! =
	\frac{i(i+1)}{2L^2} + \\ + \frac{1}{L} \sum_{k = 1}^{L-1} \sum_{j=1}^{\min(k,i)} \frac{1}{k} = \frac{i(i+1)}{2L^2}+\frac{1}{L} \sum_{k = 1}^{L-1} \frac{\min(k,i)}{k} = \\ = \frac{i(i+1)}{2 L^2} + \frac{i}{L}(1 + H_{L-1} - H_i).
	\end{multline*}
	For $L \le i \le 2L-1$, changing the order of summation, we obtain
	\begin{multline*}
	\hat{q}_i = \sum_{j = 1}^L \hat{c}_{i-L+j} = \sum_{j = i - L + 1}^{L - 1} \hat{c}_j + \frac{i - L + 1}{L} =\\
	=\frac{i - L + 1}{L} + \frac{1}{L^2} \sum_{j = i - L + 1}^{L-1}j + \frac{1}{L} \sum_{j = i-L + 1}^{L-1} \sum_{k=j}^{L-1}\frac{1}{k} =\\
	=\frac{i - L + 1}{L} + \frac{2iL - i - i^2}{2L^2} + \frac{1}{L} \sum_{k = i - L + 1}^{L - 1} \sum_{j = i - L + 1}^k \frac{1}{k} =\\
	=1 + \frac{2iL-i-i^2}{2L^2} + \frac{L-i}{L}(H_{L-1} - H_{i - L}).
	\end{multline*}
	The weights $\hat{q}_i$ for $N-2L+2 \le i \le N$ are calculated by symmetry. The center series weights are evidently equal to 1.
\end{proof}

Let us normalize series weights so that their sums equal 1. 
Normalized weights $q_i(\alpha)$, for $\alpha = 1$ (the conventional Cadzow iterations), $\alpha = 0$ (equal $q_i$), $\alpha = 0.1$,
and $\hat{q}_i$ for $N = 40$, $L = 8$, are shown in Figure~\ref{img_weights}.
\begin{figure}
		\includegraphics[width =\columnwidth]{weights.pdf}\caption{Series weights $q_i$ corresponding to $\bfC(\alpha)$ and $\widehat\bfC$}\label{img_weights}
\end{figure}
	
	\subsection{Comments to algorithms. Comparison}
\label{sec:comments}
	
	Let us comment and compare the following methods: the Weighted Cadzow iterations (Algorithm \ref{alg:WCIt}), the Extended Cadzow iterations (Algorithm \ref{alg:ECIt}), the Cadzow($\alpha$) iterations, $0< \alpha \leq 1$, coinciding with the conventional Cadzow iterations if $\alpha=1$,
	and finally the Cadzow-$\widehat\bfC$ iterations (Algorithm \ref{alg:obliqueCadzow}).
	Note that the window length $L$ is a parameter for each of the considered methods.
	
	\begin{itemize}
		\item \textit{Theoretical convergence.}
		Theorem \ref{th:converg} provides conditions for existence of subsequences convergent to a matrix from $\calM_r \cap \calH$. This theorem is applicable directly to Algorithm \ref{alg:obliqueCadzow} if all weights are positive and to Algorithm \ref{alg:WCIt} if to suppose that the weighted projection to $\calM_r$ can be calculated with no error. It is easy to extend Theorem \ref{th:converg} to be applicable to Algorithm \ref{alg:ECIt} where the weights for added values are zero, if to consider the sequence  $\bfY_k$ instead of $\widetilde\bfY_k$. 
%consisting of columns of matrices $\Pi_{\calM_r} \bfY_k$ from $L$-th to $N$-th and having positive weight of each element are matrices of rank not larger than $r$, and their Frobenius norms are not larger than seminorm of original matrix: $\|\widetilde \bfY_k\|_F \le \|\Pi_{\calM_r} \bfY_k\|_\bfM$. Hence it is bounded sequence which contains a subsequence converging to needed set.
		\item \textit{Convergence in practice.} Although the theory says about existence of converging subsequences, the convergence of the constructed sequences took place in all the training examples.
		\item \textit{Comparison by accuracy.}
		The methods are iterative, and convergence to the global minimum in the corresponding least-squares problem does not necessarily takes place. Therefore, different algorithms corresponding to the same weights can yield different approximations. Thereby, comparison of the algorithms by the approximation accuracy has sense.		
%\item \textit{Comparison by computational cost.}
%Note that we can compare numbers of iterations providing a given accuracy to compare the methods without inner iterations by computational complexity, since computational costs of one iteration are approximately the same.
\item \textit{Signal estimation and series approximation.}
		The proposed methods can be considered as both approximations of the initial series by finite-rank series and weighted least-squares methods for signal estimation. Note that generally the approximation quality can contradict to the estimation accuracy due to possible over-fitting.
		\item \textit{Algorithms and series weights.}
		The Weighted Cadzow and Extended Cadzow methods try to solve problem \eqref{L-rank_task} with equal weights $q_i$. The other methods work with weights with different levels of non-uniformity.
		\item \textit{Algorithms and computational costs.}
		All suggested algorithms are iterative. However, each outer iteration in the Weighted Cadzow and Extended Cadzow algorithms has a step with inner iterations. Therefore, these algorithms are very time-consuming. The other algorithms do not contain inner iterations; moreover, they have similar computational costs of one iteration and can be compared by the number of iterations.
		Computational complexity is described by both complexity of one iteration and the number of iterations. Evidently, the necessary number of iterations is determined by the convergence rate.
\item \textit{Fast implementation.}
There is a very fast implementation of iterations of the Cadzow algorithm suggested in \cite{Korobeynikov2010} and extended in \cite{Golyandina.etal2015}. However, it can be shown that the same implementation approach can be applied to the Cadzow($\alpha$) and Cadzow-$\widehat{\bfC}$ algorithms. Therefore, fast implementations of these algorithms still can be compared by the number of iterations.
		\item \textit{Use of the first iteration for signal estimation.}
		One iteration of the Cadzow iterations is exactly the well-known Singular Spectrum Analysis (SSA) method, which can solve significantly wider range of tasks than the iterative method does. By analogy, together with the limiting series, we are interested in the signal estimations by means of the first iteration of the considered algorithms. In a sense, each iterative method produces a modification of SSA. The first iteration is generally not of finite rank; however, it has low computational complexity and can provide sufficient accuracy.
		\item \textit{Separability, the first iteration and convergence rate.}
		Separability of a signal, which is an important concept of the SSA method, means the ability of a method to (approximately) separate the signal from a residual. From the viewpoint of the iterative methods,  separability quality is closely related to accuracy of the first iteration of the method. On the other hand, we can expect that the accuracy of the first iteration is connected with the method's convergence rate. Therefore, the separability accuracy is connected with the convergence rate of iterative methods.
		\item \textit{Separability and choice of parameters.}
		The connection between the separability and the window length $L$ is well studied for the SSA method, see \cite{Golyandina2010}. In particular, optimal window lengths are close to half of the series length. A small window length $L$ provides poor separability. We can expect that this is valid for the other algorithms too.
The Cadzow($\alpha$) method has an additional parameter $\alpha$.
Influence of parameter $\alpha$ on separability in the class of Cadzow($\alpha$) iterations is investigated in Appendix~\ref{sec:app}.
 The studied example of separability of a sine-wave signal from a constant residual shows that small values of $\alpha$ provide poor separability.
        \item \textit{Equal series weights and choice of parameters.}
        Let us consider the dependence of series weights produced by the Cadzow($\alpha$) algorithm on the window length $L$ or $\alpha$. Proposition~\ref{prop:zhigconseq} shows that more uniform weights are achieved for small $L$ and small $\alpha$. This is exactly
        the cases corresponding to poor separability. 
 		\item \textit{Equal series weights and accuracy of signal estimation.}
 Thus, the weights, which are close to equal ones, correspond to algorithms, which either have a time-consuming iteration step with inner iterations or are slowly convergent; therefore such algorithms have high computational complexity.
There are no theoretical results about the behavior of the estimation accuracy in dependence on algorithms and their parameters. However, the numerical study shows that the best accuracy is achieved in the algorithms corresponding to the weights, which are equal or almost equal.	 \end{itemize}
	
	\begin{remark}
		\label{rem:adjust}
		The adjustment defined in \eqref{eq:adjust} can be applied to the resultant signal estimation $\widehat\tsS$ for any considered algorithm. In \eqref{eq:adjust}, $\|\cdot\|$ is the standard Euclidean norm not depending on the weight matrix $\bfM$ used in the algorithms, since this norm $\|\cdot\|$ corresponds to problem \eqref{L-rank_task} with equal weights $q_i$. We will call the algorithms with the adjustment \eqref{eq:adjust} adjusted algorithms. For example, the result of the $k$-th iteration of the Cadzow iterations method can be expressed as $\widehat\tsS_k = \calT^{-1}(\Pi_\calH \Pi_{\calM_r})^k \calT \tsX$. Then the result of the $k$-th iteration of the adjusted Cadzow iterative method is $\widehat\tsS_k^*=\calA(\widehat\tsS_k)$.
	\end{remark}

\section{Numerical examples}
\label{sec:simul}
Let us carry out numerical experiments for analysis of performance of the considered methods. Comparison of the methods was performed on two examples, with a sine-wave signal and an exponentially-modulated sine-wave signal.
Since the obtained comparison results are very similar, only the results for a sine-wave signal are presented.

Let the signal $\tsS = (s_{1}, \ldots, s_N)$ of length $N = 40$ and rank $r=2$ have the form:
\begin{equation*}
s_{k} = 5\sin{\frac{2 \pi k}{6}}, \quad k = 1, \ldots, N,
\end{equation*}
and the series $\tsX = \tsS + \tsN$ be observed, where  $\tsN$ is Gaussian white noise with mean equal to $0$ and variance equal to $1$. Accuracy of a signal estimate $\widehat\tsS$ is measured as root mean-square error (RMSE) using 1000 simulations. Comparison is performed on the same simulated samples. It was checked that the stated comparison results are significant at 5\% level of significance.

We start with the investigation of the Cadzow-$\widehat\bfC$ method and a set of the Cadzow($\alpha$) methods. These methods use an oblique SVD; the Cadzow($1$) method is the conventional Cadzow method. Figure~\ref{img_cadzowspeed2} shows the rate of convergence for $\alpha = 0.1$ and $\alpha = 1$ and for two different window lengths $L$. The RMSE values are depicted versus the number of performed iterations.

\begin{figure}[!hhh]
%	\begin{center}
		\includegraphics[width = \columnwidth]{cadzowspeed_2.pdf}
		\caption{RMSE of signal estimate depending on number of iterations.}
		\label{img_cadzowspeed2}
%	\end{center}
\end{figure}

One can see that a method with a smaller limit error is the one with a slower convergence rate. For parameters involved to the simulations, the Cadzow($0.1$) method with the window length $L=8$ has the smallest limit error. At the same time, these values of parameters correspond to both the slowest convergence and the most uniform weights.

Note that the limit errors do not differ strongly, they change from 0.31 ($\alpha=0.1$, $L=8$) in the best case to 0.35 ($\alpha=1$, $L=20$) in the worst case. However, error equal to 0.35 is achieved at the first iteration in the worst case, while it takes 4--5 iterations to achieve error 0.35 in the best case.

\smallskip
Let us take the Extended and Weighted Cadzow iterations into consideration and examine the spreading of the estimation errors along the series. The maximal number of iterations equal to 100 is taken for the stop criterion STOP1 (this choice yields the error close to the limiting value); the stop criterion STOP2 for inner iterations is as follows:
$\frac{\|\bfY_k - \bfY_{k+1}\|^2}{LK} < 10^{-4}$. The initial left and right extended values $\tsL_{L-1}$ и $\tsR_{L-1}$ in the Extended Cadzow iterations are obtained using the vector SSA-forecasting method \cite[chapter 2.3.1]{Golyandina.etal2001}.

Figures~\ref{fig:s1_it1}~and~\ref{fig:s1_it100} show the dependence of RMSE on numbers of the series points. Figure~\ref{fig:s1_it1} shows errors at the first iteration, Figure~\ref{fig:s1_it100} shows errors at the 100-th iteration. It is clearly seen that the Extended Cadzow method is the most precise in both cases. The Cadzow($1$) and Cadzow-$\widehat\bfC$ methods are the best at the first iteration among the set of methods without inner iterations. The best method in the limit (after 100-th iteration, errors do not change significantly further) is the Cadzow($0.1$) one; this is not surprising according to Figure~\ref{img_cadzowspeed2}.

\begin{figure}[!hhh]
%	\begin{center}
		\includegraphics[width = \columnwidth]{s1_it1.pdf}
		\caption{RMSE of signal estimates at each series point; iteration 1; $L=20$.}
		\label{fig:s1_it1}
%	\end{center}
\end{figure}

\begin{figure}[!hhh]
%	\begin{center}
		\includegraphics[width = \columnwidth]{s1_it100.pdf}
		\caption{RMSE of signal estimate at each series point; iteration 100; $L=20$.}
		\label{fig:s1_it100}
%	\end{center}
\end{figure}

\smallskip
Since we considered the least-squares method for estimation of the signal $\tsS$, consider Table~\ref{fintable} which shows RMSE for $\tilde \tsS$ as an estimate of $\tsS$ (i.e., signal estimation error) and RMSE for $\tilde \tsS$ as an estimate of the initial series $\tsX$ (i.e., series approximation error). Here $k$ is the number of iterations, $L=20$. Table~\ref{fintable} confirms the conclusions about comparison of the methods by accuracy of signal estimation. Also it is seen that the quality of initial series approximation does not always correspond with the quality of signal estimation. For example, overfitting is clearly present for the Cadzow($0.1$) iterations at first iteration. However, methods are ordered identically by errors of series approximation and signal estimation in the limit.

The same simulations were performed with the adjusted algorithms (see Remark~\ref{rem:adjust}). One can see in Table~\ref{fintable_improved} that the accuracy is almost the same. By its definition, the adjustment always improves approximation of the initial series; however, the influence on accuracy of signal approximation is ambiguous (the adjustment improves accuracy at the 100-th iteration; results are various at the first iteration).

\begin{table*}
		\caption{Comparison of methods by RMSE, $L = 20$}\label{fintable}

		\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ccccc}
			\hline
			$P$: & $\tsS$, $k = 1$ & $\tsX$, $k = 1$ & $\tsS$, $k = 100$ & $\tsX$, $k = 100$  \\
			\hline
			Cadzow, $\alpha = 1$ & 0.3758 & 0.9195 & 0.3782 & 0.9664 \\
			\hline
			Cadzow, $\alpha = 0.1$ & 0.4329 & 0.7040 & 0.3311 & 0.9506 \\
			\hline
			Cadzow $\hat{\bfC}$ & 0.3655 & 0.8925 & 0.3559 & 0.9583 \\
			\hline
			Weighted Cadzow & 0.3644 & 0.8891 & 0.3455 & 0.9549 \\
			\hline
			Extended Cadzow & 0.3361 & 0.9030 & 0.3189 & 0.9471 \\
			\hline
		\end{tabular*}
\end{table*}

\begin{table*}
	\begin{center}
		\caption{Comparison of adjusted methods by RMSE, $L = 20$}\label{fintable_improved}
		\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ccccc}
			\hline
			$P$: & $\tsS$, $k = 1$ & $\tsX$, $k = 1$ & $\tsS$, $k = 100$ & $\tsX$, $k = 100$  \\
			\hline
			Cadzow, $\alpha = 1$ & 0.3714 & 0.9175 & 0.3667 & 0.9622 \\
			\hline
			Cadzow, $\alpha = 0.1$ & 0.4385 & 0.7023 & 0.3276 & 0.9493 \\
			\hline
			Cadzow $\hat{\bfC}$ & 0.3626 & 0.8909 & 0.3478 & 0.9555 \\
			\hline
			Weighted Cadzow & 0.3640 & 0.8883 & 0.3380 & 0.9523 \\
			\hline
			Extended Cadzow & 0.3370 & 0.9030 & 0.3184 & 0.9469 \\
			\hline
		\end{tabular*}
	\end{center}
\end{table*}

Thus, the numerical examples mostly confirm the hypotheses and conclusions itemized in Section~\ref{sec:comments}.

\section{Conclusion}
\label{sec:concl}
%In this paper, we suggested and investigated the methods of signal extraction based on subspace-based methods of approximation.
%
% We used equivalent statements of the problems for weighted matrix approximation and  weighted time-series approximations, where equal weights in the least-squares matrix problem corresponds
%to unequal weights in the least-squares time series problem, and vice verse.
%

Several known and new iterative algorithms for approximation of a noisy series by a finite-rank series (a signal) were considered in the paper. The approximation was performed by a least-squares method and its result was considered as an estimate of the signal.

 We used equivalent statements of the problems for weighted matrix approximation and  weighted time-series approximation, where equal weights in the least-squares matrix problem corresponds
 to unequal weights in the least-squares time series problem, and vice verse.

A wide range of iterative algorithms was reviewed with the aim to obtain equal weights in the least-squares method applied to time series.
Equal weights were achieved by using inner iterations, which converge to a local minimum only and also make the algorithms very time-consuming. It appears that the use of methods without inner iterations leads to approximately equal weights only.

Convergence of outer iterations by subsequences was proved for the reviewed class of algorithms.

Comparison of the considered methods by accuracy and convergence rate was performed by simulation on the example of a noisy sine-wave signal. The simulation results confirmed the theoretical results. It appears that time-series weights, which are more close to equal ones, provides in the limit more time-consuming and simultaneously more accurate methods.
Also, the simulations confirm that the convergence rate is in accordance with the separability rate. Therefore, for the methods without inner iterations,
there is the correspondence between slow convergence, poor separability,
inaccurate approximation at the first one iteration and high accuracy in the limit; and vice verse.
In particular, for Cadzow iterations, which produce Singular Spectrum Analysis for signal reconstruction at the first iteration, the window length equal to the half of the series length gives poor accuracy in the limit and
one of the best reconstructions at the first iteration.


\appendix

\section{Separability of sine-wave signal from constant residual for Cadzow($\alpha$) iterations}
\label{sec:app}

Let us consider modifications of SSA, which are produced by the first iteration of the Cadzow($\alpha$) iterative algorithms described in Section~\ref{sec:cadzow_alpha}. Recall that the Cadzow($1$) iterative algorithm produces the conventional Basic SSA method \cite{Golyandina.etal2001,Golyandina.Zhigljavsky2012}, while the first iteration of a general Cadzow($\alpha$) algorithm can be considered as a
particular case of Oblique SSA \cite{Golyandina2013} with the Euclidean inner product in the row column and a special inner product in the row space.

Separability of signals from residuals in SSA is deeply investigated in \cite{Golyandina.etal2001,Golyandina2010}. Separability of a signal means the ability of the method to extract the signal. In fact, separability is related to the accuracy of signal estimation obtained at the first iteration
of the considered iterative algorithms. Notions of exact, approximate and asymptotical (as the series length tends to infinity) separability together with examples of the asymptotic separability rates are introduced for
SSA in \cite{Golyandina.etal2001} and can be generalized for the oblique case. Following by \cite{Golyandina.etal2001}, we will measure the separability by means of the cosines between $L$- and $K$-lagged vectors
of the signal and the residual.
%Since we consider a specific Oblique SSA with Euclidean inner product
%in the column space, we still can measure the separability through orthogonality of series subspaces as it was suggested for Basic SSA in \cite{Golyandina.etal2001}.


Let $\bfC \in \sfR^{K \times K}$ be a symmetric positive semidefinite matrix, $\tsX_1$ и $\tsX_2$ be two different time series of length $N$, $\bfX^1$, $\bfX^2$ be their trajectory matrices. Define \emph{correlation coefficient of $i$-th and $j$-th column} by:
\begin{equation}\label{col_corr}
\rho^c_{i,j} = \frac{(X^1_i, X^2_j)}{\|X^1_i\| \|X^2_j\|},
\end{equation}
where $X^k_i$ is the $i$-th column of the matrix $\bfX^k$, $k = 1, 2$, $(\cdot, \cdot)$ is the Euclidean inner product, $\|\cdot\|$ is the Euclidean norm. Define \emph{correlation coefficient of $i$-th and $j$-th row} by:
\begin{equation}\label{row_corr}
\rho^r_{i,j} = \frac{(X^{1,i}, X^{2,j})_\bfC}{\|X^{1,i}\|_\bfC \|X^{2,j}\|_\bfC},
\end{equation}
where $X^{k,i}$ is the $i$-th row of matrix $\bfX^k$, $k = 1, 2$, and $(\cdot, \cdot)_\bfC$ is the oblique inner product in $\sfR^K$ generated by a matrix $\bfC$ as follows: $(X, Y)_\bfC = X \bfC Y^\sfT$ (here $X$ and $Y$ are row vectors), $\| \cdot \|_\bfC$ is the norm with respect to this inner product. We say that the series $\tsX_1$ and $\tsX_2$ are \emph{weakly $\varepsilon$-separable} if
\begin{equation}\label{weak_sep_eq}
\rho = \max\Big(\max_{1 \le i,j \le K}|\rho^c_{i,j}|, \max_{1 \le i,j \le L}|\rho^r_{i,j}|\Big) < \varepsilon.
\end{equation}
We are interested in the order of $\varepsilon$ as $N\ra \infty$ for different matrices $\bfC$, where the series $\tsX_k$, $k=1,2$, consist of the first $N$ terms of infinite series $\tsX_k^\infty$.

Here we apply the theory to an example with sine-wave signal and constant residual. By analogy with SSA, we can expect that the asymptotic separability rate will be the same if the residual is Gaussian white noise.
Thus, let
$\tsX_1^\infty = (\cos(2 \pi \omega k), k = 1, 2, \ldots)$ and  $\tsX_2^\infty = (c, c, \ldots)$. Instead of $N \to \infty$ it is convenient to consider $L,K \to \infty$ and take $N = L + K - 1$. When $\bfC$ is the identity matrix, the answer is known: $\varepsilon$ has order $1/\min(L,K)$, i.e. the rate of separability has order $1/N$ for $L$ proportional to $N$.
This result can be found in \cite[Chapter 6.1]{Golyandina.etal2001}. 

\smallskip
Let us consider the separability rate for the Cadzow($\alpha$) iterations introduced in Section~\ref{sec:cadzow_alpha}.

\begin{proposition}
	\label{prop:separ1} 
	Let $\tsX_1^\infty = (\cos(2 \pi \omega k), k = 1, 2, \ldots)$, where $0<\omega <0.5$, be a sine wave, $\tsX_2^\infty = (c, c, \ldots)$ be a constant series,  $L,K\ra \infty$ such that $h = h_L = N/L$, where $N=L+K-1$, is integer, and $\bfC=\bfC(\alpha)$ be defined in \eqref{zhigweights}, i.e.  $\bfC$ is a diagonal matrix with diagonal elements:
	\begin{equation*}
	c_k = \begin{cases}
	1, & \text{если} \quad k = jL+1 \quad \text{for some} \ j = 0, \ldots, h-1,\\
	\alpha, & \text{otherwise},
	\end{cases}
	\end{equation*}
	where $0 \le \alpha \le 1$. Then $\rho$ given by \eqref{weak_sep_eq} has order $\max\left(\frac{1}{L}, \frac{(1-\alpha)C_{L,K}+\alpha}{(1-\alpha)N/L+\alpha K}\right)$, where the order of $C_{L,K}$
	can vary from $O(1)$ to $O(N/L)$ depending on how $K$ tends to infinity.
\end{proposition}

\begin{proof}
To prove the theorem, we should evaluate the order of the expressions:
	\begin{equation}\label{th5_sep1}
	\rho^c_{i,j} = \frac{\sum_{k=j}^{j + L - 1} \cos(2 \pi \omega k)}{\sqrt{L \left(\sum_{k=j}^{j + L - 1} \cos^2(2 \pi \omega k)\right)}},
	\end{equation}
	\begin{equation}\label{th5_sep2}
	\rho^r_{i,j} = \frac{\sum_{k=1}^K c_k\cos(2 \pi \omega (j + k - 1))}{\sqrt{\left(\sum_{k=1}^K c_k\right) \left(\sum_{k=1}^K c_k\cos^2(2 \pi \omega (j + k - 1))\right)}}.
	\end{equation}
	The following trigonometric equalities hold:
	\begin{multline*}
	\sum_{k=1}^n \cos(ak + b) = \csc(a/2) \sin(an / 2) \cos \left(\frac{an + a + 2b}{2} \right), \\
	\sum_{k=1}^n \cos^2(ak + b) = \frac{1}{4}(\csc(a) \sin(2an + a + 2b) -\\ - \csc(a)\sin(a + 2b) + 2n),
	\end{multline*}
	for any real $a, b$ and positive integer $n$.
	Therefore, since the series $\tsX_1$ is not constant, the numerator in \eqref{th5_sep1} has order $O(1)$, while the denominator has order $O(L)$. Thus, we obtain the order $1/L$.
	
	To evaluate the order of \eqref{th5_sep2}, consider the sum over $k$ such that $c_k=1$ separately:
	\begin{multline*}
	\sum_{k=1}^K c_k\cos(2 \pi \omega (j + k - 1)) = \\ (1-\alpha) \sum_{\substack{1 \le k \le K: \\ c_k = 1}}\cos(2 \pi \omega (j + k - 1)) +\\ +\sum_{1 \le k \le K}\alpha \cos(2 \pi \omega (j + k - 1)) = (1-\alpha) C_{L,K} + \alpha\, O(1),
	\end{multline*}
where
    \begin{gather*}
	C_{L, K} = \sum_{\substack{1 \le k \le K: \\ c_k = 1}}\cos(2 \pi \omega (j + k - 1)),
	\\
	\sum_{k=1}^K c_k = (1-\alpha) N/L + \alpha K,
	\end{gather*}
and
    \begin{multline*}
	\sum_{k=1}^K c_k\cos^2(2 \pi \omega (j + k - 1)) = \\ (1-\alpha)\sum_{\substack{1 \le k \le K: \\ c_k = 1}}\cos^2(2 \pi \omega (j + k - 1)) +\\ +\sum_{1 \le k \le K }\alpha \cos^2(2 \pi \omega (j + k - 1)) = (1-\alpha) O(N/L) + \alpha\, O(K).
    \end{multline*}
	In the worst case when $L\omega$ is integer we obtain that $\cos(2 \pi \omega (j + k - 1))$ is a constant not depending on $j$ and $k$, and then $C_{L,K}$ has order $O(N/L)$.
\end{proof}

Thus, even in the best case when $C_{L,K}$ has order $O(1)$, separability of sine-wave and constant signals becomes worse than in the standard Cadzow($1$) iterations: for $\alpha$ close to 0, the optimal choice for $L$ is $L \approx \sqrt{N}$ and therefore the rate of separability has order $1/\sqrt{N}$.
In the worst case, the separability vanishes as $\alpha$ tends to zero.

%\smallskip
%Now let us consider Cadzow-$\widehat\bfC$ iterations introduced in Section~\ref{sec:cadzow_hat}.
%
%\begin{proposition}
%	\label{prop:separ2}
%	Let $\tsX_1 = (\cos(2 \pi \omega k), k = 1, 2, \ldots)$, where $0<\omega <0.5$, be a sine wave, $\tsX_2 = (c, c, \ldots)$ be a constant series,   $L,K\ra \infty$, and $\bfC=\widehat\bfC$ be defined in Cadzow-$\widehat\bfC$ iterations.
%	Then $\rho$ given by \eqref{weak_sep_eq} has order $\max \left(1/L, \frac{H_L}{\sqrt{NK}} \right)$ as $L, K \to \infty$, where $H_L$ is $L$-th harmonic number.
%\end{proposition}
%
%\begin{proof}
%	To prove the theorem, we should evaluate the order of the expressions:
%	\begin{equation}\label{th6_sep1}
%	\rho^c_{i,j} = \frac{\sum_{k=j}^{j + L - 1} \cos(2 \pi \omega k)}{\sqrt{L \left(\sum_{k=j}^{j + L - 1} \cos^2(2 \pi \omega k)\right)}},
%	\end{equation}
%	\begin{equation}\label{th6_sep2}
%	\rho^r_{i,j} = \frac{\sum_{k=1}^K \hat c_k\cos(2 \pi \omega (j + k - 1))}{\sqrt{\left(\sum_{k=1}^K \hat c_k\right) \left(\sum_{k=1}^K \hat c_k\cos^2(2 \pi \omega (j + k - 1))\right)}}.
%	\end{equation}
%\eqref{th6_sep1} coincides with \eqref{th5_sep1} in Theorem \ref{prop:separ1}, 
%thus the order of \eqref{th6_sep1} is already obtained, so let us just move to the estimation of order of \eqref{th6_sep2}. Consider the correlation of first rows only, since the proof for remained rows is absolutely analogous. Consider a numerator of $\rho^r_{1,1}$:
%	\begin{gather*}
%	\sum_{k=1}^K \hat c_k\cos(2 \pi \omega k) = \sum_{k=1}^{L-1} \hat c_k\cos(2 \pi \omega k) + \sum_{k=L}^{K - L + 1} \frac{\cos(2 \pi \omega k)}{L} +\\+ \sum_{k=K - L + 2}^{K} \hat c_k\cos(2 \pi \omega k) = I_1 + I_2 + I_3,
%	\end{gather*}
%	which is splitted by three parts. $I_2$ has order $O(1/L)$, and the order of $I_3$ is equal to the order of $I_1$:
%	\begin{multline*}
%	|I_1|=\bigg|\sum_{k=1}^{L-1}\frac{1}{L}\left(\frac{k}{L} + \sum_{j=k}^{L-1} \frac{1}{j} \right) \cos(2 \pi \omega k)\bigg| =\\= \bigg|\sum_{k=1}^{L-1} \frac{k \cos(2 \pi \omega k)}{L^2} +  \frac{1}{L}\sum_{k = 1}^{L-1}\sum_{j = k}^{L-1}\frac{\cos(2 \pi \omega k)}{j}\bigg| \le \\ \le
%	\bigg|\sum_{k=1}^{L-1} \frac{k \cos(2 \pi \omega k)}{L^2}\bigg| + \bigg|\frac{1}{L}\sum_{k = 1}^{L-1}\sum_{j = k}^{L-1}\frac{\cos(2 \pi \omega k)}{j}\bigg|.
%	\end{multline*}
%	Using the fact that
%	\begin{multline*}
%	\sum_{k=1}^n k \cos(ak + b) = -\frac{1}{4}\csc^2(a/2)(-(n+1)\cos(an+b) + \\ + n\cos(an + a + b) + \cos b),
%	\end{multline*}
%	we obtain that:
%	\begin{multline*}
%	\bigg|\sum_{k=1}^{L-1} \frac{k \cos(2 \pi \omega k)}{L^2}\bigg| = O(1/L), \quad
%	\bigg|\frac{1}{L}\sum_{k = 1}^{L-1}\sum_{j = k}^{L-1}\frac{\cos(2 \pi \omega k)}{j}\bigg| = \\ =\bigg|\frac{1}{L}\sum_{j = 1}^{L-1}\sum_{k = 1}^{j}\frac{\cos(2 \pi \omega k)}{j}\bigg| \le \bigg|\frac{1}{L}\sum_{j = 1}^{L-1}\frac{d}{j}\bigg| = O \left(\frac{H_L}{L} \right),
%	\end{multline*}
%	where $d$ is some constant.
%	
%	Following sum is considered for the denominator of $\rho^r_{1,1}$:
%	\begin{equation*}
%	\sum_{k=1}^K \hat c_k = N / L
%	\end{equation*}
%	by definition, and the following part has lower bound:
%	\begin{multline*}
%	\sum_{k=1}^K \hat c_k\cos^2(2 \pi \omega (j + k - 1)) \ge \\ \ge \sum_{k=1}^K \frac{1}{L}\cos^2(2 \pi \omega (j + k - 1)) = O \left(\frac{K}{L} \right).
%	\end{multline*}
%\end{proof}

%\section*{Acknowledgements}
%And this is an acknowledgements section with a heading that was produced by the
%$\backslash$section* command. Thank you all for helping me writing this
%\LaTeX\ sample file.

\bibliographystyle{imsart-number}
\bibliography{zvonarev}

\end{document}
