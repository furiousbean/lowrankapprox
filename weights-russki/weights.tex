\documentclass[10pt]{article}

\usepackage[text={14cm,20cm}]{geometry}
\usepackage{mathtext}
\usepackage{amsmath,amssymb,amsthm,amscd,amsfonts}

\usepackage[utf8x]{inputenc}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{lmodern}


\usepackage{euscript}
\usepackage{relsize}
\usepackage{mathdots}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{caption2}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{titlesec}
\usepackage{bbold}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\usepackage{mathtext}%русские буквы в формулах

\usepackage[colorlinks, urlcolor=blue, pdfborder={0 0 0 [0 0]}]{hyperref}

\hyphenation{Struc-tu-red}
\hyphenation{Ran-do-mized}
\hyphenation{Ma-xi-mi-za-tion}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\tr}{tr}
\providecommand*{\BibDash}{}

\def\rank{\mathop{\mathrm{rank}}}
\newtheorem{corollary}{Следствие}
\newtheorem{proposition}{Предложение}
\newtheorem{algorithm}{Алгоритм}
\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}

\usepackage{euscript}
\input{newcommands}
\input{letters}

%\sectionfont{\centering}

%\subsectionfont{\centering}
%\subsubsectionfont{\normalsize}
%\setcounter{page}{1}


\author{Звонарев Никита}
\title{Односторонние веса ряда}
\begin{document}
\maketitle
\section{Постановка задачи нахождения весов}
\subsection{Общие соображения}
В этом разделе рассмотрим различные способы нахождения матричных весов $c_i$, $i = 1, \ldots, K$ с целью получения приближённо равных весов ряда $q_i$, $i = 1, \ldots, N$. Теорема \footnote{ссылка на соотношение между весами} даёт необходимые и достаточные условия существования точных единичных весов ряда. Однако, мы тут же сталкиваемся с двумя проблемами. Во-первых, $N$ должно быть кратно $L$. Чаще всего $L$ стоит брать приближённо равным $N/2$, но $N$ может быть как нечётным, так и вовсе простым числом! Естественно, как резкое ограничение на возможные длины окна $L$, так и возможное обрезание ряда $\tsX$ до кратной $L$ длины крайне нежелательны. Во-вторых, полученные в теореме (?) веса $c_i$ содержат нули, что неприменимо, согласно \footnote{замечание про кривой Cadzow(0)}: вырожденная матрица весов $\bfC = \text{diag}(c_1, \ldots, c_K)$ порождает косоугольное полускалярное произведение и приводит к неэквивалентной задаче аппроксимации ряда.

Естественный шаг --- ограничить степень вырожденности матрицы $\bfC$. Введём следующие ограничения на допустимые веса $c_i$: во-первых, все $c_i > 0$. Во-вторых, по аналогии с алгоритмом Cadzow($\alpha$), введём параметр $0 \le \alpha \le 1$. Как будет выяснено далее в Разделе \footnote{ссылка}, на скорость сходимости алгоритма Oblique Cadzow влияет число обусловленности (condition number) матрицы: чем больше $\text{Сond}(\bfC)$, тем медленнее сходится метод Oblique Cadzow.
 Поэтому рассмотрим следующее ограничение: 
\begin{equation} \label{eq:ratiocond}
\frac{\min_i c_i}{\max_i c_i} \ge \alpha,
\end{equation}
что эквивалентно тому, что $\text{Сond}(\bfC) \le 1/\alpha$.

В работе \footnote{ссылка на работу Жиглявского} рассматривается иное ограничение:  $c_i >= \alpha > 0$, $i = 1, \ldots, K$. Заметим, что ограничение весов $c_i$ снизу не имеет такой удобной интерпретации, как используемое в нашей работе условие \eqref{eq:ratiocond}.

\subsection{Общая постановка задачи аппроксимации весов}
Для удобства перепишем уравнение \footnote{ссылка на уравнение, связывающее $c_i$ и $q_i$} в матричном виде. Для этого рассмотрим матрицу $\bfT$ порядка $N \times K$, имеющую следующий вид:
\begin{equation} \label{eq:tmatrix}
\bfT = (t_{i, j}), \quad t_{i, j} = \begin{cases}
1, & \text{для} \; i = j, \ldots, j + L - 1, \\
0, & \text{в противном случае}.
\end{cases}
\end{equation}
Таким образом, вектор весов ряда $Q = (q_1, \ldots, q_N)^\rmT$ может быть выражен через вектор матричных весов $C = (c_1, \ldots, c_K)^\rmT$ как $Q = \bfT C$. Заметим, что матрица $\bfT$ является матрицей полного ранга.

В общем виде задача формулируется следующим образом:
\begin{multline} \label{eq:commonw}
\|\bfT C - I_N\|_\cdot \to \min_{c_i, i = 1, \ldots, K} \quad \text{при условиях} \;
c_i > 0, \; i = 1, \ldots, K, \; 
\frac{\min_i c_i}{\max_i c_i} \ge \alpha,
\end{multline}
где $I_N = (1, \ldots, 1)^\rmT$ --- требуемые веса ряда (вектор из $N$ единиц), $0 \le \alpha \le 1$ --- параметр, регулирующий скорость сходимости алгоритма Oblique Cadzow, $\|\cdot\|_\cdot$ --- норма в $\sfR^N$. Рассмотрим следующие стандартные нормы:
\begin{enumerate}
	\item $\|X\|_\cdot = \|X\|_\infty = \max_i |x_i|$,
	\item $\|X\|_\cdot = \|X\|_1 = \sum_i |x_i|$ --- норма, порождающая манхеттенскую метрику,
	\item $\|X\|_\cdot = \|X\|_2 = \sqrt{\sum_i x_i^2}$ --- обычная евклидова норма.
\end{enumerate}
Рассмотрим каждый случай по отдельности.

\subsection{Случай нормы $\|X\|_2$: эквивалентные формулировки}
Рассмотрим задачу \ref{eq:commonw} в случае нормы $\|X\|_2$. В терминах квадратичного программирования (КП) она выглядит следующим образом:
\begin{gather}\label{eq:quadtf}
f(C) = \frac{1}{2} C^\rmT \bfS C - L_K^T C \quad \text{при условиях} \\
\label{eq:quadtf_conf}\begin{cases}
c_i - \alpha c_j \ge 0, & i \ne j \\
c_i \ge 0, & i = j
\end{cases}, \quad 1 \le i, j \le K,
\end{gather}
где 	$\bfS = \bfT^\rmT \bfT$ --- положительно определённая матрица размера $K \times K$, которая равна:
		\begin{equation*}
		\bfS = (s_{i,j}), \quad s_{i,j} = \begin{cases}
		L - |i - j|, & |i - j| \le L, \\
		0, & \text{в противном случае},
		\end{cases}
		\end{equation*}
		а вектор $L_K = \bfT^\rmT I_N  \in \sfR^K$, $L_K = (L, \ldots, L)^\rmT$. 

Справедливо следующее:
\begin{proposition}\label{prop:uniqsymm}
\begin{enumerate}
\item Задача \eqref{eq:quadtf} с условиями \eqref{eq:quadtf_conf} имеет единственное решение $C^\star$.
\item Решение $C^\star = (c^\star_1, \ldots, c^\star_K)$ является симметричным, то есть для любого индекса $1 \le i \le K$: $c^\star_i = c^\star_{K - i + 1}$.
\end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
\item Задача \eqref{eq:quadtf} с набором условий \eqref{eq:quadtf_conf} --- задача КП с набором из $K^2$ линейных ограничений и строго выпуклой квадратичной целевой функцией, поэтому решение такой задачи единственно.
\item Достаточно рассмотреть вектор $C^{\star \star} = (c^\star_K, \ldots, c^\star_1)$, и заметить, что $f(C^\star) = f(C^{\star \star})$ и $C^{\star \star}$ удовлетворяет условию \ref{eq:ratiocond}, значит, $C^\star = C^{\star \star}$, что и требовалось доказать.
\end{enumerate}
\end{proof}
Предложение \ref{prop:uniqsymm} даёт формулировку задачи аппроксимации весов в терминах квадратичного программирования, но, к сожалению, $K^2$ линейных ограничений являются серьёзной проблемой при решении задачи на практике. Рассмотрим ещё две эквивалентных формулировки, свойства которых используются при решении задачи.

Сделаем дополнительное предположение: пусть в точке $j$ веса достигают своего максимума (тем самым, снизим число линейных ограничений до линейного по $K$ размера), а само $j$ будем перебирать в цикле от $1$ до $\lceil K/2\rceil$. Таким образом, решение задачи --- это ответ на одну из $\lceil K/2\rceil$ подзадач квадратичного программирования. При этом воспользуемся тем фактом, что решение симметрично.

Формально, задача выглядит следующим образом:
\begin{equation} \label{eq:positw}
f(C_j^\star) \to \min_{j = 1,\ldots, \lceil K/2\rceil},
\end{equation}
где
\begin{gather} \label{eq:j_positw}
C^\star_j  = \argmin_{C} \frac{1}{2} C^\rmT \bfS C - L_K^T C \quad \text{при условиях} \\
c_i = c_{K - i + 1}, \; i = 1,\ldots, \lceil K/2\rceil, \label{eq:positw_symm}\\ 
c_j \ge 0, \label{eq:positw_notnull}\\
c_i - \alpha c_j \ge 0, \; i = 1,\ldots, j-1, j+1, \ldots, \lceil K/2\rceil, \label{eq:positw_min}\\
c_j -  c_i \ge 0, \; i = 1,\ldots, j-1, j+1, \ldots, \lceil K/2\rceil \label{eq:positw_max}.
\end{gather}

Для решения каждой из подзадач воспользуемся так называемым ``Primary Active Set'' методом решения задачи КП, описанным в \footnote{ссылка}. Специфика задачи позволяет существенно сократить время работы алгоритма. О том, как эффективно воспользоваться этим методом, см. раздел \ref{subsect:graphprog}.

Рассматривается ещё одна эквивалентная формулировка задачи \eqref{eq:commonw} в терминах КП. Для этой формулировки получен простой Алгоритм \ref{alg:nonnegatfc} проверки решения на оптимальность.

Введём $c_\text{max}$ --- дополнительную переменную, хранящую максимальный вес, и перейдём к новым переменным $\hat c_i = c_\text{max} - c_i$, $i = 1, \ldots, K$ --- разница между максимальным среди всех и текущим весом, при этом $c_\text{max} \ge 0$, $\hat c_i \ge 0$. Между векторами $C = (c_1, \ldots, c_K)^\rmT$ и $\widehat C = (\hat c_1, \ldots, \hat c_K, c_\text{max})^\rmT$ существует простое линейное соответствие: $C = \bfH \widehat C$, где $\bfH \in \sfR^{K \times (K+1)}$ --- матрица следующего вида:
\begin{equation} \label{eq:hmatrix}
\bfH = \left(
\begin{array}{cccc}
-1 &  &  & 1 \\ 
& \ddots &  & \vdots \\ 
&  & -1 & 1
\end{array} 
\right).
\end{equation}
Условие \eqref{eq:ratiocond}, устанавливающее границу снизу для весов, в новых обозначениях записывается как $(1 - \alpha) c_\text{max} - \hat c_i \ge 0$, $i = 1, \ldots, K$.

Получаем следующую задачу квадратичного программирования с линейными ограничениями, но с нестрого выпуклой целевой функцией:
\begin{gather}\label{eq:nonnegatqw}
\hat f(\widehat C) = \frac{1}{2} \widehat C^\rmT  \bfH^\rmT \bfS \bfH \widehat C - L_K^\rmT \bfH  \widehat C \to \min_{c_\text{max}, \, \hat c_1, \ldots, \hat c_K} \quad \text{при условиях} \\
\label{eq:nonnegatqw_cond}
c_\text{max} \ge 0, \quad \hat c_j \ge 0, \quad (1 - \alpha) c_\text{max} - \hat c_j \ge 0, \quad j = 1, \ldots, K.
\end{gather}

\subsection{Случай нормы $\|X\|_2$: алгоритм решения}

\begin{theorem} \label{th:eqivqw}
	Задачи \eqref{eq:quadtf}, \eqref{eq:positw} и \eqref{eq:nonnegatqw}  эквивалентны.
\end{theorem}
\begin{proof}
	Приведём идею. Достаточно показать следующее:
	\begin{enumerate}
		\item Если $C$ --- решение задачи \eqref{eq:quadtf}, то при $j = \argmax_i c_i$ целевая функция задачи \eqref{eq:positw} достигает меньшего значения, то есть $f(C_j^\star) \le f(C)$. Для этого необходимо построить симметричное решение $C_\text{symm}$: $c_{\text{symm}, i} = (c_i + c_{K - i + 1})/2$, $i = 1, \ldots, K$, после чего легко увидеть, что $f(C_j^\star) \le f(C_\text{symm}) \le f(C)$.
		\item Если $j$ --- решение задачи \eqref{eq:positw}, то положив $c_\text{max} = \max(c_{j, 1}, \ldots, c_{j, K})$, $\hat c_i = c_\text{max} - c_{j, i}$, $i = 1, \ldots, K$, где $C_j^\star = (c_1, \ldots, c_K)$, получим, что $\hat f(\hat C) = f(C^\star_j)$.
		\item Если $\hat C$ --- решение задачи \eqref{eq:nonnegatqw}, то взяв $c_i =c_\text{max} - \hat c_i$, получим, что $f(C) = \hat f(\hat C)$.
	\end{enumerate}
	
	При этом везде необходимо проверить, что выполняются все соответствующие линейные ограничения.
\end{proof}

Теорема \ref{th:eqivqw} является обоснованием следующего алгоритма:

\begin{algorithm}
	\label{alg:solveqw}
	\textbf{Вход}: Параметры $L$, $K$, $\alpha$.
	
	\textbf{Результат}:
	Вектор оптимальных весов $C$.
	
	\begin{enumerate}
		\item Положить $j$ = 1.
		\item Решить подзадачу КП \eqref{eq:j_positw} при заданном индексе максимального веса $j$. Пусть $C^\star_j = (c_{j, 1}, \ldots, c_{j, K})$ --- решение.
		\item Зададим вектор $\widehat C$, взяв $c_\text{max} = \max(c_{j, 1}, \ldots, c_{j, K})$, $\hat c_i = c_\text{max} - c_{j, i}$.
		\item Проверить, является ли вектор $\widehat C$ решением задачи \eqref{eq:nonnegatqw}. Если да, то положить $C$ решением задачи, иначе взять $j = j + 1$ и перейти к Пункту 2.
	\end{enumerate}
\end{algorithm}

Таким образом, если встретился нужный индекс $j$, то алгоритму не понадобится перебирать оставшиеся индексы и решать подзадачу в \ref{eq:positw} лишний раз. В практических экспериментах максимальный вес всегда находится на краях, то есть Алгоритм \ref{alg:solveqw} останавливается уже при $j = 1$.

\subsection{Случай нормы $\|X\|_2$: проверка решения задачи}
Предложим быстрый алгоритм, проверяющий, является ли заданный вектор $\widehat C$ точкой, в которой достигается глобальный минимум в задаче \eqref{eq:nonnegatqw}. Для этого применим теорему о необходимом и достаточном условии минимума в задаче квадратичного программирования для задачи \eqref{eq:nonnegatqw}.
\begin{theorem}
	Рассмотрим вектор $R =  \bfH^\rmT \bfS \bfH \widehat C - \bfH^\rmT L_K = (r_1, \ldots, r_{K+1})^\rmT \in \sfR^{K+1}$. Тогда $\widehat C$ является решением задачи \eqref{eq:nonnegatqw} тогда и только тогда, когда:
	\begin{enumerate}
		\item $R^\rmT \widehat C = 0$,
		\item Существует вектор $U = (u_1, \ldots, u_K)^\rmT \in \sfR^K$ такой, что: \begin{itemize}
			\item $u_i \ge 0$, $i = 1, \ldots, K$,
			\item $u_i \ge -r_i$, $i = 1, \ldots, K$,
			\item $(1 - \alpha) \sum_{i=1}^K u_i \le r_{K+1}$.
		\end{itemize}
	\end{enumerate}
\end{theorem} \label{th:nonnegatfc}
\begin{proof}
	Прямое применение теоремы \footnote{теорема Куна-Таккера} из \footnote{модная книжка по квадратичному программированию. В русской статье можно не мучиться и сослаться на Гавурина-Малоземова} к задаче \eqref{eq:nonnegatqw} с линейными ограничениями \eqref{eq:nonnegatqw_cond}.
\end{proof}

Рассмотрим следующий алгоритм, использующий теорему \ref{th:nonnegatfc} в качестве обоснования.
\begin{algorithm}
	\label{alg:nonnegatfc}
	\textbf{Вход}: предполагаемое решение $\widehat C$, параметр $\alpha$.
	
	\textbf{Результат}:
	Булево значение: является ли $\widehat C$ решением задачи \eqref{eq:nonnegatqw}.
	
	\begin{enumerate}
		\item Вычислить $R = \bfH^\rmT \bfS \bfH \widehat C - \bfH^\rmT L_K$.
		\item Если $R^\rmT \widehat C \neq 0$, то вернуть FALSE, иначе перейти к следующему пункту.
		\item Выбрать в качестве вектора $U = (u_1, \ldots, u_K)$ следующие значения: $u_i = \max(0, -r_i)$, $i = 1, \ldots, K$.
		\item Если $(1 - \alpha) \sum_{i=1}^K u_i \le r_{K+1}$, то вернуть TRUE, иначе FALSE.
	\end{enumerate}
\end{algorithm}

Прокомментируем последние два шага алгоритма. В третьем пункте $u_i$ выбираются так, чтобы удовлетворять всем условиям теоремы, кроме последнего условия $(1 - \alpha) \sum_{i=1}^K u_i \le r_{K+1}$. При этом очевидно, что выбор делается так, чтобы сумма $(1 - \alpha) \sum_{i=1}^K u_i$ достигала своего минимума, из чего следует, что выполнение последнего условия эквивалентно оптимальности решения.


\subsection{Задача квадратичного программирования специального вида} \label{subsect:graphprog}
Суть любого ``Active Set'' метода состоит в последовательном переборе подмножества ограничений, которые выполняются, как равенство для промежуточной точки --- решения задачи квадратичного программирования. Такое множество называется \emph{рабочим множеством}. Заметим, что в задаче \eqref{eq:j_positw} каждое ограничения затрагивает максимум две переменные. Эту информацию можно использовать для построения эффективного с точки зрения вычислительной сложности решения.

В общем случае, для использования ``Active Set'' метода требуется решать следующую вспомогательную задачу квадратичного программирования вместе с вычислением множителей Лагранжа в точке минимума:
\begin{gather}\label{eq:subtask}
\frac{1}{2} X^\rmT \bfS X - V^\rmT X \to \min_X \quad \text{при условии} \\
\bfA^\rmT X = \mathbb{0},
\end{gather}
где $\bfS \in \sfR^{K \times K}$ --- произвольная симметричная положительно определенная матрица, $V \in \sfR^{K \times K}$ --- произвольный вектор, а матрица $\bfA \in \sfR^{K \times m}$ полного ранга $m \le K$, при этом каждый столбец матрицы $\bfA = [A_1:\ldots:A_m]$ содержит либо один, либо два ненулевых коэффициента, а $m$ велико и близко к $K$.

Для решения поставленной вспомогательной задачи \ref{eq:subtask} есть явная формула обобщенного метода наименьших квадратов:
\begin{equation*}
X^\star = \overline \bfA (\overline \bfA^\rmT \bfS \overline \bfA)^{-1} \overline \bfA^\rmT V,
\end{equation*}
где матрица $\overline \bfA \in \sfR^{K \times (K-m)}$ состоит из столбцов --- базиса дополнения к базису столбцов матрицы $\bfA$. За счет специального вида матрицы $\bfS$ (как, например, в нашем случае) и матрицы $\overline \bfA$ соответствующие члены данного выражения могут быть быстро вычислены, при этом система линейных уравнений имеет порядок $K-m$, что при большом значении $m$ позволяет быстро найти решение.

Множители Лагранжа, составляющие вектор $U = (u_1, \ldots, u_m)^\rmT \in \sfR^{m}$, находятся из переопределенной системы линейных уравнений $\bfA U =  \bfS X^\star - V = R = (r_1, \ldots, r_K)^\rmT$, при этом вектор $R \in \text{span}(A_1, \ldots, A_m)$. Аналогично, решение можно найти быстро, используя специальный вид матрицы $\bfA$.

Для этого построим неориентированный граф $\sfG=(\sfV, \sfE)$, где множество вершин $\sfV = \{1, 2, \ldots, K\}$, а множество рёбер $\sfE = \{e_1, \ldots, e_m\}$ строится следующим образом: если $i$-й столбец матрицы $A_i = (a_{i, 1}, \ldots, a_{i, K})^\rmT$ содержит два ненулевых коэффициента $a_{i,k}$ и $a_{i,l}$, то $i$-е ребро соединяет вершины $k$ и $l$, то есть $e_i = (k, l)$; если же ребро содержит только один ненулевой коэффициент $a_{i,k}$, то $i$-е ребро представляет из себя петлю из вершины $k$ в вершину $k$, т.е. $e_i=(k, k)$. Назовём такой граф \emph{графом подзадачи} \eqref{eq:subtask}.

Разбиение графа $\sfG$ на компоненты связности соответствует приведению матрицы $\bfA$ к блочно-диагональному виду. Такое разбиение множества вершин $\sfV$ = $\bigcup_{i=1}^{s} \sfV_i$ на $s$ подмножеств, где каждое множество вершин $\sfV_i$ вместе с соответствующим подмножеством ребер $\sfE_i$ является связным графом $\sfG_i$, и при этом среди $\sfE$ не существует ребра, связывающего две разные компоненты, может быть построено, например, используя алгоритм поиска в глубину (DFS), см. \footnote{ссылка на Кормена}.

Существует следующее простое утверждение о графе $\sfG_i$.
\begin{proposition}
Каждый граф $\sfG_i$ является либо деревом, либо графом с одним циклом.
\end{proposition}
\begin{proof}
Пусть $p_i$ = $|\sfV_i|$. Так как $\sfG_i$ --- связный подграф, то он содержит минимум $p_i - 1$ вершину, то есть $|\sfE_i| \ge p_i - 1$. С другой стороны, каждому ребру из $\sfE_i$ соответствует столбец матрицы $\bfA$, у которого коэффициенты на всех индексах вне $\sfV_i$ гарантированно нулевые. Так как по предположению матрица $\bfA$ содержит линейно-независимые столбцы, получаем, что $\sfE_i \le p_i$.
\end{proof}

Рассмотрим каждую компоненту связности $\sfG_i$ по отдельности, и для них опишем алгоритм поиска матрицы $\overline \bfA$ и решения системы $\bfA U = R$.
\subsubsection{Решение системы линейных уравнений}
Заметим простой факт: если некоторая вершина $k$ является листом, то есть существует только одно ребро $e_i = (l, k)$, ведущее в эту вершину, то коэффициент $u_i = r_k / a_{i, k}$, после чего $i$-е ребро можно удалить и перейти к решению системы $\widehat \bfA \widehat U = \widehat R$, где $\widehat \bfA = [A_1, \ldots, A_{i-1}, A_{i+1}, \ldots, A_m]$, $\widehat U = (u_1, \ldots, u_{i - 1}, u_{i + 1}, \ldots, u_m)$, $\widehat R = R - u_i A_i$. Соответственно, быстрый алгоритм решения представляет из себя последовательное сокращение дерева путем ``отрезания'' листов и нахождения решения системы.

Вкратце, алгоритм выглядит так: с помощью DFS проверим, содержит ли $\sfG_i$ цикл. Если нет (то есть $\sfG_i$ является деревом), то соответствующее последовательное сокращение дерева можно реализовать внутри обхода дерева с помощью DFS. В том случае, когда $\sfG_i$ содержит цикл, найдём его и пометим ребра, входящие в него. Затем из каждой вершины, лежащей в цикле, запустим описанный ранее спуск в глубину с сокращением (естественно, исключая вершины в цикле). В итоге, получим коэффициенты вектора $U$ для всех индексов, соответствующие ребра которых не лежат в цикле, а лежат в исходящих деревьях. Оставшаяся система линейных уравнений после перенумерации сводится к решению системы с циклической ленточной матрицей ширины $2$. Данную систему можно решить за линейное время с помощью метода Гаусса, но, в целом, обход графа в глубину и решение таких систем лежат вне области данной статьи. За пояснениями см. \footnote{Кормен}, \footnote{Какая-нибудь подходящая книга по разряженым матрицам}.
\subsubsection{Метод поиска матрицы $\overline \bfA$}
Покажем, как можно найти матрицу $\overline \bfA$, базис столбцов которой является дополнением к базису $\bfA$. Это можно сделать, предъявив ортогональную матрицу порядка $K \times (K-m)$, каждый столбец которой ортогонален всем столбцам матрицы $\bfA$.

Рассмотрим компоненту связности $\sfG_i$. Если она является графом с одним циклом, то дополнение к соответствующему базису столбцов является пустым. Если же $\sfG_i$ является деревом, то линейная оболочка столбцов матрицы $\bfA$, соответствующих ребрам, лежащих в $\sfE_i$, имеет размерность $p_{i-1}$, где $p_i = | \sfV_i |$, а дополнение до линейного подпространства имеет размерность $1$, где $\sfR^{\sfV_i}$ --- подпространство $\sfR^K$ с элементами, у которых элементы на лежащих в $\sfV_i$ индексах произвольные, и нулевые для всех остальных индексов.

Тогда если мы рассмотрим вектор $\overline A_i = (\overline a_{i, 1}, \ldots, \overline a_{i, K})^\rmT$, лежащий в дополнении до $\sfR^{\sfV_i}$, то для любого ребра $e_j= (l,k)$, лежащего в $\sfE_i$, должно выполняться условие ортогональности: $\overline a_{i, l} a_{j, l} + \overline a_{i, k} a_{j, k} = 0$.

Алгоритм поиска следующий: для всех $j \notin \sfV_i$ положим $\overline a_{i, j} = 0$. Выберем произвольный индекс $k \in \sfV_i$, положим $\overline a_{i, k} = 1$, после чего запустим алгоритм спуска по дереву в глубину из вершины $k$ по компоненте связности $\sfG_i$. При рассмотрении ребра $e_j$, идущего из посещенной алгоритмом вершины $k$ в непосещенную $l$, положим
\begin{equation*}
\overline a_{i, l} = -\frac{\overline a_{i, k} a_{j, k}}{a_{j, l}}.
\end{equation*}

Таким образом, получим матрицу $\overline \bfA = [\overline A_i \; | \; i = 1, \ldots, s, \; \sfG_i \; \text{является деревом}]$, удовлетворяющую всем нужным свойствам.

\subsection{Эвристика выбора начального приближения}
Оставшийся шаг --- описать начальную точку для ``Primary Active Set'' метода. На самом деле, на первом шаге подбирается рабочее множество, которое во многом совпадает с итоговым --- тем, на котором достигается минимум.

Посмотрим на набор ограничений в задаче \eqref{eq:j_positw}. Ограничения \eqref{eq:positw_symm} задают симметричность весов --- они обязательно входят в любое рабочее множество. Ограничение \eqref{eq:positw_notnull}, если оно входит в рабочее множество, устанавливает равенство нулю у точки с индексом $j$. Ограничения \eqref{eq:positw_min} устанавливают вес точки, равный минимально допустимому при заданном параметре $\alpha$, а \eqref{eq:positw_max} --- максимально возможный и равный $c_j$.

Эвристика заключается в том, чтобы для точек $1$, $2, \ldots, j-1$, $j + 1, \ldots, \ldots, \lceil K/2\rceil$ назначить первым $0 \le t \le \lceil K/2\rceil - 1$ максимальный вес, а оставшимся --- минимальный, после чего решить соответствующую задачу \eqref{eq:subtask} с $V = L_K$.  Графически, используя терминологию раздела \ref{subsect:graphprog}, такие ограничения можно изобразить следующим образом (на графе изображен случай $j = 1$, положение вершин соответствует реальному решению задачи, рёбра чёрного цвета соответствуют ограничениям вида \eqref{eq:positw_symm}, красного ---  \eqref{eq:positw_max}, синего --- \eqref{eq:positw_min}, ребра на рисунке ориентированы согласно порядку обхода графа из первой вершины):

\begin{center}
\begin{tikzpicture}[scale=0.85]
\node[shape=circle,draw=black] (A) at (0,3) {1};
\node[shape=circle,draw=black] (B) at (1.4,3) {2};
\node[shape=circle,draw=black] (C) at (2.8, 3) {\ldots};
\node[shape=circle,draw=black] (D) at (4.5,3) {$t + 1$};
\node[shape=circle,draw=black] (E) at (5.7, 0) {$t + 2$};
\node[shape=circle,draw=black] (F) at (7.4,0) {\ldots} ;
\node[shape=circle,draw=black] (G) at (9.4,0) {$K - t + 1$} ;
\node[shape=circle,draw=black] (H) at (10.3,3) {$K - t$} ;
\node[shape=circle,draw=black] (I) at (11.8,3) {\ldots} ;
\node[shape=circle,draw=black] (J) at (13.3,3) {$K - 1$} ;
\node[shape=circle,draw=black] (K) at (14.8,3) {$K$} ;

\path [->, draw=red](A) edge node[left] {} (B);
\path [->, draw=red](A) edge[bend left = 30] node[left] {} (C);
\path [->, draw=red](A) edge[bend left = 30] node[left] {} (D);
\path [->, draw=blue](A) edge node[left] {} (E);
\path [->, draw=blue](A) edge[bend left = 5] node[left]{} (F);
\path [->](E) edge[bend right = 30] node[left]{} (G);
\path [->](D) edge node[left] {} (H);
\path [->](C) edge[bend right = 35] node[left]{} (I);
\path [->](B) edge[bend right = 30] node[left]{} (J);
\path [->](A) edge[bend right = 70] node[left]{} (K);

\end{tikzpicture}
\end{center}


%$t$ будем перебирать в цикле жадно, начиная с $0$, пока значение целевой функции не начнет снова возрастать или не достигнет $0 \le t \le \lceil K/2\rceil - 1$.
Так как угадать значение $t$ трудно, опишем жадный алгоритм перебора $t$:
\begin{algorithm}
	\label{alg:beginheuristic}
	\textbf{Вход}: текущий индекс максимального веса $j$.
	
	\textbf{Результат}:
	Начальное рабочее множество ограничений и стартовая точка $C_0$.
	
	\begin{enumerate}
	    \item Положить $f_\text{previous} = +\infty$
	    \item Вычислить вектор индексов $Z = (1, 2, \ldots, j-1, j+1, \ldots, \lceil K/2\rceil)$
		\item Для $t = 1, 2, \ldots, \lceil K/2\rceil - 1$:
		\begin{enumerate}
		\item Решить следующую задачу квадратичного программирования, вычислить значение целевой функции $f$ и точки минимума $C^\star$, запомнить множество ограничений:
		\begin{gather*}
\frac{1}{2} C^\rmT \bfS C - L_K^T C \to \min_{C} \quad \text{при условиях} \\
c_i = c_{K - i + 1}, \; i = 1,\ldots, \lceil K/2\rceil, \\ 
c_{z_i} - \alpha c_j = 0, \; i = t+1, \ldots, \lceil K/2\rceil - 1\\
c_{z_i} -  c_i = 0, \; i = 1,\ldots, t.
\end{gather*}
        \item Если $f > f_\text{previous}$, то вернуть набор ограничений с предыдущей итерации и точку $C^\star_\text{previous}$, иначе положить $f_\text{previous} = f$, $C^\star_\text{previous} = C^\star$.
		\end{enumerate}
	\item Вернуть набор ограничений с последней итерации и точку $C^\star_\text{previous}$. \footnote{Этот шаг выполняется только в том случае, когда жадный алгоритм не встретил ``излом'' целевой функции}
\end{enumerate}
\end{algorithm}
Заметим, что при использовании теории раздела \ref{subsect:graphprog} для решения задачи КП по формуле взвешенного метода наименьших квадратов требуется решать систему линейных уравнений порядка $1$.
%\begin{theorem} \label{th:eqivqw}
%	Задачи \eqref{eq:nonnegatqw} и \eqref{eq:positw} эквивалентны.
%\end{theorem}
%\begin{proof}
%	Для доказательства эквивалентности нужно показать, что по индексу $j$ можно построить вектор $\widehat C$ такой, что $f_j \ge f(\widehat C)$, и наоборот: по вектору $\widehat C$ найти такой индекс $j$, что $f(\widehat C) \ge f_j$.
%	
%	Рассмотрим оба пункта доказательства.
%	\begin{enumerate}
%		\item Пусть $j$ --- индекс максимального веса, $\widetilde C_j = \widetilde C_j^*$ --- решение соответствующей задачи \eqref{eq:j_positqw}. Тогда рассмотрим следующий вектор $\widehat C$: $\hat c_i = \tilde c_i$, $i = 1, \ldots, j-1, j+1, \ldots, K$, $c_\text{max} = \tilde c_\text{max}$, $\hat c_j = 0$. Так как по построению $\bfH_j \widetilde C_j = \bfH \widehat C$, то целевые функции равны. Выполнение свойств \eqref{eq:nonnegatqw_cond} очевидным образом следует из условий \eqref{eq:j_positqw_cond}.
%		\item Вначале рассмотрим вектор $\widehat C$, и найдём такой индекс $j$ и вектор $\widetilde C_j$, что $f(\widehat C) = f_j(\widetilde C_j)$. В качестве $j$ возьмём $j = \argmax_i c_\text{max} - \hat c_i$. Тогда $\tilde c_\text{max} = c_\text{max} - \hat c_j$, а $\tilde c_i = \tilde c_\text{max} - c_\text{max} + \hat c_i = \hat c_i - \hat c_j$, $i = 1, \ldots, j-1, j+1, \ldots, K$. Заметим, что $\tilde c_\text{max} \le c_\text{max}$. Из того, что $c_i = c_\text{max} - \hat c_i = c_\text{max} - \hat c_j + \hat c_j - \hat c_i = \tilde c_\text{max} + \hat c_j - \hat c_i =$ $\tilde c_\text{max} - \tilde c_i$, что эквивалентно $\bfH_j \widetilde C_j = \bfH \widehat C$, следует равенство целевых функций задачи \eqref{eq:nonnegatqw} и вспомогательной задачи \eqref{eq:j_positqw}.
%		
%		Покажем, что $\tilde c_\text{max} \ge 0$. $\tilde c_\text{max} = c_\text{max} - \hat c_j \ge \hat c_j (1/(1 - \alpha) - 1) \ge 0$, если $\alpha < 1$. Если же $\alpha = 1$, то все $\hat c_i = 0$, и $\tilde c_\text{max} = c_\text{max} \ge 0$. $\tilde c_i \ge 0$, $i = 1, \ldots, K$, $i \neq j$ следует из того, что $\hat c_j$ --- минимальное среди всех $\hat c_i$, $i = 1, \ldots, K$. Последние неравенства проверяются следующим способом: $(1 - \alpha) \tilde c_\text{max} - \tilde c_i = (1 - \alpha)(c_\text{max} - \hat c_j) - \hat c_i + \hat c_j = $ $(1 - \alpha)c_\text{max} - \hat c_i + \alpha \hat c_j \ge 0$.
%		
%		Заметим, что $f_j = f_j(\widetilde C_j^*) \le f_j(\widetilde C_j)$, что и требовалось доказать.
%	\end{enumerate}
%\end{proof}

\subsection{Случай нормы $\|X\|_\infty$}
Покажем, что данная задача сводится к задаче линейного программирования с линейными ограничениями с помощью добавления вспомогательных переменных.

Рассмотрим $\omega \in \sfR$ --- ещё один дополнительный параметр, который хранит значение целевой функции. Заметим, что при любых дополнительных условиях, любой матрице $\bfA \in \sfR^{N \times K}$, любого $\widetilde Y \in \sfR_N$, $\widetilde Y = (\tilde y_1, \ldots, \tilde y_N)^\rmT$, следующие задачи эквивалентны: 
\begin{equation*}
\|\bfA X - \widetilde Y \|_\infty \to \min_{X \in \sfR_K}
\end{equation*}
и 
\begin{gather*}
\omega \to \min_{X, \omega} \quad \text{при условиях} \\ \bfA X = Y = (y_1, \ldots, y_N)^\rmT, \quad y_i - \tilde y_i \le \omega, \quad y_i - \tilde y_i \ge -\omega, \quad i = 1, \ldots, N. 
\end{gather*}

Таким образом, в терминах линейного программирования задача \eqref{eq:commonw} в случае нормы $\|X\|_\infty$ переписывается следующим образом:
\begin{gather*}
\omega \to \min_{c_\text{max}, \, \hat c_1, \ldots, \hat c_K, \, \omega} \quad \text{при условиях} \\ \bfT \bfH \widehat C = Q = (q_1, \ldots, q_N)^\rmT, \quad q_i - 1 \le \omega, \quad q_i - 1 \ge -\omega, \quad i = 1, \ldots, N, \\
c_\text{max} \ge 0, \quad \hat c_j \ge 0, \quad (1 - \alpha) c_\text{max} - \hat c_j \ge 0, \quad j = 1, \ldots, K.
\end{gather*}

\subsection{Случай нормы $\|X\|_1$}
Аналогично предыдущему случаю, задача \eqref{eq:commonw} может быть записана в терминах линейного программирования.

Для этого заметим следующее: если ввести $2N$ дополнительных переменных \\ $\kappa_1, \ldots, \kappa_N$, $\theta_1, \ldots, \theta_N \in \sfR$, то при любых дополнительных условиях, любой матрице $\bfA \in \sfR^{N \times K}$, любого $\widetilde Y \in \sfR_N$, $\widetilde Y = (\tilde y_1, \ldots, \tilde y_N)^\rmT$, следующие задачи эквивалентны: 
\begin{equation*}
\|\bfA X - \widetilde Y \|_1 \to \min_{X \in \sfR^K}
\end{equation*}
и 
\begin{gather*}
\sum_{i = 1}^N (\kappa_i + \theta_i) \to \min_{X, \, \kappa_1, \ldots, \kappa_N, \, \theta_1, \ldots, \theta_N} \quad \text{при условиях} \\ \bfA X = Y = (y_1, \ldots, y_N)^\rmT, \quad y_i - \tilde y_i = \kappa_i - \theta_i, \quad \kappa_i \ge 0, \quad \theta_i \ge 0, \quad i = 1, \ldots, N. 
\end{gather*}

В итоге, задача \eqref{eq:commonw} в терминах линейного программирования в случае нормы $\|X\|_1$ переписывается следующим образом:
\begin{gather*}
\sum_{i = 1}^N (\kappa_i + \theta_i) \to \min_{c_\text{max}, \, \hat c_1, \ldots, \hat c_K, \, \kappa_1, \ldots, \kappa_N, \, \theta_1, \ldots, \theta_N} \quad \text{при условиях} \\ \bfT \bfH \widehat C = Q = (q_1, \ldots, q_N)^\rmT,  \quad q_i - 1 = \kappa_i - \theta_i, \quad \kappa_i \ge 0, \quad \theta_i \ge 0, \quad i = 1, \ldots, N, \\
c_\text{max} \ge 0, \quad \hat c_j \ge 0, \quad (1 - \alpha) c_\text{max} - \hat c_j \ge 0, \quad j = 1, \ldots, K.
\end{gather*}

\end{document}
